<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>LogLikelihoodDifferentiableFunction.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.loglinear.learning</a> &gt; <span class="el_source">LogLikelihoodDifferentiableFunction.java</span></div><h1>LogLikelihoodDifferentiableFunction.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.loglinear.learning;

import edu.stanford.nlp.loglinear.inference.CliqueTree;
import edu.stanford.nlp.loglinear.model.ConcatVector;
import edu.stanford.nlp.loglinear.model.GraphicalModel;

import java.util.Iterator;

/**
 * Created on 8/23/15.
 * @author keenon
 * &lt;p&gt;
 * Generates (potentially noisy, no promises about exactness) gradients from a batch of examples that were provided to
 * the system.
 */
<span class="fc" id="L16">public class LogLikelihoodDifferentiableFunction extends AbstractDifferentiableFunction&lt;GraphicalModel&gt; {</span>
  // This sets a gold observation for a model to use as training gold data
  public static final String VARIABLE_TRAINING_VALUE = &quot;learning.LogLikelihoodDifferentiableFunction.VARIABLE_TRAINING_VALUE&quot;;

  /**
   * Gets a summary of the log-likelihood of a singe model at a point
   * &lt;p&gt;
   * It assumes that the models have observations for training set as metadata in
   * LogLikelihoodDifferentiableFunction.OBSERVATION_FOR_TRAINING. The models can also have observations fixed in
   * CliqueTree.VARIABLE_OBSERVED_VALUE, but these will be considered fixed and will not be learned against.
   *
   * @param model   the model to find the log-likelihood of
   * @param weights the weights to use
   * @return the gradient and value of the function at that point
   */
  @Override
  public double getSummaryForInstance(GraphicalModel model, ConcatVector weights, ConcatVector gradient) {
<span class="fc" id="L33">    double logLikelihood = 0.0;</span>

<span class="fc" id="L35">    CliqueTree.MarginalResult result = new CliqueTree(model, weights).calculateMarginals();</span>

    // Cache everything in preparation for multiple redundant requests for feature vectors

<span class="fc bfc" id="L39" title="All 2 branches covered.">    for (GraphicalModel.Factor factor : model.factors) {</span>
<span class="fc" id="L40">      factor.featuresTable.cacheVectors();</span>
<span class="fc" id="L41">    }</span>

    // Subtract log partition function

<span class="fc" id="L45">    logLikelihood -= Math.log(result.partitionFunction);</span>

    // Quit if we have an infinite partition function

<span class="pc bpc" id="L49" title="1 of 2 branches missed.">    if (Double.isInfinite(logLikelihood)) return 0.0;</span>

    // Add the determined assignment by training values

<span class="fc bfc" id="L53" title="All 2 branches covered.">    for (GraphicalModel.Factor factor : model.factors) {</span>
      // Find the assignment, taking both fixed and training observed variables into account
<span class="fc" id="L55">      int[] assignment = new int[factor.neigborIndices.length];</span>
<span class="fc bfc" id="L56" title="All 2 branches covered.">      for (int i = 0; i &lt; assignment.length; i++) {</span>
<span class="fc" id="L57">        int deterministicValue = getDeterministicAssignment(result.marginals[factor.neigborIndices[i]]);</span>
<span class="fc bfc" id="L58" title="All 2 branches covered.">        if (deterministicValue != -1) {</span>
<span class="fc" id="L59">          assignment[i] = deterministicValue;</span>
        } else {
<span class="fc" id="L61">          int trainingObservation = Integer.parseInt(model.getVariableMetaDataByReference(factor.neigborIndices[i]).get(LogLikelihoodDifferentiableFunction.VARIABLE_TRAINING_VALUE));</span>
<span class="fc" id="L62">          assignment[i] = trainingObservation;</span>
        }
      }
<span class="fc" id="L65">      ConcatVector features = factor.featuresTable.getAssignmentValue(assignment).get();</span>
      // Add the log-likelihood from this observation to the log-likelihood
<span class="fc" id="L67">      logLikelihood += features.dotProduct(weights);</span>
      // Add the vector from this observation to the gradient
<span class="fc" id="L69">      gradient.addVectorInPlace(features, 1.0);</span>
<span class="fc" id="L70">    }</span>

    // Take expectations over features given marginals
    // NOTE: This is extremely expensive. Not sure what to do about that

<span class="fc bfc" id="L75" title="All 2 branches covered.">    for (GraphicalModel.Factor factor : model.factors) {</span>
      // OPTIMIZATION:
      // Rather than use the standard iterator, which creates lots of int[] arrays on the heap, which need to be GC'd,
      // we use the fast version that just mutates one array. Since this is read once for us here, this is ideal.
<span class="fc" id="L79">      Iterator&lt;int[]&gt; fastPassByReferenceIterator = factor.featuresTable.fastPassByReferenceIterator();</span>
<span class="fc" id="L80">      int[] assignment = fastPassByReferenceIterator.next();</span>
      while (true) {
        // calculate assignment prob
<span class="fc" id="L83">        double assignmentProb = result.jointMarginals.get(factor).getAssignmentValue(assignment);</span>
        // subtract this feature set, weighted by the probability of the assignment
<span class="fc bfc" id="L85" title="All 2 branches covered.">        if (assignmentProb &gt; 0) {</span>
<span class="fc" id="L86">          gradient.addVectorInPlace(factor.featuresTable.getAssignmentValue(assignment).get(), -assignmentProb);</span>
        }
        // This mutates the assignment[] array, rather than creating a new one
<span class="fc bfc" id="L89" title="All 2 branches covered.">        if (fastPassByReferenceIterator.hasNext()) fastPassByReferenceIterator.next();</span>
        else break;
<span class="fc" id="L91">      }</span>
<span class="fc" id="L92">    }</span>

    // Uncache everything, now that the computations have completed

<span class="fc bfc" id="L96" title="All 2 branches covered.">    for (GraphicalModel.Factor factor : model.factors) {</span>
<span class="fc" id="L97">      factor.featuresTable.releaseCache();</span>
<span class="fc" id="L98">    }</span>

<span class="fc" id="L100">    return logLikelihood;</span>
  }

  /**
   * Finds the deterministic assignment forced by a distribution, or if none exists returns -1
   *
   * @param distribution the potentially deterministic distribution
   * @return the assignment given by the distribution with probability 1, if one exists, else -1
   */
  private static int getDeterministicAssignment(double[] distribution) {
<span class="fc" id="L110">    int assignment = -1;</span>
<span class="fc bfc" id="L111" title="All 2 branches covered.">    for (int i = 0; i &lt; distribution.length; i++) {</span>
<span class="fc bfc" id="L112" title="All 2 branches covered.">      if (distribution[i] == 1.0) {</span>
<span class="pc bpc" id="L113" title="1 of 2 branches missed.">        if (assignment == -1) assignment = i;</span>
<span class="nc" id="L114">        else return -1;</span>
<span class="fc bfc" id="L115" title="All 2 branches covered.">      } else if (distribution[i] != 0.0) return -1;</span>
    }
<span class="fc" id="L117">    return assignment;</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>