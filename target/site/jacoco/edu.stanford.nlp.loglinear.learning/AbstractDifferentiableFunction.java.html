<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>AbstractDifferentiableFunction.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.loglinear.learning</a> &gt; <span class="el_source">AbstractDifferentiableFunction.java</span></div><h1>AbstractDifferentiableFunction.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.loglinear.learning;

import edu.stanford.nlp.loglinear.model.ConcatVector;

/**
 * Created on 8/26/15.
 * @author keenon
 * &lt;p&gt;
 * This provides a separation between the functions and optimizers, that lets us test optimizers more effectively by
 * generating convex functions that are solvable in closed form, then checking the optimizer arrives at the same
 * solution.
 */
<span class="fc" id="L13">public abstract class AbstractDifferentiableFunction&lt;T&gt; {</span>
  /**
   * Gets a summary of the function of a singe data instance at a single point
   *
   * @param dataPoint the data point we want a summary for
   * @param weights   the weights to use
   * @param gradient  the gradient to use, will be updated by accumulating the gradient from this instance
   * @return value of the function at this point
   */
  public abstract double getSummaryForInstance(T dataPoint, ConcatVector weights, ConcatVector gradient);
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>