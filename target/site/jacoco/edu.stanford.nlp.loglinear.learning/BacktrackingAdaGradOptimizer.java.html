<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>BacktrackingAdaGradOptimizer.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.loglinear.learning</a> &gt; <span class="el_source">BacktrackingAdaGradOptimizer.java</span></div><h1>BacktrackingAdaGradOptimizer.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.loglinear.learning; 
import edu.stanford.nlp.util.logging.Redwood;

import edu.stanford.nlp.loglinear.model.ConcatVector;

/**
 * Created on 8/26/15.
 * @author keenon
 * &lt;p&gt;
 * Handles optimizing an AbstractDifferentiableFunction through AdaGrad guarded by backtracking.
 */
<span class="nc" id="L12">public class BacktrackingAdaGradOptimizer extends AbstractBatchOptimizer  {</span>

  /** A logger for this class */
<span class="nc" id="L15">  private static Redwood.RedwoodChannels log = Redwood.channels(BacktrackingAdaGradOptimizer.class);</span>

  // this magic number was arrived at with relation to the CoNLL benchmark, and tinkering
  final static double alpha = 0.1;

  @Override
  public boolean updateWeights(ConcatVector weights, ConcatVector gradient, double logLikelihood, OptimizationState optimizationState, boolean quiet) {
<span class="nc" id="L22">    AdaGradOptimizationState s = (AdaGradOptimizationState) optimizationState;</span>

<span class="nc" id="L24">    double logLikelihoodChange = logLikelihood - s.lastLogLikelihood;</span>

<span class="nc bnc" id="L26" title="All 2 branches missed.">    if (logLikelihoodChange == 0) {</span>
<span class="nc bnc" id="L27" title="All 2 branches missed.">      if (!quiet) log.info(&quot;\tlogLikelihood improvement = 0: quitting&quot;);</span>
<span class="nc" id="L28">      return true;</span>
    }

    // Check if we should backtrack

<span class="nc bnc" id="L33" title="All 2 branches missed.">    else if (logLikelihoodChange &lt; 0) {</span>

      // If we should, move the weights back by half, and cut the lastDerivative by half

<span class="nc" id="L37">      s.lastDerivative.mapInPlace((d) -&gt; d / 2);</span>
<span class="nc" id="L38">      weights.addVectorInPlace(s.lastDerivative, -1.0);</span>

<span class="nc bnc" id="L40" title="All 2 branches missed.">      if (!quiet) log.info(&quot;\tBACKTRACK...&quot;);</span>

      // if the lastDerivative norm falls below a threshold, it means we've converged

<span class="nc bnc" id="L44" title="All 2 branches missed.">      if (s.lastDerivative.dotProduct(s.lastDerivative) &lt; 1.0e-10) {</span>
<span class="nc bnc" id="L45" title="All 2 branches missed.">        if (!quiet)</span>
<span class="nc" id="L46">          log.info(&quot;\tBacktracking derivative norm &quot; + s.lastDerivative.dotProduct(s.lastDerivative) + &quot; &lt; 1.0e-9: quitting&quot;);</span>
<span class="nc" id="L47">        return true;</span>
      }
    }

    // Apply AdaGrad

    else {
<span class="nc" id="L54">      ConcatVector squared = gradient.deepClone();</span>
<span class="nc" id="L55">      squared.mapInPlace((d) -&gt; d * d);</span>
<span class="nc" id="L56">      s.adagradAccumulator.addVectorInPlace(squared, 1.0);</span>

<span class="nc" id="L58">      ConcatVector sqrt = s.adagradAccumulator.deepClone();</span>
<span class="nc" id="L59">      sqrt.mapInPlace((d) -&gt; {</span>
<span class="nc bnc" id="L60" title="All 2 branches missed.">        if (d == 0) return alpha;</span>
<span class="nc" id="L61">        else return alpha / Math.sqrt(d);</span>
      });

<span class="nc" id="L64">      gradient.elementwiseProductInPlace(sqrt);</span>

<span class="nc" id="L66">      weights.addVectorInPlace(gradient, 1.0);</span>

      // Setup for backtracking, in case necessary

<span class="nc" id="L70">      s.lastDerivative = gradient;</span>
<span class="nc" id="L71">      s.lastLogLikelihood = logLikelihood;</span>

<span class="nc bnc" id="L73" title="All 2 branches missed.">      if (!quiet) log.info(&quot;\tLL: &quot; + logLikelihood);</span>
    }

<span class="nc" id="L76">    return false;</span>
  }

<span class="nc" id="L79">  protected class AdaGradOptimizationState extends OptimizationState {</span>
<span class="nc" id="L80">    ConcatVector lastDerivative = new ConcatVector(0);</span>
<span class="nc" id="L81">    ConcatVector adagradAccumulator = new ConcatVector(0);</span>
<span class="nc" id="L82">    double lastLogLikelihood = Double.NEGATIVE_INFINITY;</span>
  }

  @Override
  protected OptimizationState getFreshOptimizationState(ConcatVector initialWeights) {
<span class="nc" id="L87">    return new AdaGradOptimizationState();</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>