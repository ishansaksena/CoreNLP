<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>LuceneSentenceIndex.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.patterns</a> &gt; <span class="el_source">LuceneSentenceIndex.java</span></div><h1>LuceneSentenceIndex.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.patterns;

import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.patterns.surface.Token;
import edu.stanford.nlp.pipeline.CoreNLPProtos;
import edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer;
import edu.stanford.nlp.util.CollectionValuedMap;
import edu.stanford.nlp.util.LuceneFieldType;
import edu.stanford.nlp.util.ArgumentParser;
import edu.stanford.nlp.util.ArgumentParser.Option;
import edu.stanford.nlp.util.logging.Redwood;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.core.KeywordAnalyzer;
import org.apache.lucene.document.*;
import org.apache.lucene.index.*;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.search.*;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.Version;

import java.io.*;

import java.util.*;
import java.util.function.Function;



/**
 * To create a lucene inverted index from tokens to sentence ids.
 * (Right now it is not storing all core tokens although some functions might suggest that.)
 *
 * @author Sonal Gupta on 10/14/14.
 */
public class LuceneSentenceIndex&lt;E extends Pattern&gt; extends SentenceIndex&lt;E&gt; {

<span class="nc" id="L38">  @Option(name=&quot;saveTokens&quot;)</span>
  boolean saveTokens = false;

  IndexWriter indexWriter;
<span class="nc" id="L42">  File indexDir = null;</span>
  Directory dir;

<span class="nc" id="L45">  Analyzer analyzer = new KeywordAnalyzer();</span>
//  Analyzer analyzer = new Analyzer() {
//    @Override
//    protected TokenStreamComponents createComponents(String fieldName, Reader reader) {
//      Tokenizer source = new KeywordTokenizer(reader);
//      TokenStream result = new StopWordsFilter(source);
//      return new TokenStreamComponents(source, result);
//    }
//  };

//  public final class StopWordsFilter extends FilteringTokenFilter {
//    /**
//     * Build a filter that removes words that are too long or too
//     * short from the text.
//     */
//    public StopWordsFilter(TokenStream in) {
//      super(true, in);
//    }
//
//    @Override
//    public boolean accept() throws IOException {
//      return !stopWords.contains(input.toString().toLowerCase());
//    }
//  }

  //StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_42);
<span class="nc" id="L71">  IndexWriterConfig iwc=new IndexWriterConfig(Version.LUCENE_42, analyzer);</span>
<span class="nc" id="L72">  DirectoryReader reader = null;</span>
  IndexSearcher searcher;
<span class="nc" id="L74">  ProtobufAnnotationSerializer p = new ProtobufAnnotationSerializer();</span>

  //The fields in index are: tokens, sentence id, List&lt;CoreLabel&gt; annotation of the sentence (optional; not used when sentences are in memory)
  public LuceneSentenceIndex(Properties props, Set&lt;String&gt; stopWords, String indexDirStr, Function&lt;CoreLabel, Map&lt;String, String&gt;&gt; transformer) {
<span class="nc" id="L78">    super(stopWords, transformer);</span>
<span class="nc" id="L79">    ArgumentParser.fillOptions(this, props);</span>
<span class="nc" id="L80">    indexDir = new File(indexDirStr);</span>
<span class="nc" id="L81">  }</span>


  void setIndexReaderSearcher() throws IOException {
<span class="nc" id="L85">    FSDirectory index = FSDirectory.open(indexDir);</span>
<span class="nc bnc" id="L86" title="All 2 branches missed.">    if(reader == null){</span>
<span class="nc" id="L87">      reader = DirectoryReader.open(index);</span>
<span class="nc" id="L88">      searcher = new IndexSearcher(reader);</span>
    }else{
<span class="nc" id="L90">      DirectoryReader newreader = DirectoryReader.openIfChanged(reader);</span>
<span class="nc bnc" id="L91" title="All 2 branches missed.">      if(newreader != null) {</span>
<span class="nc" id="L92">        reader.close();</span>
<span class="nc" id="L93">        reader = newreader;</span>
<span class="nc" id="L94">        searcher = new IndexSearcher(reader);</span>
      }
    }
<span class="nc" id="L97">  }</span>

//  SentenceIndex.SentenceIteratorWithWords queryIndex(SurfacePattern pat){
//
//
//    String[] n = pat.getSimplerTokensNext();
//    String[] pr = pat.getSimplerTokensPrev();
//    boolean rest = false;
//    if(n!=null){
//      for(String e: n){
//        if(!specialWords.contains(e)){
//          rest = true;
//          break;
//        }
//      }
//    }
//    if(rest == false &amp;&amp; pr!=null){
//      for(String e: pr){
//        if(!specialWords.contains(e) &amp;&amp; !stopWords.contains(e)){
//          rest = true;
//          break;
//        }
//      }
//    }
//
//  }

  /**
   * give all sentences that have these words
   * **/
  Set&lt;String&gt; queryIndexGetSentences(CollectionValuedMap&lt;String, String&gt; words) throws IOException, ParseException {
<span class="nc" id="L128">    setIndexReaderSearcher();</span>
<span class="nc" id="L129">    BooleanQuery query = new BooleanQuery();</span>
<span class="nc" id="L130">    String pkey  = Token.getKeyForClass(PatternsAnnotations.ProcessedTextAnnotation.class);</span>

<span class="nc bnc" id="L132" title="All 2 branches missed.">    for(Map.Entry&lt;String, Collection&lt;String&gt;&gt; en: words.entrySet()){</span>
<span class="nc" id="L133">      boolean processedKey = en.getKey().equals(pkey);</span>
<span class="nc bnc" id="L134" title="All 2 branches missed.">      for(String en2: en.getValue()){</span>
<span class="nc bnc" id="L135" title="All 4 branches missed.">          if(!processedKey || !stopWords.contains(en2.toLowerCase()))</span>
<span class="nc" id="L136">            query.add(new BooleanClause(new TermQuery(new Term(en.getKey(), en2)), BooleanClause.Occur.MUST));</span>
<span class="nc" id="L137">      }</span>
<span class="nc" id="L138">    }</span>
    //query.add(new BooleanClause(new TermQuery(new Term(&quot;textannotation&quot;,&quot;sonal&quot;)), BooleanClause.Occur.MUST));

//    String queryStr = &quot;&quot;;
//    for(Map.Entry&lt;String, Collection&lt;String&gt;&gt; en: words.entrySet()){
//      for(String en2: en.getValue()){
//        queryStr+= &quot; &quot; + en.getKey() + &quot;:&quot;+en2;
//      }
//    }
//    QueryParser queryParser = new QueryParser(Version.LUCENE_42, &quot;sentence&quot;, analyzer);
//
//    queryParser.setDefaultOperator(QueryParser.Operator.AND);
//
//    Query query = queryParser.parse(queryStr);

    //Map&lt;String, List&lt;CoreLabel&gt;&gt; sents = null;
<span class="nc" id="L154">    TopDocs tp = searcher.search(query, Integer.MAX_VALUE);</span>
<span class="nc" id="L155">    Set&lt;String&gt; sentids = new HashSet&lt;&gt;();</span>
<span class="nc bnc" id="L156" title="All 2 branches missed.">    if (tp.totalHits &gt; 0) {</span>
<span class="nc bnc" id="L157" title="All 2 branches missed.">      for (ScoreDoc s : tp.scoreDocs) {</span>
<span class="nc" id="L158">        int docId = s.doc;</span>
<span class="nc" id="L159">        Document d = searcher.doc(docId);</span>
//        byte[] sent = d.getBinaryValue(&quot;tokens&quot;).bytes;
//        if(saveTokens) {
//          sents = new HashMap&lt;String, List&lt;CoreLabel&gt;&gt;();
//          List&lt;CoreLabel&gt; tokens = readProtoBufAnnotation(sent);
//          sents.put(d.get(&quot;sentid&quot;), tokens);
//        } else{
<span class="nc" id="L166">        sentids.add(d.get(&quot;sentid&quot;));</span>
        //}
      }
    } else {
<span class="nc" id="L170">      throw new RuntimeException(&quot;how come no documents for &quot; + words + &quot;. Query formed is &quot; + query);</span>
      //System.out.println(&quot;number of sentences for tokens &quot; + words + &quot; are &quot; + sentids);
//    if(!saveTokens){
//      sents = getSentences(sentids);
//    }
    }
<span class="nc" id="L176">    return sentids;</span>
  }


  @Override
  public void add(Map&lt;String, DataInstance&gt; sentences, boolean addProcessedText) {
    try {
<span class="nc" id="L183">      this.setIndexWriter();</span>

<span class="nc bnc" id="L185" title="All 2 branches missed.">      for(Map.Entry&lt;String, DataInstance&gt; en: sentences.entrySet()){</span>
        //String sentence = StringUtils.joinWords(en.getValue(), &quot; &quot;);
<span class="nc" id="L187">        add(en.getValue().getTokens(), en.getKey(), addProcessedText);</span>
<span class="nc" id="L188">      }</span>

<span class="nc" id="L190">      indexWriter.commit();</span>
<span class="nc" id="L191">      closeIndexWriter();</span>

<span class="nc" id="L193">    } catch (IOException e) {</span>
<span class="nc" id="L194">      throw new RuntimeException(e);</span>
<span class="nc" id="L195">    }</span>

<span class="nc" id="L197">  }</span>

  @Override
  public Map&lt;E, Set&lt;String&gt;&gt; queryIndex(Collection&lt;E&gt; patterns) {
    try{
<span class="nc" id="L202">      Map&lt;E, Set&lt;String&gt;&gt; sents = new HashMap&lt;&gt;();</span>
<span class="nc bnc" id="L203" title="All 2 branches missed.">      for(E p : patterns){</span>
<span class="nc" id="L204">        Set&lt;String&gt; sentids = queryIndexGetSentences(p.getRelevantWords());</span>
<span class="nc" id="L205">        sents.put(p, sentids);</span>
<span class="nc" id="L206">      }</span>
<span class="nc" id="L207">      return sents;</span>
    }
<span class="nc" id="L209">    catch (ParseException | IOException e) {</span>
<span class="nc" id="L210">      throw new RuntimeException(e);</span>
    }
  }



  public void listAllDocuments() throws IOException {
<span class="nc" id="L217">    setIndexReaderSearcher();</span>
<span class="nc bnc" id="L218" title="All 2 branches missed.">    for(int i = 0; i &lt; reader.numDocs(); i++){</span>
<span class="nc" id="L219">      Document d = searcher.doc(i);</span>
//      byte[] sent = d.getBinaryValue(&quot;tokens&quot;).bytes;
//      List&lt;CoreLabel&gt; tokens = readProtoBufAnnotation(sent);
<span class="nc" id="L222">      System.out.println(d.get(&quot;sentid&quot;));</span>
    }
<span class="nc" id="L224">  }</span>

  private List&lt;CoreLabel&gt; readProtoBufAnnotation(byte[] sent) throws IOException {
<span class="nc" id="L227">    ProtobufAnnotationSerializer p = new ProtobufAnnotationSerializer();</span>
<span class="nc" id="L228">    List&lt;CoreLabel&gt; toks = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L229">    ByteArrayInputStream is = new ByteArrayInputStream(sent);</span>
    CoreNLPProtos.Token d;
    do{
<span class="nc" id="L232">      d = CoreNLPProtos.Token.parseDelimitedFrom(is);</span>
<span class="nc bnc" id="L233" title="All 2 branches missed.">      if(d != null)</span>
<span class="nc" id="L234">        toks.add(p.fromProto(d));</span>
<span class="nc bnc" id="L235" title="All 2 branches missed.">    } while(d != null);</span>
<span class="nc" id="L236">    return toks;</span>
  }



  byte[] getProtoBufAnnotation(List&lt;CoreLabel&gt; tokens) throws IOException {
<span class="nc" id="L242">    ByteArrayOutputStream os = new ByteArrayOutputStream();</span>
<span class="nc bnc" id="L243" title="All 2 branches missed.">    for(CoreLabel token: tokens){</span>
<span class="nc" id="L244">      CoreNLPProtos.Token ptoken = p.toProto(token);</span>
<span class="nc" id="L245">      ptoken.writeDelimitedTo(os);</span>
<span class="nc" id="L246">    }</span>
<span class="nc" id="L247">    os.flush();</span>
<span class="nc" id="L248">    return os.toByteArray();</span>
  }

  @Override
  protected void add(List&lt;CoreLabel&gt; tokens, String sentid, boolean addProcessedText){
    try{
<span class="nc" id="L254">      setIndexWriter();</span>

<span class="nc" id="L256">      Document doc = new Document();</span>

<span class="nc bnc" id="L258" title="All 2 branches missed.">      for(CoreLabel l : tokens) {</span>
<span class="nc bnc" id="L259" title="All 2 branches missed.">        for (Map.Entry&lt;String, String&gt; en: transformCoreLabeltoString.apply(l).entrySet()) {</span>
<span class="nc" id="L260">          doc.add(new StringField(en.getKey(), en.getValue(), Field.Store.YES));//, ANALYZED));</span>
<span class="nc" id="L261">        }</span>
<span class="nc bnc" id="L262" title="All 2 branches missed.">        if(addProcessedText){</span>
<span class="nc" id="L263">          String ptxt = l.get(PatternsAnnotations.ProcessedTextAnnotation.class);</span>
<span class="nc bnc" id="L264" title="All 2 branches missed.">          if(!stopWords.contains(ptxt.toLowerCase()))</span>
<span class="nc" id="L265">            doc.add(new StringField(Token.getKeyForClass(PatternsAnnotations.ProcessedTextAnnotation.class), ptxt, Field.Store.YES));//, ANALYZED));</span>
        }
<span class="nc" id="L267">      }</span>
<span class="nc" id="L268">      doc.add(new StringField(&quot;sentid&quot;, sentid, Field.Store.YES));</span>


<span class="nc bnc" id="L271" title="All 4 branches missed.">      if(tokens!= null &amp;&amp; saveTokens)</span>
<span class="nc" id="L272">        doc.add(new Field(&quot;tokens&quot;, getProtoBufAnnotation(tokens), LuceneFieldType.NOT_INDEXED));</span>

<span class="nc" id="L274">      indexWriter.addDocument(doc);</span>

<span class="nc" id="L276">    }catch(IOException e){</span>
<span class="nc" id="L277">      throw new RuntimeException(e);</span>
<span class="nc" id="L278">    }</span>
<span class="nc" id="L279">  }</span>

  @Override
  public void finishUpdating() {
<span class="nc bnc" id="L283" title="All 2 branches missed.">    if(indexWriter != null){</span>
    try {
<span class="nc" id="L285">      indexWriter.commit();</span>
<span class="nc" id="L286">    } catch (IOException e) {</span>
<span class="nc" id="L287">      throw new RuntimeException(e);</span>
<span class="nc" id="L288">    }</span>
    }
<span class="nc" id="L290">    closeIndexWriter();</span>
<span class="nc" id="L291">  }</span>

  @Override
  public void update(List&lt;CoreLabel&gt; tokens, String sentid) {
    try {
<span class="nc" id="L296">      setIndexWriter();</span>
<span class="nc" id="L297">      indexWriter.deleteDocuments(new TermQuery(new Term(&quot;sentid&quot;,sentid)));</span>
<span class="nc" id="L298">      add(tokens, sentid, true);</span>
<span class="nc" id="L299">    } catch (IOException e) {</span>
<span class="nc" id="L300">      throw new RuntimeException(e);</span>
<span class="nc" id="L301">    }</span>

<span class="nc" id="L303">  }</span>

  void setIndexWriter()  {try{
<span class="nc bnc" id="L306" title="All 2 branches missed.">    if(indexWriter == null){</span>
<span class="nc" id="L307">      dir = FSDirectory.open(indexDir);</span>
<span class="nc" id="L308">      Redwood.log(Redwood.DBG, &quot;Updating lucene index at &quot; + indexDir);</span>
<span class="nc" id="L309">      indexWriter = new IndexWriter(dir, iwc);</span>
<span class="nc" id="L310">    }}catch(IOException e){</span>
<span class="nc" id="L311">    throw new RuntimeException(e);</span>
<span class="nc" id="L312">  }</span>
<span class="nc" id="L313">  }</span>

  void closeIndexWriter(){
    try {
<span class="nc bnc" id="L317" title="All 2 branches missed.">    if(indexWriter != null)</span>
<span class="nc" id="L318">        indexWriter.close();</span>
<span class="nc" id="L319">    indexWriter = null;</span>

<span class="nc bnc" id="L321" title="All 2 branches missed.">      if(dir != null)</span>
<span class="nc" id="L322">      dir.close();</span>
<span class="nc" id="L323">    } catch (IOException e) {</span>
<span class="nc" id="L324">      throw new RuntimeException(e);</span>
<span class="nc" id="L325">    }</span>
<span class="nc" id="L326">  }</span>

  @Override
  public void saveIndex(String dir){
<span class="nc bnc" id="L330" title="All 2 branches missed.">    if(!indexDir.toString().equals(dir)){</span>
      try {
<span class="nc" id="L332">        IOUtils.cp(indexDir, new File(dir), true);</span>
<span class="nc" id="L333">      } catch (IOException e) {</span>
<span class="nc" id="L334">        throw new RuntimeException(e);</span>
<span class="nc" id="L335">      }</span>
    }
<span class="nc" id="L337">  }</span>


  public static LuceneSentenceIndex createIndex(Map&lt;String, List&lt;CoreLabel&gt;&gt; sentences,  Properties props, Set&lt;String&gt; stopWords, String indexDiskDir, Function&lt;CoreLabel, Map&lt;String, String&gt;&gt; transformer) {
    try{
<span class="nc" id="L342">      LuceneSentenceIndex sentindex = new LuceneSentenceIndex(props, stopWords, indexDiskDir, transformer);</span>
<span class="nc" id="L343">      System.out.println(&quot;Creating lucene index at &quot; + indexDiskDir);</span>
<span class="nc" id="L344">      IOUtils.deleteDirRecursively(sentindex.indexDir);</span>
<span class="nc bnc" id="L345" title="All 2 branches missed.">      if(sentences!= null){</span>
<span class="nc" id="L346">        sentindex.setIndexWriter();</span>

<span class="nc" id="L348">        sentindex.add(sentences, true);</span>

<span class="nc" id="L350">        sentindex.closeIndexWriter();</span>
<span class="nc" id="L351">        sentindex.setIndexReaderSearcher();</span>
<span class="nc" id="L352">        System.out.println(&quot;Number of documents added are &quot; + sentindex.reader.numDocs());</span>
<span class="nc" id="L353">        sentindex.numAllSentences += sentindex.reader.numDocs();</span>
      }
<span class="nc" id="L355">      return sentindex;</span>
<span class="nc" id="L356">    }catch(IOException e){</span>
<span class="nc" id="L357">      throw new RuntimeException(e);</span>
    }
  }

  public static LuceneSentenceIndex loadIndex(Properties props, Set&lt;String&gt; stopwords, String dir,  Function&lt;CoreLabel, Map&lt;String, String&gt;&gt; transformSentenceToString) {
    try {
<span class="nc" id="L363">    LuceneSentenceIndex sentindex = new LuceneSentenceIndex(props, stopwords, dir, transformSentenceToString);</span>
<span class="nc" id="L364">    sentindex.setIndexReaderSearcher();</span>
<span class="nc" id="L365">    System.out.println(&quot;Number of documents read from the index &quot; + dir + &quot; are &quot; + sentindex.reader.numDocs());</span>
<span class="nc" id="L366">    sentindex.numAllSentences += sentindex.reader.numDocs();</span>
<span class="nc" id="L367">    return sentindex;</span>
<span class="nc" id="L368">    } catch (IOException e) {</span>
<span class="nc" id="L369">     throw new RuntimeException(e);</span>
    }
  }

}


</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>