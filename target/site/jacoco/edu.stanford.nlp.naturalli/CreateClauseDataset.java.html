<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>CreateClauseDataset.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.naturalli</a> &gt; <span class="el_source">CreateClauseDataset.java</span></div><h1>CreateClauseDataset.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.naturalli; 
import edu.stanford.nlp.util.logging.Redwood;

import edu.stanford.nlp.ie.machinereading.structure.Span;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.HasIndex;
import edu.stanford.nlp.ling.IndexedWord;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.process.TSVSentenceProcessor;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.semgraph.SemanticGraphEdge;
import edu.stanford.nlp.semgraph.semgrex.SemgrexMatcher;
import edu.stanford.nlp.semgraph.semgrex.SemgrexPattern;
import edu.stanford.nlp.trees.PennTreeReader;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeReader;
import edu.stanford.nlp.trees.UniversalEnglishGrammaticalStructureFactory;
import edu.stanford.nlp.util.*;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.text.DecimalFormat;
import java.util.*;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

import static edu.stanford.nlp.util.logging.Redwood.Util.*;

/**
 * A script to convert a TSV dump from our KBP sentences table into a Turk-task ready clause splitting dataset.
 *
 * @author Gabor Angeli
 */
<span class="nc bnc" id="L39" title="All 2 branches missed.">public class CreateClauseDataset implements TSVSentenceProcessor  {</span>

  /** A logger for this class */
<span class="nc" id="L42">  private static Redwood.RedwoodChannels log = Redwood.channels(CreateClauseDataset.class);</span>

  @ArgumentParser.Option(name=&quot;in&quot;, gloss=&quot;The input to read from&quot;)
<span class="nc" id="L45">  private static InputStream in = System.in;</span>

<span class="nc" id="L47">  public CreateClauseDataset() {</span>
<span class="nc" id="L48">  }</span>


  private static Span toSpan(List&lt;? extends HasIndex&gt; chunk) {
<span class="nc" id="L52">    int min = Integer.MAX_VALUE;</span>
<span class="nc" id="L53">    int max = -1;</span>
<span class="nc bnc" id="L54" title="All 2 branches missed.">    for (HasIndex word : chunk) {</span>
<span class="nc" id="L55">      min = Math.min(word.index() - 1, min);</span>
<span class="nc" id="L56">      max = Math.max(word.index(), max);</span>
<span class="nc" id="L57">    }</span>
<span class="nc bnc" id="L58" title="All 4 branches missed.">    assert min &gt;= 0;</span>
<span class="nc bnc" id="L59" title="All 6 branches missed.">    assert max &lt; Integer.MAX_VALUE &amp;&amp; max &gt; 0;</span>
<span class="nc" id="L60">    return new Span(min, max);</span>
  }

  @Override
  public void process(long id, Annotation doc) {
<span class="nc" id="L65">    CoreMap sentence = doc.get(CoreAnnotations.SentencesAnnotation.class).get(0);</span>
<span class="nc" id="L66">    SemanticGraph depparse = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);</span>
<span class="nc" id="L67">    log.info(&quot;| &quot; + sentence.get(CoreAnnotations.TextAnnotation.class));</span>

    // Get all valid subject spans
<span class="nc" id="L70">    BitSet consumedAsSubjects = new BitSet();</span>
<span class="nc" id="L71">    @SuppressWarnings(&quot;MismatchedQueryAndUpdateOfCollection&quot;) List&lt;Span&gt; subjectSpans = new ArrayList&lt;&gt;();</span>
<span class="nc bnc" id="L72" title="All 2 branches missed.">    NEXTNODE: for (IndexedWord head : depparse.topologicalSort()) {</span>
      // Check if the node is a noun/pronoun
<span class="nc bnc" id="L74" title="All 4 branches missed.">      if (head.tag().startsWith(&quot;N&quot;) || head.tag().equals(&quot;PRP&quot;)) {</span>
        // Try to get the NP chunk
<span class="nc" id="L76">        Optional&lt;List&lt;IndexedWord&gt;&gt; subjectChunk = segmenter.getValidChunk(depparse, head, segmenter.VALID_SUBJECT_ARCS, Optional.empty(), true);</span>
<span class="nc bnc" id="L77" title="All 2 branches missed.">        if (subjectChunk.isPresent()) {</span>
          // Make sure it's not already a member of a larger NP
<span class="nc bnc" id="L79" title="All 2 branches missed.">          for (IndexedWord tok : subjectChunk.get()) {</span>
<span class="nc bnc" id="L80" title="All 2 branches missed.">            if (consumedAsSubjects.get(tok.index())) {</span>
<span class="nc" id="L81">              continue NEXTNODE;  // Already considered. Continue to the next node.</span>
            }
<span class="nc" id="L83">          }</span>
          // Register it as an NP
<span class="nc bnc" id="L85" title="All 2 branches missed.">          for (IndexedWord tok : subjectChunk.get()) {</span>
<span class="nc" id="L86">            consumedAsSubjects.set(tok.index());</span>
<span class="nc" id="L87">          }</span>
          // Add it as a subject
<span class="nc" id="L89">          subjectSpans.add(toSpan(subjectChunk.get()));</span>
        }
      }
<span class="nc" id="L92">    }</span>
<span class="nc" id="L93">  }</span>


  /**
   * The pattern for traces which are potential subjects
   */
<span class="nc" id="L99">  private static Pattern TRACE_TARGET_PATTERN = Pattern.compile(&quot;(NP-.*)-([0-9]+)&quot;);</span>

  /**
   * The pattern for trace markers.
   */
<span class="nc" id="L104">  private static Pattern TRACE_SOURCE_PATTERN = Pattern.compile(&quot;.*\\*-([0-9]+)&quot;);</span>

  /**
   * The converter from constituency to dependency trees.
   */
<span class="nc" id="L109">  private static UniversalEnglishGrammaticalStructureFactory parser = new UniversalEnglishGrammaticalStructureFactory();</span>

  /**
   * The OpenIE segmenter to use.
   */
<span class="nc" id="L114">  private static RelationTripleSegmenter segmenter = new RelationTripleSegmenter();</span>

  /**
   * The natural logic annotator for marking polarity.
   */
<span class="nc" id="L119">  private static NaturalLogicAnnotator natlog = new NaturalLogicAnnotator();</span>

  /**
   * Parse a given constituency tree into a dependency graph.
   *
   * @param tree The constituency tree, in Penn Treebank style.
   * @return The dependency graph for the tree.
   */
  private static SemanticGraph parse(Tree tree) {
<span class="nc" id="L128">    return new SemanticGraph(parser.newGrammaticalStructure(tree).typedDependenciesCollapsed());</span>
  }

  /**
   * Create a dataset of subject/object pairs, such that a sequence of splits that segments this
   * subject and object is a correct sequence.
   *
   * @param depparse The dependency parse of the sentence.
   * @param traceTargets The set of spans corresponding to targets of traces.
   * @param traceSources The set of indices in a sentence corresponding to the sources of traces.
   * @return A dataset of subject/object spans.
   */
  @SuppressWarnings(&quot;UnusedParameters&quot;)
  private static Collection&lt;Pair&lt;Span, Span&gt;&gt; subjectObjectPairs(SemanticGraph depparse,
                                                                 List&lt;CoreLabel&gt; tokens,
                                                                 Map&lt;Integer, Span&gt; traceTargets,
                                                                 Map&lt;Integer, Integer&gt; traceSources) {
//    log(StringUtils.join(tokens.stream().map(CoreLabel::word), &quot; &quot;));
<span class="nc" id="L146">    List&lt;Pair&lt;Span, Span&gt;&gt; data = new ArrayList&lt;&gt;();</span>
<span class="nc bnc" id="L147" title="All 2 branches missed.">    for (SemgrexPattern vpPattern : segmenter.VP_PATTERNS) {</span>
<span class="nc" id="L148">      SemgrexMatcher matcher = vpPattern.matcher(depparse);</span>
<span class="nc bnc" id="L149" title="All 2 branches missed.">      while (matcher.find()) {</span>
        // Get the verb and object
<span class="nc" id="L151">        IndexedWord verb = matcher.getNode(&quot;verb&quot;);</span>
<span class="nc" id="L152">        IndexedWord object = matcher.getNode(&quot;object&quot;);</span>
<span class="nc bnc" id="L153" title="All 4 branches missed.">        if (verb != null &amp;&amp; object != null) {</span>
          // See if there is already a subject attached
<span class="nc" id="L155">          boolean hasSubject = false;</span>
<span class="nc bnc" id="L156" title="All 2 branches missed.">          for (SemanticGraphEdge edge : depparse.outgoingEdgeIterable(verb)) {</span>
<span class="nc bnc" id="L157" title="All 2 branches missed.">            if (edge.getRelation().toString().contains(&quot;subj&quot;)) {</span>
<span class="nc" id="L158">              hasSubject = true;</span>
            }
<span class="nc" id="L160">          }</span>
<span class="nc bnc" id="L161" title="All 2 branches missed.">          for (SemanticGraphEdge edge : depparse.outgoingEdgeIterable(object)) {</span>
<span class="nc bnc" id="L162" title="All 2 branches missed.">            if (edge.getRelation().toString().contains(&quot;subj&quot;)) {</span>
<span class="nc" id="L163">              hasSubject = true;</span>
            }
<span class="nc" id="L165">          }</span>
<span class="nc bnc" id="L166" title="All 2 branches missed.">          if (!hasSubject) {</span>
            // Get the spans for the verb and object
<span class="nc" id="L168">            Optional&lt;List&lt;IndexedWord&gt;&gt; verbChunk = segmenter.getValidChunk(depparse, verb, segmenter.VALID_ADVERB_ARCS, Optional.empty(), true);</span>
<span class="nc" id="L169">            Optional&lt;List&lt;IndexedWord&gt;&gt; objectChunk = segmenter.getValidChunk(depparse, object, segmenter.VALID_OBJECT_ARCS, Optional.empty(), true);</span>
<span class="nc bnc" id="L170" title="All 4 branches missed.">            if (verbChunk.isPresent() &amp;&amp; objectChunk.isPresent()) {</span>
<span class="nc" id="L171">              Collections.sort(verbChunk.get(), (a, b) -&gt; a.index() - b.index());</span>
<span class="nc" id="L172">              Collections.sort(objectChunk.get(), (a, b) -&gt; a.index() - b.index());</span>
              // Find a trace
<span class="nc" id="L174">              int traceId = -1;</span>
<span class="nc" id="L175">              Span verbSpan = toSpan(verbChunk.get());</span>
<span class="nc" id="L176">              Span traceSpan = Span.fromValues(verbSpan.start() - 1, verbSpan.end() + 1);</span>
<span class="nc bnc" id="L177" title="All 2 branches missed.">              for (Map.Entry&lt;Integer, Integer&gt; entry : traceSources.entrySet()) {</span>
<span class="nc bnc" id="L178" title="All 2 branches missed.">                if (traceSpan.contains(entry.getValue())) {</span>
<span class="nc" id="L179">                  traceId = entry.getKey();</span>
                }
<span class="nc" id="L181">              }</span>
              //noinspection StatementWithEmptyBody
<span class="nc bnc" id="L183" title="All 2 branches missed.">              if (traceId &lt; 0) {</span>
                // Register the VP as an unknown VP
//                List&lt;CoreLabel&gt; vpChunk = new ArrayList&lt;&gt;();
//                vpChunk.addAll(verbChunk.get());
//                vpChunk.addAll(objectChunk.get());
//                Collections.sort(vpChunk, (a, b) -&gt; a.index() - b.index());
//                debug(&quot;could not find trace for &quot; + vpChunk);
              } else {
                // Add the obj chunk
<span class="nc" id="L192">                Span subjectSpan = traceTargets.get(traceId);</span>
<span class="nc" id="L193">                Span objectSpan = toSpan(objectChunk.get());</span>
<span class="nc bnc" id="L194" title="All 2 branches missed.">                if (subjectSpan != null) {</span>
//                  debug(&quot;(&quot; +
//                      StringUtils.join(tokens.subList(subjectSpan.start(), subjectSpan.end()).stream().map(CoreLabel::word), &quot; &quot;) + &quot;; &quot; +
//                      verb.word() + &quot;; &quot; +
//                      StringUtils.join(tokens.subList(objectSpan.start(), objectSpan.end()).stream().map(CoreLabel::word), &quot; &quot;) +
//                      &quot;)&quot;);
<span class="nc" id="L200">                  data.add(Pair.makePair(subjectSpan, objectSpan));</span>
                }
              }
            }
          }
        }
<span class="nc" id="L206">      }</span>
<span class="nc" id="L207">    }</span>

    // Run vanilla pattern splits
<span class="nc bnc" id="L210" title="All 2 branches missed.">    for (SemgrexPattern vpPattern : segmenter.VERB_PATTERNS) {</span>
<span class="nc" id="L211">      SemgrexMatcher matcher = vpPattern.matcher(depparse);</span>
<span class="nc bnc" id="L212" title="All 2 branches missed.">      while (matcher.find()) {</span>
        // Get the verb and object
<span class="nc" id="L214">        IndexedWord subject = matcher.getNode(&quot;subject&quot;);</span>
<span class="nc" id="L215">        IndexedWord object = matcher.getNode(&quot;object&quot;);</span>
<span class="nc bnc" id="L216" title="All 4 branches missed.">        if (subject != null &amp;&amp; object != null) {</span>
<span class="nc" id="L217">          Optional&lt;List&lt;IndexedWord&gt;&gt; subjectChunk = segmenter.getValidChunk(depparse, subject, segmenter.VALID_SUBJECT_ARCS, Optional.empty(), true);</span>
<span class="nc" id="L218">          Optional&lt;List&lt;IndexedWord&gt;&gt; objectChunk = segmenter.getValidChunk(depparse, object, segmenter.VALID_OBJECT_ARCS, Optional.empty(), true);</span>
<span class="nc bnc" id="L219" title="All 4 branches missed.">          if (subjectChunk.isPresent() &amp;&amp; objectChunk.isPresent()) {</span>
<span class="nc" id="L220">            Span subjectSpan = toSpan(subjectChunk.get());</span>
<span class="nc" id="L221">            Span objectSpan = toSpan(objectChunk.get());</span>
<span class="nc" id="L222">            data.add(Pair.makePair(subjectSpan, objectSpan));</span>
          }
        }
<span class="nc" id="L225">      }</span>
<span class="nc" id="L226">    }</span>

<span class="nc" id="L228">    return data;</span>
  }

  /**
   * Collect all the possible targets for traces. This is limited to NP-style traces.
   *
   * @param root The tree to search in. This is a recursive function.
   * @return The set of trace targets. The key is the id of the trace, the value is the span of the target of the trace.
   */
  private static Map&lt;Integer, Span&gt; findTraceTargets(Tree root) {
<span class="nc" id="L238">    Map&lt;Integer, Span&gt; spansInTree = new HashMap&lt;&gt;(4);</span>

<span class="nc bnc" id="L240" title="All 2 branches missed.">    Matcher m = TRACE_TARGET_PATTERN.matcher(root.label().value() == null ? &quot;NULL&quot; : root.label().value());</span>
<span class="nc bnc" id="L241" title="All 2 branches missed.">    if (m.matches()) {</span>
<span class="nc" id="L242">      int index = Integer.parseInt(m.group(2));</span>
<span class="nc" id="L243">      spansInTree.put(index, Span.fromPair(root.getSpan()).toExclusive());</span>
    }
<span class="nc bnc" id="L245" title="All 2 branches missed.">    for (Tree child : root.children()) {</span>
<span class="nc" id="L246">      spansInTree.putAll(findTraceTargets(child));</span>
    }
<span class="nc" id="L248">    return spansInTree;</span>
  }

  /**
   * Collect all the trace markers in the sentence.
   *
   * @param root The tree to search in. This is a recursive function.
   * @return A map of trace sources. The key is hte id of the trace, the value is the index of the trace's source in the sentence.
   */
  private static Map&lt;Integer, Integer&gt; findTraceSources(Tree root) {
<span class="nc" id="L258">    Map&lt;Integer, Integer&gt; spansInTree = new HashMap&lt;&gt;(4);</span>

<span class="nc bnc" id="L260" title="All 2 branches missed.">    Matcher m = TRACE_SOURCE_PATTERN.matcher(root.label().value() == null ? &quot;NULL&quot; : root.label().value());</span>
<span class="nc bnc" id="L261" title="All 2 branches missed.">    if (m.matches()) {</span>
<span class="nc" id="L262">      int index = Integer.parseInt(m.group(1));</span>
<span class="nc" id="L263">      spansInTree.put(index, ((CoreLabel) root.label()).index() - 1);</span>
    }
<span class="nc bnc" id="L265" title="All 2 branches missed.">    for (Tree child : root.children()) {</span>
<span class="nc" id="L266">      spansInTree.putAll(findTraceSources(child));</span>
    }
<span class="nc" id="L268">    return spansInTree;</span>
  }

  /**
   * Count the number of extractions in the given dataset. That is, the sum count of the pair spans
   * for each sentence.
   *
   * @param data The dataset.
   * @return The number of extractions in the datasets..
   */
  private static int countDatums(List&lt;Pair&lt;CoreMap, Collection&lt;Pair&lt;Span,Span&gt;&gt;&gt;&gt; data) {
<span class="nc" id="L279">    int count = 0;</span>
<span class="nc bnc" id="L280" title="All 2 branches missed.">    for (Pair&lt;CoreMap, Collection&lt;Pair&lt;Span, Span&gt;&gt;&gt; datum : data) {</span>
<span class="nc" id="L281">      count += datum.second.size();</span>
<span class="nc" id="L282">    }</span>
<span class="nc" id="L283">    return count;</span>
  }

  /**
   * Process all the trees in the given directory. For example, the WSJ section of the Penn Treebank.
   *
   * @param name The name of the directory we are processing.
   * @param directory The directory we are processing.
   * @return A dataset of subject/object pairs in the trees in the directory.
   *         This is a list of sentences, such that each sentence has a collection of pairs of spans.
   *         Each pair of spans is a subject/object span pair that constitutes a valid extraction.
   * @throws IOException
   */
  private static List&lt;Pair&lt;CoreMap, Collection&lt;Pair&lt;Span, Span&gt;&gt;&gt;&gt; processDirectory(String name, File directory) throws IOException {
<span class="nc" id="L297">    forceTrack(&quot;Processing &quot; + name);</span>

    // Prepare the files to iterate over
<span class="nc" id="L300">    Iterable&lt;File&gt; files = IOUtils.iterFilesRecursive(directory, &quot;mrg&quot;);</span>
    Tree tree;
<span class="nc" id="L302">    int numTreesProcessed = 0;</span>
<span class="nc" id="L303">    List&lt;Pair&lt;CoreMap, Collection&lt;Pair&lt;Span, Span&gt;&gt;&gt;&gt; trainingData = new ArrayList&lt;&gt;(1024);</span>

    // Iterate over the files
<span class="nc bnc" id="L306" title="All 2 branches missed.">    for (File file : files) {</span>
//      log(file);
<span class="nc" id="L308">      TreeReader reader = new PennTreeReader(IOUtils.readerFromFile(file));</span>
<span class="nc bnc" id="L309" title="All 2 branches missed.">      while ( (tree = reader.readTree()) != null ) {</span>
        try {
          // Prepare the tree
<span class="nc" id="L312">          tree.indexSpans();</span>
<span class="nc" id="L313">          tree.setSpans();</span>

          // Get relevant information from sentence
<span class="nc" id="L316">          List&lt;CoreLabel&gt; tokens = tree.getLeaves().stream()</span>
<span class="nc" id="L317">              .map(leaf -&gt; (CoreLabel) leaf.label())</span>
//            .filter(leaf -&gt; !TRACE_SOURCE_PATTERN.matcher(leaf.word()).matches() &amp;&amp; !leaf.tag().equals(&quot;-NONE-&quot;))
<span class="nc" id="L319">              .collect(Collectors.toList());</span>
<span class="nc" id="L320">          SemanticGraph graph = parse(tree);</span>
<span class="nc" id="L321">          Map&lt;Integer, Span&gt; targets = findTraceTargets(tree);</span>
<span class="nc" id="L322">          Map&lt;Integer, Integer&gt; sources = findTraceSources(tree);</span>

          // Create a sentence object
<span class="nc" id="L325">          CoreMap sentence = new ArrayCoreMap(4) {{</span>
<span class="nc" id="L326">            set(CoreAnnotations.TokensAnnotation.class, tokens);</span>
<span class="nc" id="L327">            set(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class, graph);</span>
<span class="nc" id="L328">            set(SemanticGraphCoreAnnotations.EnhancedDependenciesAnnotation.class, graph);</span>
<span class="nc" id="L329">            set(SemanticGraphCoreAnnotations.EnhancedPlusPlusDependenciesAnnotation.class, graph);</span>
<span class="nc" id="L330">          }};</span>
<span class="nc" id="L331">          natlog.doOneSentence(null, sentence);</span>

          // Generate training data
<span class="nc" id="L334">          Collection&lt;Pair&lt;Span, Span&gt;&gt; trainingDataFromSentence = subjectObjectPairs(graph, tokens, targets, sources);</span>
<span class="nc" id="L335">          trainingData.add(Pair.makePair(sentence, trainingDataFromSentence));</span>

          // Debug print
<span class="nc" id="L338">          numTreesProcessed += 1;</span>
<span class="nc bnc" id="L339" title="All 2 branches missed.">          if (numTreesProcessed % 100 == 0) {</span>
<span class="nc" id="L340">            log(&quot;[&quot; + new DecimalFormat(&quot;00000&quot;).format(numTreesProcessed) + &quot;] &quot; + countDatums(trainingData) + &quot; known extractions&quot;);</span>
          }
<span class="nc" id="L342">        } catch (Throwable t) {</span>
<span class="nc" id="L343">          t.printStackTrace();</span>
<span class="nc" id="L344">        }</span>
      }
<span class="nc" id="L346">    }</span>

    // End
<span class="nc" id="L349">    log(&quot;&quot; + numTreesProcessed + &quot; trees processed yielding &quot; + countDatums(trainingData) + &quot; known extractions&quot;);</span>
<span class="nc" id="L350">    endTrack(&quot;Processing &quot; + name);</span>
<span class="nc" id="L351">    return trainingData;</span>
  }


  /**
   * The main entry point of the code.
   */
  public static void main(String[] args) throws IOException {
<span class="nc" id="L359">    forceTrack(&quot;Processing treebanks&quot;);</span>
<span class="nc" id="L360">    List&lt;Pair&lt;CoreMap, Collection&lt;Pair&lt;Span, Span&gt;&gt;&gt;&gt; trainingData = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L361">    trainingData.addAll(processDirectory(&quot;WSJ&quot;, new File(&quot;/home/gabor/lib/data/penn_treebank/wsj&quot;)));</span>
<span class="nc" id="L362">    trainingData.addAll(processDirectory(&quot;Brown&quot;, new File(&quot;/home/gabor/lib/data/penn_treebank/brown&quot;)));</span>
<span class="nc" id="L363">    endTrack(&quot;Processing treebanks&quot;);</span>

<span class="nc" id="L365">    forceTrack(&quot;Training&quot;);</span>
<span class="nc" id="L366">    log(&quot;dataset size: &quot; + trainingData.size());</span>
<span class="nc" id="L367">    ClauseSplitter.train(</span>
<span class="nc" id="L368">        trainingData.stream(),</span>
        new File(&quot;/home/gabor/tmp/clauseSearcher.ser.gz&quot;),
        new File(&quot;/home/gabor/tmp/clauseSearcherData.tab.gz&quot;));
<span class="nc" id="L371">    endTrack(&quot;Training&quot;);</span>




//    Execution.fillOptions(CreateClauseDataset.class, args);
//
//    new CreateClauseDataset().runAndExit(in, System.err, code -&gt; code);
<span class="nc" id="L379">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>