<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>GeneralizedExpectationObjectiveFunction.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.classify</a> &gt; <span class="el_source">GeneralizedExpectationObjectiveFunction.java</span></div><h1>GeneralizedExpectationObjectiveFunction.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.classify;

import edu.stanford.nlp.ling.Datum;
import edu.stanford.nlp.ling.RVFDatum;
import edu.stanford.nlp.math.ArrayMath;
import edu.stanford.nlp.optimization.AbstractCachingDiffFunction;
import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.stats.Counter;
import edu.stanford.nlp.util.Generics;
import edu.stanford.nlp.util.Triple;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.Set;


/**
 * Implementation of Generalized Expectation Objective function for
 * an I.I.D. log-linear model. See Mann and McCallum, ACL 2008 for GE in CRFs.
 * This code, however, is just for log-linear model
 * IMPORTANT: the current implementation is only correct as long as
 * the labeled features passed to GE are binary.
 * However, other features are allowed to be real valued.
 * The original paper also discusses GE only for binary features.
 *
 * @author Ramesh Nallapati (nmramesh@cs.stanford.edu)
 */

public class GeneralizedExpectationObjectiveFunction&lt;L,F&gt; extends AbstractCachingDiffFunction {

  private final GeneralDataset&lt;L,F&gt; labeledDataset;
  private final List&lt;? extends Datum&lt;L,F&gt;&gt; unlabeledDataList;
  private final List&lt;F&gt; geFeatures;
  private final LinearClassifier&lt;L,F&gt; classifier;
  private double[][] geFeature2EmpiricalDist; //empirical label distributions of each feature. Really final but java won't let us.
  private List&lt;List&lt;Integer&gt;&gt; geFeature2DatumList; //an inverted list of active unlabeled documents for each feature. Really final but java won't let us.

  private final int numFeatures;
  private final int numClasses;


  @Override
  public int domainDimension() {
<span class="nc" id="L46">    return numFeatures * numClasses;</span>
  }

  int classOf(int index) {
<span class="nc" id="L50">    return index % numClasses;</span>
  }

  int featureOf(int index) {
<span class="nc" id="L54">    return index / numClasses;</span>
  }

  protected int indexOf(int f, int c) {
<span class="nc" id="L58">    return f * numClasses + c;</span>
  }

  public double[][] to2D(double[] x) {
<span class="nc" id="L62">    double[][] x2 = new double[numFeatures][numClasses];</span>
<span class="nc bnc" id="L63" title="All 2 branches missed.">    for (int i = 0; i &lt; numFeatures; i++) {</span>
<span class="nc bnc" id="L64" title="All 2 branches missed.">      for (int j = 0; j &lt; numClasses; j++) {</span>
<span class="nc" id="L65">        x2[i][j] = x[indexOf(i, j)];</span>
      }
    }
<span class="nc" id="L68">    return x2;</span>
  }

  @Override
  protected void calculate(double[] x) {
<span class="nc" id="L73">    classifier.setWeights(to2D(x));</span>
<span class="nc bnc" id="L74" title="All 2 branches missed.">    if (derivative == null) {</span>
<span class="nc" id="L75">      derivative = new double[x.length];</span>
    } else {
<span class="nc" id="L77">      Arrays.fill(derivative, 0.0);</span>
    }
<span class="nc" id="L79">    Counter&lt;Triple&lt;Integer,Integer,Integer&gt;&gt; feature2classPairDerivatives = new ClassicCounter&lt;&gt;();</span>

<span class="nc" id="L81">    value = 0.0;</span>
<span class="nc bnc" id="L82" title="All 2 branches missed.">    for(int n = 0; n &lt; geFeatures.size(); n++){</span>
      //F feature = geFeatures.get(n);
<span class="nc" id="L84">      double[] modelDist = new double[numClasses];</span>
<span class="nc" id="L85">      Arrays.fill(modelDist,0);</span>

    //go over the unlabeled active data to compute expectations
<span class="nc" id="L88">      List&lt;Integer&gt; activeData = geFeature2DatumList.get(n);</span>
<span class="nc bnc" id="L89" title="All 2 branches missed.">      for (Integer activeDatum : activeData) {</span>
<span class="nc" id="L90">        Datum&lt;L, F&gt; datum = unlabeledDataList.get(activeDatum);</span>
<span class="nc" id="L91">        double[] probs = getModelProbs(datum);</span>
<span class="nc bnc" id="L92" title="All 2 branches missed.">        for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L93">          modelDist[c] += probs[c];</span>
        }
<span class="nc" id="L95">        updateDerivative(datum, probs, feature2classPairDerivatives); //computes p(y_d)*(1-p(y_d))*f_d for all active features.</span>
<span class="nc" id="L96">      }</span>

      //now  compute the value (KL-divergence) and the final value of the derivative.
<span class="nc bnc" id="L99" title="All 2 branches missed.">      if (activeData.size()&gt;0) {</span>
<span class="nc bnc" id="L100" title="All 2 branches missed.">        for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L101">          modelDist[c]/= activeData.size();</span>
        }
<span class="nc" id="L103">        smoothDistribution(modelDist);</span>

<span class="nc bnc" id="L105" title="All 2 branches missed.">        for(int c = 0; c &lt; numClasses; c++)</span>
<span class="nc" id="L106">          value += -geFeature2EmpiricalDist[n][c]*Math.log(modelDist[c]);</span>

<span class="nc bnc" id="L108" title="All 2 branches missed.">        for(int f = 0; f &lt; labeledDataset.featureIndex().size(); f++) {</span>
<span class="nc bnc" id="L109" title="All 2 branches missed.">          for(int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L110">            int wtIndex = indexOf(f,c);</span>
<span class="nc bnc" id="L111" title="All 2 branches missed.">            for(int cPrime = 0;  cPrime &lt; numClasses; cPrime++){</span>
<span class="nc" id="L112">              derivative[wtIndex] += feature2classPairDerivatives.getCount(new Triple&lt;&gt;(f, c, cPrime))*geFeature2EmpiricalDist[n][cPrime]/modelDist[cPrime];</span>
            }
<span class="nc" id="L114">            derivative[wtIndex] /= activeData.size();</span>
          }
        } // loop over each feature for derivative computation
      } //end of if condition
    } //loop over each GE feature
<span class="nc" id="L119">  }</span>


   private void updateDerivative(Datum&lt;L,F&gt; datum, double[] probs,Counter&lt;Triple&lt;Integer,Integer,Integer&gt;&gt; feature2classPairDerivatives){
<span class="nc bnc" id="L123" title="All 2 branches missed.">     for (F feature : datum.asFeatures()) {</span>
<span class="nc" id="L124">       int fID = labeledDataset.featureIndex.indexOf(feature);</span>
<span class="nc bnc" id="L125" title="All 2 branches missed.">       if (fID &gt;= 0) {</span>
<span class="nc bnc" id="L126" title="All 2 branches missed.">         for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc bnc" id="L127" title="All 2 branches missed.">           for (int cPrime = 0; cPrime &lt; numClasses; cPrime++) {</span>
<span class="nc bnc" id="L128" title="All 2 branches missed.">             if (cPrime == c) {</span>
<span class="nc" id="L129">               feature2classPairDerivatives.incrementCount(new Triple&lt;&gt;(fID, c, cPrime), - probs[c]*(1-probs[c])*valueOfFeature(feature,datum));</span>
             } else {
<span class="nc" id="L131">               feature2classPairDerivatives.incrementCount(new Triple&lt;&gt;(fID, c, cPrime), probs[c]*probs[cPrime]*valueOfFeature(feature,datum));</span>
             }
           }
         }
       }
<span class="nc" id="L136">     }</span>
<span class="nc" id="L137">   }</span>

   /*
    * This method assumes the feature already exists in the datum.
    */
   private double valueOfFeature(F feature, Datum&lt;L,F&gt; datum){
<span class="nc bnc" id="L143" title="All 2 branches missed.">      if(datum instanceof RVFDatum)</span>
<span class="nc" id="L144">        return ((RVFDatum&lt;L,F&gt;)datum).asFeaturesCounter().getCount(feature);</span>
<span class="nc" id="L145">      else return 1.0;</span>
    }

    private void computeEmpiricalStatistics(List&lt;F&gt; geFeatures){
      //allocate memory to the containers and initialize them
<span class="nc" id="L150">      geFeature2EmpiricalDist = new double[geFeatures.size()][labeledDataset.labelIndex.size()];</span>
<span class="nc" id="L151">      geFeature2DatumList = new ArrayList&lt;&gt;(geFeatures.size());</span>
<span class="nc" id="L152">      Map&lt;F,Integer&gt; geFeatureMap = Generics.newHashMap();</span>
<span class="nc" id="L153">      Set&lt;Integer&gt; activeUnlabeledExamples = Generics.newHashSet();</span>
<span class="nc bnc" id="L154" title="All 2 branches missed.">      for(int n = 0; n &lt; geFeatures.size(); n++){</span>
<span class="nc" id="L155">        F geFeature = geFeatures.get(n);</span>
<span class="nc" id="L156">        geFeature2DatumList.add(new ArrayList&lt;&gt;());</span>
<span class="nc" id="L157">        Arrays.fill(geFeature2EmpiricalDist[n], 0);</span>
<span class="nc" id="L158">        geFeatureMap.put(geFeature,n);</span>
      }

      //compute the empirical label distribution for each GE feature
<span class="nc bnc" id="L162" title="All 2 branches missed.">      for(int i = 0; i &lt; labeledDataset.size(); i++){</span>
<span class="nc" id="L163">        Datum&lt;L,F&gt; datum = labeledDataset.getDatum(i);</span>
<span class="nc" id="L164">        int labelID = labeledDataset.labelIndex.indexOf(datum.label());</span>
<span class="nc bnc" id="L165" title="All 2 branches missed.">        for(F feature : datum.asFeatures()){</span>
<span class="nc bnc" id="L166" title="All 2 branches missed.">          if(geFeatureMap.containsKey(feature)){</span>
<span class="nc" id="L167">            int geFnum = geFeatureMap.get(feature);</span>
<span class="nc" id="L168">            geFeature2EmpiricalDist[geFnum][labelID]++;</span>
          }
<span class="nc" id="L170">        }</span>
      }
      //now normalize and smooth the label distribution for each feature.
<span class="nc bnc" id="L173" title="All 2 branches missed.">      for(int n = 0;  n &lt; geFeatures.size(); n++){</span>
<span class="nc" id="L174">        ArrayMath.normalize(geFeature2EmpiricalDist[n]);</span>
<span class="nc" id="L175">        smoothDistribution(geFeature2EmpiricalDist[n]);</span>
      }

      //now build the inverted index from each GE feature to unlabeled datums that contain it.
<span class="nc bnc" id="L179" title="All 2 branches missed.">      for (int i = 0; i &lt; unlabeledDataList.size(); i++) {</span>
<span class="nc" id="L180">        Datum&lt;L,F&gt; datum = unlabeledDataList.get(i);</span>
<span class="nc bnc" id="L181" title="All 2 branches missed.">        for (F feature : datum.asFeatures()) {</span>
<span class="nc bnc" id="L182" title="All 2 branches missed.">          if (geFeatureMap.containsKey(feature)) {</span>
<span class="nc" id="L183">            int geFnum = geFeatureMap.get(feature);</span>
<span class="nc" id="L184">            geFeature2DatumList.get(geFnum).add(i);</span>
<span class="nc" id="L185">            activeUnlabeledExamples.add(i);</span>
          }
<span class="nc" id="L187">        }</span>
      }
<span class="nc" id="L189">      System.out.println(&quot;Number of active unlabeled examples:&quot;+activeUnlabeledExamples.size());</span>
<span class="nc" id="L190">    }</span>

    private static void smoothDistribution(double [] dist) {
      //perform Laplace smoothing
<span class="nc" id="L194">      double epsilon = 1e-6;</span>
<span class="nc bnc" id="L195" title="All 2 branches missed.">      for(int i = 0; i &lt; dist.length; i++)</span>
<span class="nc" id="L196">        dist[i] += epsilon;</span>
<span class="nc" id="L197">      ArrayMath.normalize(dist);</span>
<span class="nc" id="L198">    }</span>

    private double[] getModelProbs(Datum&lt;L,F&gt; datum){
<span class="nc" id="L201">      double[] condDist = new double[labeledDataset.numClasses()];</span>
<span class="nc" id="L202">      Counter&lt;L&gt; probCounter = classifier.probabilityOf(datum);</span>
<span class="nc bnc" id="L203" title="All 2 branches missed.">      for(L label : probCounter.keySet()){</span>
<span class="nc" id="L204">        int labelID = labeledDataset.labelIndex.indexOf(label);</span>
<span class="nc" id="L205">        condDist[labelID] = probCounter.getCount(label);</span>
<span class="nc" id="L206">      }</span>
<span class="nc" id="L207">      return condDist;</span>
    }

<span class="nc" id="L210">  public GeneralizedExpectationObjectiveFunction(GeneralDataset&lt;L, F&gt; labeledDataset, List&lt;? extends Datum&lt;L,F&gt;&gt; unlabeledDataList,List&lt;F&gt; geFeatures) {</span>
<span class="nc" id="L211">    System.out.println(&quot;Number of labeled examples:&quot;+labeledDataset.size+&quot;\nNumber of unlabeled examples:&quot;+unlabeledDataList.size());</span>
<span class="nc" id="L212">    System.out.println(&quot;Number of GE features:&quot;+geFeatures.size());</span>
<span class="nc" id="L213">    this.numFeatures = labeledDataset.numFeatures();</span>
<span class="nc" id="L214">    this.numClasses = labeledDataset.numClasses();</span>
<span class="nc" id="L215">    this.labeledDataset = labeledDataset;</span>
<span class="nc" id="L216">    this.unlabeledDataList = unlabeledDataList;</span>
<span class="nc" id="L217">    this.geFeatures = geFeatures;</span>
<span class="nc" id="L218">    this.classifier = new LinearClassifier&lt;&gt;(null, labeledDataset.featureIndex, labeledDataset.labelIndex);</span>
<span class="nc" id="L219">    computeEmpiricalStatistics(geFeatures);</span>
    //empirical distributions don't change with iterations, so compute them only once.
    //model distributions will have to be recomputed every iteration though.
<span class="nc" id="L222">  }</span>

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>