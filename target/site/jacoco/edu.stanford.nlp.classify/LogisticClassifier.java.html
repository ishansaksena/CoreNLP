<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>LogisticClassifier.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.classify</a> &gt; <span class="el_source">LogisticClassifier.java</span></div><h1>LogisticClassifier.java</h1><pre class="source lang-java linenums">// Stanford Classifier - a multiclass maxent classifier
// LogisticClassifier
// Copyright (c) 2003-2007 The Board of Trustees of
// The Leland Stanford Junior University. All Rights Reserved.
//
// This program is free software; you can redistribute it and/or
// modify it under the terms of the GNU General Public License
// as published by the Free Software Foundation; either version 2
// of the License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program; if not, write to the Free Software
// Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
//
// For more information, bug reports, fixes, contact:
//    Christopher Manning
//    Dept of Computer Science, Gates 1A
//    Stanford CA 94305-9010
//    USA
//    Support/Questions: java-nlp-user@lists.stanford.edu
//    Licensing: java-nlp-support@lists.stanford.edu
//    http://www-nlp.stanford.edu/software/classifier.shtml

package edu.stanford.nlp.classify;

import java.io.File;
import java.util.*;

import edu.stanford.nlp.ling.Datum;
import edu.stanford.nlp.ling.RVFDatum;
import edu.stanford.nlp.optimization.DiffFunction;
import edu.stanford.nlp.optimization.Minimizer;
import edu.stanford.nlp.optimization.QNMinimizer;
import edu.stanford.nlp.stats.Counter;
import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.objectbank.ObjectBank;
import edu.stanford.nlp.util.ErasureUtils;
import edu.stanford.nlp.util.Index;
import edu.stanford.nlp.util.ReflectionLoading;
import edu.stanford.nlp.util.StringUtils;

/**
 * A classifier for binary logistic regression problems.
 * This uses the standard statistics textbook formulation of binary
 * logistic regression, which is more efficient than using the
 * LinearClassifier class.
 *
 * @author Galen Andrew
 * @author Sarah Spikes (sdspikes@cs.stanford.edu) (Templatization)
 * @author Ramesh Nallapati nmramesh@cs.stanford.edu {@link #justificationOf(Collection)}
 *
 * @param &lt;L&gt; The type of the labels in the Dataset
 * @param &lt;F&gt; The type of the features in the Dataset
 */
public class LogisticClassifier&lt;L, F&gt; implements Classifier&lt;L, F&gt;, RVFClassifier&lt;L, F&gt; /* Serializable */ {

  //TODO make it implement ProbabilisticClassifier as well. --Ramesh 12/03/2009.
  /**
   *
   */
  private static final long serialVersionUID = 6672245467246897192L;
  private double[] weights;
  private Index&lt;F&gt; featureIndex;
<span class="nc" id="L69">  private L[] classes = ErasureUtils.&lt;L&gt;mkTArray(Object.class,2);</span>
  @Deprecated
  private LogPrior prior;
<span class="nc" id="L72">  @Deprecated</span>
  private boolean biased = false;

  @Override
  public String toString() {
<span class="nc bnc" id="L77" title="All 2 branches missed.">    if (featureIndex == null) {</span>
<span class="nc" id="L78">      return &quot;&quot;;</span>
    }

<span class="nc" id="L81">    StringBuilder sb = new StringBuilder();</span>
<span class="nc bnc" id="L82" title="All 2 branches missed.">    for (F f : featureIndex) {</span>
<span class="nc" id="L83">      sb.append(classes[1]).append(&quot; / &quot;).append(f).append(&quot; = &quot;).append(weights[featureIndex.indexOf(f)]);</span>
<span class="nc" id="L84">    }</span>

<span class="nc" id="L86">    return sb.toString();</span>
  }

  public L getLabelForInternalPositiveClass(){
<span class="nc" id="L90">    return classes[1];</span>
  }

  public L getLabelForInternalNegativeClass(){
<span class="nc" id="L94">    return classes[0];</span>
  }

  public Counter&lt;F&gt; weightsAsCounter() {
<span class="nc" id="L98">    Counter&lt;F&gt; c = new ClassicCounter&lt;&gt;();</span>
<span class="nc bnc" id="L99" title="All 2 branches missed.">    for (F f : featureIndex) {</span>
<span class="nc" id="L100">      double w =  weights[featureIndex.indexOf(f)];</span>
<span class="nc bnc" id="L101" title="All 2 branches missed.">      if (w != 0.0) {</span>
<span class="nc" id="L102">        c.setCount(f, w);</span>
      }
<span class="nc" id="L104">    }</span>
<span class="nc" id="L105">    return c;</span>
  }

  public Index&lt;F&gt; getFeatureIndex() {
<span class="nc" id="L109">    return featureIndex;</span>
  }

  public double[] getWeights() {
<span class="nc" id="L113">    return weights;</span>
  }


<span class="nc" id="L117">  public LogisticClassifier(double[] weights, Index&lt;F&gt; featureIndex, L[] classes){</span>
<span class="nc" id="L118">    this.weights = weights;</span>
<span class="nc" id="L119">    this.featureIndex = featureIndex;</span>
<span class="nc" id="L120">    this.classes = classes;</span>
<span class="nc" id="L121">  }</span>


  @Deprecated //use  LogisticClassifierFactory instead
  public LogisticClassifier(boolean biased) {
<span class="nc" id="L126">    this(new LogPrior(LogPrior.LogPriorType.QUADRATIC), biased);</span>
<span class="nc" id="L127">  }</span>

  @Deprecated //use  in LogisticClassifierFactory instead.
<span class="nc" id="L130">  public LogisticClassifier(LogPrior prior) {</span>
<span class="nc" id="L131">    this.prior = prior;</span>
<span class="nc" id="L132">  }</span>


  @Deprecated //use  in LogisticClassifierFactory instead
<span class="nc" id="L136">  public LogisticClassifier(LogPrior prior, boolean biased) {</span>
<span class="nc" id="L137">    this.prior = prior;</span>
<span class="nc" id="L138">    this.biased = biased;</span>
<span class="nc" id="L139">  }</span>

  @Override
  public Collection&lt;L&gt; labels() {
<span class="nc" id="L143">    Collection&lt;L&gt; l = new LinkedList&lt;&gt;();</span>
<span class="nc" id="L144">    l.add(classes[0]);</span>
<span class="nc" id="L145">    l.add(classes[1]);</span>
<span class="nc" id="L146">    return l;</span>
  }

  @Override
  public L classOf(Datum&lt;L, F&gt; datum) {
<span class="nc bnc" id="L151" title="All 2 branches missed.">    if(datum instanceof RVFDatum&lt;?,?&gt;){</span>
<span class="nc" id="L152">      return classOfRVFDatum((RVFDatum&lt;L,F&gt;) datum);</span>
    }
<span class="nc" id="L154">    return classOf(datum.asFeatures());</span>
  }

  @Override
  @Deprecated //use classOf(Datum) instead.
  public L classOf(RVFDatum&lt;L, F&gt; example) {
<span class="nc" id="L160">    return classOf(example.asFeaturesCounter());</span>
  }

  private L classOfRVFDatum(RVFDatum&lt;L, F&gt; example) {
<span class="nc" id="L164">    return classOf(example.asFeaturesCounter());</span>
  }

  public L classOf(Counter&lt;F&gt; features) {
<span class="nc bnc" id="L168" title="All 2 branches missed.">    if (scoreOf(features) &gt; 0) {</span>
<span class="nc" id="L169">      return classes[1];</span>
    }
<span class="nc" id="L171">    return classes[0];</span>
  }

  public L classOf(Collection&lt;F&gt; features) {
<span class="nc bnc" id="L175" title="All 2 branches missed.">    if (scoreOf(features) &gt; 0) {</span>
<span class="nc" id="L176">      return classes[1];</span>
    }
<span class="nc" id="L178">    return classes[0];</span>
  }


  public double scoreOf(Collection&lt;F&gt; features) {
<span class="nc" id="L183">    double sum = 0;</span>
<span class="nc bnc" id="L184" title="All 2 branches missed.">    for (F feature : features) {</span>
<span class="nc" id="L185">      int f = featureIndex.indexOf(feature);</span>
<span class="nc bnc" id="L186" title="All 2 branches missed.">      if (f &gt;= 0) {</span>
<span class="nc" id="L187">        sum += weights[f];</span>
      }
<span class="nc" id="L189">    }</span>
<span class="nc" id="L190">    return sum;</span>
  }

  public double scoreOf(Counter&lt;F&gt; features) {
<span class="nc" id="L194">    double sum = 0;</span>
<span class="nc bnc" id="L195" title="All 2 branches missed.">    for (F feature : features.keySet()) {</span>
<span class="nc" id="L196">      int f = featureIndex.indexOf(feature);</span>
<span class="nc bnc" id="L197" title="All 2 branches missed.">      if (f &gt;= 0) {</span>
<span class="nc" id="L198">        sum += weights[f]*features.getCount(feature);</span>
      }
<span class="nc" id="L200">    }</span>
<span class="nc" id="L201">    return sum;</span>
  }
  /*
   * returns the weights to each feature assigned by the classifier
   * nmramesh@cs.stanford.edu
   */
  public Counter&lt;F&gt; justificationOf(Counter&lt;F&gt; features){
<span class="nc" id="L208">    Counter&lt;F&gt; fWts = new ClassicCounter&lt;&gt;();</span>
<span class="nc bnc" id="L209" title="All 2 branches missed.">    for (F feature : features.keySet()) {</span>
<span class="nc" id="L210">      int f = featureIndex.indexOf(feature);</span>
<span class="nc bnc" id="L211" title="All 2 branches missed.">      if (f &gt;= 0) {</span>
<span class="nc" id="L212">        fWts.incrementCount(feature,weights[f]*features.getCount(feature));</span>
      }
<span class="nc" id="L214">    }</span>
<span class="nc" id="L215">    return fWts;</span>
  }
  /**
   * returns the weights assigned by the classifier to each feature
   */
  public Counter&lt;F&gt; justificationOf(Collection&lt;F&gt; features){
<span class="nc" id="L221">    Counter&lt;F&gt; fWts = new ClassicCounter&lt;&gt;();</span>
<span class="nc bnc" id="L222" title="All 2 branches missed.">    for (F feature : features) {</span>
<span class="nc" id="L223">      int f = featureIndex.indexOf(feature);</span>
<span class="nc bnc" id="L224" title="All 2 branches missed.">      if (f &gt;= 0) {</span>
<span class="nc" id="L225">        fWts.incrementCount(feature,weights[f]);</span>
      }
<span class="nc" id="L227">    }</span>
<span class="nc" id="L228">    return fWts;</span>
  }

  /**
   * returns the scores for both the classes
   */
  @Override
  public Counter&lt;L&gt; scoresOf(Datum&lt;L, F&gt; datum) {
<span class="nc bnc" id="L236" title="All 2 branches missed.">    if(datum instanceof RVFDatum&lt;?,?&gt;)return scoresOfRVFDatum((RVFDatum&lt;L,F&gt;)datum);</span>
<span class="nc" id="L237">    Collection&lt;F&gt; features = datum.asFeatures();</span>
<span class="nc" id="L238">    double sum = scoreOf(features);</span>
<span class="nc" id="L239">    Counter&lt;L&gt; c = new ClassicCounter&lt;&gt;();</span>
<span class="nc" id="L240">    c.setCount(classes[0], -sum);</span>
<span class="nc" id="L241">    c.setCount(classes[1], sum);</span>
<span class="nc" id="L242">    return c;</span>
  }


  @Override
  @Deprecated //use scoresOfDatum(Datum) instead.
  public Counter&lt;L&gt; scoresOf(RVFDatum&lt;L, F&gt; example) {
<span class="nc" id="L249">    return scoresOfRVFDatum(example);</span>
  }


  private Counter&lt;L&gt; scoresOfRVFDatum(RVFDatum&lt;L, F&gt; example) {
<span class="nc" id="L254">    Counter&lt;F&gt; features = example.asFeaturesCounter();</span>
<span class="nc" id="L255">    double sum = scoreOf(features);</span>
<span class="nc" id="L256">    Counter&lt;L&gt; c = new ClassicCounter&lt;&gt;();</span>
<span class="nc" id="L257">    c.setCount(classes[0], -sum);</span>
<span class="nc" id="L258">    c.setCount(classes[1], sum);</span>
<span class="nc" id="L259">    return c;</span>
  }

  public double probabilityOf(Datum&lt;L, F&gt; example) {
<span class="nc bnc" id="L263" title="All 2 branches missed.">    if (example instanceof RVFDatum&lt;?,?&gt;) {</span>
<span class="nc" id="L264">      return probabilityOfRVFDatum((RVFDatum&lt;L,F&gt;)example);</span>
    }
<span class="nc" id="L266">    return probabilityOf(example.asFeatures(), example.label());</span>
  }

  public double probabilityOf(Collection&lt;F&gt; features, L label) {
<span class="nc bnc" id="L270" title="All 2 branches missed.">    short sign = (short)(label.equals(classes[0]) ? 1 : -1);</span>
<span class="nc" id="L271">    return 1.0 / (1.0 + Math.exp(sign * scoreOf(features)));</span>
  }

  public double probabilityOf(RVFDatum&lt;L, F&gt; example) {
<span class="nc" id="L275">    return probabilityOfRVFDatum(example);</span>
  }

  private double probabilityOfRVFDatum(RVFDatum&lt;L, F&gt; example) {
<span class="nc" id="L279">    return probabilityOf(example.asFeaturesCounter(), example.label());</span>
  }

  public double probabilityOf(Counter&lt;F&gt; features, L label) {
<span class="nc bnc" id="L283" title="All 2 branches missed.">    short sign = (short)(label.equals(classes[0]) ? 1 : -1);</span>
<span class="nc" id="L284">    return 1.0 / (1.0 + Math.exp(sign * scoreOf(features)));</span>
  }

  /**
   * Trains on weighted dataset.
   * @param dataWeights weights of the data.
   */
  @Deprecated //Use LogisticClassifierFactory to train instead.
  public void trainWeightedData(GeneralDataset&lt;L,F&gt; data, float[] dataWeights){
<span class="nc bnc" id="L293" title="All 2 branches missed.">    if (data.labelIndex.size() != 2) {</span>
<span class="nc" id="L294">      throw new RuntimeException(&quot;LogisticClassifier is only for binary classification!&quot;);</span>
    }

    Minimizer&lt;DiffFunction&gt; minim;
<span class="nc" id="L298">      LogisticObjectiveFunction lof = null;</span>
<span class="nc bnc" id="L299" title="All 2 branches missed.">      if(data instanceof Dataset&lt;?,?&gt;)</span>
<span class="nc" id="L300">        lof = new LogisticObjectiveFunction(data.numFeatureTypes(), data.getDataArray(), data.getLabelsArray(), prior,dataWeights);</span>
<span class="nc bnc" id="L301" title="All 2 branches missed.">      else if(data instanceof RVFDataset&lt;?,?&gt;)</span>
<span class="nc" id="L302">        lof = new LogisticObjectiveFunction(data.numFeatureTypes(), data.getDataArray(), data.getValuesArray(), data.getLabelsArray(), prior,dataWeights);</span>
<span class="nc" id="L303">      minim = new QNMinimizer(lof);</span>
<span class="nc" id="L304">      weights = minim.minimize(lof, 1e-4, new double[data.numFeatureTypes()]);</span>

<span class="nc" id="L306">    featureIndex = data.featureIndex;</span>
<span class="nc" id="L307">    classes[0] = data.labelIndex.get(0);</span>
<span class="nc" id="L308">    classes[1] = data.labelIndex.get(1);</span>
<span class="nc" id="L309">  }</span>

  @Deprecated //Use LogisticClassifierFactory to train instead.
  public void train(GeneralDataset&lt;L, F&gt; data) {
<span class="nc" id="L313">    train(data, 0.0, 1e-4);</span>
<span class="nc" id="L314">  }</span>

  @Deprecated //Use LogisticClassifierFactory to train instead.
  public void train(GeneralDataset&lt;L, F&gt; data, double l1reg, double tol) {
<span class="nc bnc" id="L318" title="All 2 branches missed.">    if (data.labelIndex.size() != 2) {</span>
<span class="nc" id="L319">      throw new RuntimeException(&quot;LogisticClassifier is only for binary classification!&quot;);</span>
    }

    Minimizer&lt;DiffFunction&gt; minim;
<span class="nc bnc" id="L323" title="All 2 branches missed.">    if (!biased) {</span>
<span class="nc" id="L324">      LogisticObjectiveFunction lof = null;</span>
<span class="nc bnc" id="L325" title="All 2 branches missed.">      if(data instanceof Dataset&lt;?,?&gt;)</span>
<span class="nc" id="L326">        lof = new LogisticObjectiveFunction(data.numFeatureTypes(), data.getDataArray(), data.getLabelsArray(), prior);</span>
<span class="nc bnc" id="L327" title="All 2 branches missed.">      else if(data instanceof RVFDataset&lt;?,?&gt;)</span>
<span class="nc" id="L328">        lof = new LogisticObjectiveFunction(data.numFeatureTypes(), data.getDataArray(), data.getValuesArray(), data.getLabelsArray(), prior);</span>
<span class="nc bnc" id="L329" title="All 2 branches missed.">      if (l1reg &gt; 0.0) {</span>
<span class="nc" id="L330">        minim = ReflectionLoading.loadByReflection(&quot;edu.stanford.nlp.optimization.OWLQNMinimizer&quot;, l1reg);</span>
      } else {
<span class="nc" id="L332">        minim = new QNMinimizer(lof);</span>
      }
<span class="nc" id="L334">      weights = minim.minimize(lof, tol, new double[data.numFeatureTypes()]);</span>
<span class="nc" id="L335">    } else {</span>
<span class="nc" id="L336">      BiasedLogisticObjectiveFunction lof = new BiasedLogisticObjectiveFunction(data.numFeatureTypes(), data.getDataArray(), data.getLabelsArray(), prior);</span>
<span class="nc bnc" id="L337" title="All 2 branches missed.">      if (l1reg &gt; 0.0) {</span>
<span class="nc" id="L338">        minim = ReflectionLoading.loadByReflection(&quot;edu.stanford.nlp.optimization.OWLQNMinimizer&quot;, l1reg);</span>
      } else {
<span class="nc" id="L340">        minim = new QNMinimizer(lof);</span>
      }
<span class="nc" id="L342">      weights = minim.minimize(lof, tol, new double[data.numFeatureTypes()]);</span>
    }

<span class="nc" id="L345">    featureIndex = data.featureIndex;</span>
<span class="nc" id="L346">    classes[0] = data.labelIndex.get(0);</span>
<span class="nc" id="L347">    classes[1] = data.labelIndex.get(1);</span>
<span class="nc" id="L348">  }</span>

  /** This runs a simple train and test regime.
   *  The data file format is one item per line, space separated, with first the class label
   *  and then a bunch of (categorical) string features.
   *
   *  @param args The arguments/flags are: -trainFile trainFile -testFile testFile [-l1reg num] [-biased]
   *  @throws Exception
   */
  public static void main(String[] args) throws Exception {
<span class="nc" id="L358">    Properties prop = StringUtils.argsToProperties(args);</span>

<span class="nc" id="L360">    double l1reg = Double.parseDouble(prop.getProperty(&quot;l1reg&quot;,&quot;0.0&quot;));</span>

<span class="nc" id="L362">    Dataset&lt;String, String&gt; ds = new Dataset&lt;&gt;();</span>
<span class="nc bnc" id="L363" title="All 2 branches missed.">    for (String line : ObjectBank.getLineIterator(new File(prop.getProperty(&quot;trainFile&quot;)))) {</span>
<span class="nc" id="L364">      String[] bits = line.split(&quot;\\s+&quot;);</span>
<span class="nc" id="L365">      Collection&lt;String&gt; f = new LinkedList&lt;&gt;(Arrays.asList(bits).subList(1, bits.length));</span>
<span class="nc" id="L366">      String l = bits[0];</span>
<span class="nc" id="L367">      ds.add(f, l);</span>
<span class="nc" id="L368">    }</span>

<span class="nc" id="L370">    ds.summaryStatistics();</span>

<span class="nc" id="L372">    boolean biased = prop.getProperty(&quot;biased&quot;, &quot;false&quot;).equals(&quot;true&quot;);</span>
<span class="nc" id="L373">    LogisticClassifierFactory&lt;String, String&gt; factory = new LogisticClassifierFactory&lt;&gt;();</span>
<span class="nc" id="L374">    LogisticClassifier&lt;String, String&gt; lc = factory.trainClassifier(ds, l1reg, 1e-4, biased);</span>

<span class="nc bnc" id="L376" title="All 2 branches missed.">    for (String line : ObjectBank.getLineIterator(new File(prop.getProperty(&quot;testFile&quot;)))) {</span>
<span class="nc" id="L377">      String[] bits = line.split(&quot;\\s+&quot;);</span>
<span class="nc" id="L378">      Collection&lt;String&gt; f = new LinkedList&lt;&gt;(Arrays.asList(bits).subList(1, bits.length));</span>
      //String l = bits[0];
<span class="nc" id="L380">      String g = lc.classOf(f);</span>
<span class="nc" id="L381">      double prob = lc.probabilityOf(f, g);</span>
<span class="nc" id="L382">      System.out.printf(&quot;%4.3f\t%s\t%s%n&quot;, prob, g, line);</span>
<span class="nc" id="L383">    }</span>
<span class="nc" id="L384">  }</span>

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>