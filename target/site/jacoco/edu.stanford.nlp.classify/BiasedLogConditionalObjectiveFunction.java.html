<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>BiasedLogConditionalObjectiveFunction.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.classify</a> &gt; <span class="el_source">BiasedLogConditionalObjectiveFunction.java</span></div><h1>BiasedLogConditionalObjectiveFunction.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.classify;

import edu.stanford.nlp.math.ArrayMath;
import edu.stanford.nlp.optimization.AbstractCachingDiffFunction;

import java.util.Arrays;


/**
 * Maximizes the conditional likelihood with a given prior.
 *
 * @author Jenny Finkel
 */

public class BiasedLogConditionalObjectiveFunction extends AbstractCachingDiffFunction {

  public void setPrior(LogPrior prior) {
<span class="nc" id="L18">    this.prior = prior;</span>
<span class="nc" id="L19">  }</span>

  protected LogPrior prior;

<span class="nc" id="L23">  protected int numFeatures = 0;</span>
<span class="nc" id="L24">  protected int numClasses = 0;</span>

<span class="nc" id="L26">  protected int[][] data = null;</span>
<span class="nc" id="L27">  protected int[] labels = null;</span>

  private double[][] confusionMatrix;
  
  @Override
  public int domainDimension() {
<span class="nc" id="L33">    return numFeatures * numClasses;</span>
  }

  int classOf(int index) {
<span class="nc" id="L37">    return index % numClasses;</span>
  }

  int featureOf(int index) {
<span class="nc" id="L41">    return index / numClasses;</span>
  }

  protected int indexOf(int f, int c) {
<span class="nc" id="L45">    return f * numClasses + c;</span>
  }

  public double[][] to2D(double[] x) {
<span class="nc" id="L49">    double[][] x2 = new double[numFeatures][numClasses];</span>
<span class="nc bnc" id="L50" title="All 2 branches missed.">    for (int i = 0; i &lt; numFeatures; i++) {</span>
<span class="nc bnc" id="L51" title="All 2 branches missed.">      for (int j = 0; j &lt; numClasses; j++) {</span>
<span class="nc" id="L52">        x2[i][j] = x[indexOf(i, j)];</span>
      }
    }
<span class="nc" id="L55">    return x2;</span>
  }

  @Override
  protected void calculate(double[] x) {
    
<span class="nc bnc" id="L61" title="All 2 branches missed.">    if (derivative == null) {</span>
<span class="nc" id="L62">      derivative = new double[x.length];</span>
    } else {
<span class="nc" id="L64">      Arrays.fill(derivative, 0.0);</span>
    }

<span class="nc" id="L67">    value = 0.0;</span>

<span class="nc" id="L69">    double[] sums = new double[numClasses];</span>
<span class="nc" id="L70">    double[] probs = new double[numClasses];</span>
<span class="nc" id="L71">    double[] weightedProbs = new double[numClasses];</span>

<span class="nc bnc" id="L73" title="All 2 branches missed.">    for (int d = 0; d &lt; data.length; d++) {</span>
<span class="nc" id="L74">      int[] features = data[d];</span>
<span class="nc" id="L75">      int observedLabel = labels[d];</span>
      // activation
<span class="nc" id="L77">      Arrays.fill(sums, 0.0);</span>

<span class="nc bnc" id="L79" title="All 2 branches missed.">      for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc bnc" id="L80" title="All 2 branches missed.">        for (int feature : features) {</span>
<span class="nc" id="L81">          int i = indexOf(feature, c);</span>
<span class="nc" id="L82">          sums[c] += x[i];</span>
        }
      }

<span class="nc" id="L86">      double total = ArrayMath.logSum(sums);</span>

<span class="nc" id="L88">      double[] weightedSums = new double[numClasses];</span>
<span class="nc bnc" id="L89" title="All 2 branches missed.">      for (int trueLabel = 0; trueLabel &lt; numClasses; trueLabel++) {</span>
<span class="nc" id="L90">        weightedSums[trueLabel] = Math.log(confusionMatrix[observedLabel][trueLabel]) + sums[trueLabel];</span>
      }

<span class="nc" id="L93">      double weightedTotal = ArrayMath.logSum(weightedSums);</span>
      
<span class="nc bnc" id="L95" title="All 2 branches missed.">      for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L96">        probs[c] = Math.exp(sums[c] - total);</span>
<span class="nc" id="L97">        weightedProbs[c] = Math.exp(weightedSums[c] - weightedTotal);</span>
<span class="nc bnc" id="L98" title="All 2 branches missed.">        for (int feature : features) {</span>
<span class="nc" id="L99">          int i = indexOf(feature, c);</span>
<span class="nc" id="L100">          derivative[i] += probs[c] - weightedProbs[c];</span>
        }
      }

<span class="nc" id="L104">      double tmpValue = 0.0;</span>
<span class="nc bnc" id="L105" title="All 2 branches missed.">      for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L106">        tmpValue += confusionMatrix[observedLabel][c] * Math.exp(sums[c] - total);</span>
      }
<span class="nc" id="L108">      value -= Math.log(tmpValue);</span>
    }
    
<span class="nc" id="L111">    value += prior.compute(x, derivative);</span>
    
<span class="nc" id="L113">  }</span>



  public BiasedLogConditionalObjectiveFunction(GeneralDataset&lt;?, ?&gt; dataset, double[][] confusionMatrix) {
<span class="nc" id="L118">    this(dataset, confusionMatrix, new LogPrior(LogPrior.LogPriorType.QUADRATIC));</span>
<span class="nc" id="L119">  }</span>

  public BiasedLogConditionalObjectiveFunction(GeneralDataset&lt;?, ?&gt; dataset, double[][] confusionMatrix, LogPrior prior) {
<span class="nc" id="L122">    this(dataset.numFeatures(), dataset.numClasses(), dataset.getDataArray(), dataset.getLabelsArray(), confusionMatrix, prior);</span>
<span class="nc" id="L123">  }</span>

  public BiasedLogConditionalObjectiveFunction(int numFeatures, int numClasses, int[][] data, int[] labels, double[][] confusionMatrix) {
<span class="nc" id="L126">    this(numFeatures, numClasses, data, labels, confusionMatrix, new LogPrior(LogPrior.LogPriorType.QUADRATIC));</span>
<span class="nc" id="L127">  }</span>

<span class="nc" id="L129">  public BiasedLogConditionalObjectiveFunction(int numFeatures, int numClasses, int[][] data, int[] labels, double[][] confusionMatrix, LogPrior prior) {</span>
<span class="nc" id="L130">    this.numFeatures = numFeatures;</span>
<span class="nc" id="L131">    this.numClasses = numClasses;</span>
<span class="nc" id="L132">    this.data = data;</span>
<span class="nc" id="L133">    this.labels = labels;</span>
<span class="nc" id="L134">    this.prior = prior;</span>
<span class="nc" id="L135">    this.confusionMatrix = confusionMatrix;</span>
<span class="nc" id="L136">  }</span>
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>