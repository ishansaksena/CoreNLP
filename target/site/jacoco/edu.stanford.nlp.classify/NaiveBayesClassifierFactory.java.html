<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>NaiveBayesClassifierFactory.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.classify</a> &gt; <span class="el_source">NaiveBayesClassifierFactory.java</span></div><h1>NaiveBayesClassifierFactory.java</h1><pre class="source lang-java linenums">// Stanford Classifier - a multiclass maxent classifier
// NaiveBayesClassifierFactory
// Copyright (c) 2003-2007 The Board of Trustees of
// The Leland Stanford Junior University. All Rights Reserved.
//
// This program is free software; you can redistribute it and/or
// modify it under the terms of the GNU General Public License
// as published by the Free Software Foundation; either version 2
// of the License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program; if not, write to the Free Software
// Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
//
// For more information, bug reports, fixes, contact:
//    Christopher Manning
//    Dept of Computer Science, Gates 1A
//    Stanford CA 94305-9010
//    USA
//    Support/Questions: java-nlp-user@lists.stanford.edu
//    Licensing: java-nlp-support@lists.stanford.eduu
//    http://www-nlp.stanford.edu/software/classifier.shtml

package edu.stanford.nlp.classify;

import edu.stanford.nlp.ling.RVFDatum;
import edu.stanford.nlp.optimization.DiffFunction;
import edu.stanford.nlp.optimization.Minimizer;
import edu.stanford.nlp.optimization.QNMinimizer;
import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.stats.Counter;
import edu.stanford.nlp.stats.Counters;
import edu.stanford.nlp.util.Generics;
import edu.stanford.nlp.util.Index;
import edu.stanford.nlp.util.Pair;
import edu.stanford.nlp.util.HashIndex;


import edu.stanford.nlp.util.logging.Redwood;

import java.util.*;


/** Creates a NaiveBayesClassifier given an RVFDataset.
 *
 * @author Kristina Toutanova (kristina@cs.stanford.edu)
 */
public class NaiveBayesClassifierFactory&lt;L, F&gt; implements ClassifierFactory&lt;L, F, NaiveBayesClassifier&lt;L, F&gt;&gt;  {

  /** A logger for this class */
<span class="nc" id="L56">  private static Redwood.RedwoodChannels log = Redwood.channels(NaiveBayesClassifierFactory.class);</span>

  private static final long serialVersionUID = -8164165428834534041L;
  public static final int JL = 0;
  public static final int CL = 1;
  public static final int UCL = 2;
<span class="nc" id="L62">  private int kind = JL;</span>
  private double alphaClass;
  private double alphaFeature;
  private double sigma;
<span class="nc" id="L66">  private int prior = LogPrior.LogPriorType.NULL.ordinal();</span>
  private Index&lt;L&gt; labelIndex;
  private Index&lt;F&gt; featureIndex;

<span class="nc" id="L70">  final static Redwood.RedwoodChannels logger = Redwood.channels(NaiveBayesClassifierFactory.class);</span>

<span class="nc" id="L72">  public NaiveBayesClassifierFactory() {</span>
<span class="nc" id="L73">  }</span>

<span class="nc" id="L75">  public NaiveBayesClassifierFactory(double alphaC, double alphaF, double sigma, int prior, int kind) {</span>
<span class="nc" id="L76">    alphaClass = alphaC;</span>
<span class="nc" id="L77">    alphaFeature = alphaF;</span>
<span class="nc" id="L78">    this.sigma = sigma;</span>
<span class="nc" id="L79">    this.prior = prior;</span>
<span class="nc" id="L80">    this.kind = kind;</span>
<span class="nc" id="L81">  }</span>

  private NaiveBayesClassifier&lt;L, F&gt; trainClassifier(int[][] data, int[] labels, int numFeatures,
      int numClasses, Index&lt;L&gt; labelIndex, Index&lt;F&gt; featureIndex) {
<span class="nc" id="L85">    Set&lt;L&gt; labelSet = Generics.newHashSet();</span>
<span class="nc" id="L86">    NBWeights nbWeights = trainWeights(data, labels, numFeatures, numClasses);</span>
<span class="nc" id="L87">    Counter&lt;L&gt; priors = new ClassicCounter&lt;&gt;();</span>
<span class="nc" id="L88">    double[] pr = nbWeights.priors;</span>
<span class="nc bnc" id="L89" title="All 2 branches missed.">    for (int i = 0; i &lt; pr.length; i++) {</span>
<span class="nc" id="L90">      priors.incrementCount(labelIndex.get(i), pr[i]);</span>
<span class="nc" id="L91">      labelSet.add(labelIndex.get(i));</span>
    }
<span class="nc" id="L93">    Counter&lt;Pair&lt;Pair&lt;L, F&gt;, Number&gt;&gt; weightsCounter = new ClassicCounter&lt;&gt;();</span>
<span class="nc" id="L94">    double[][][] wts = nbWeights.weights;</span>
<span class="nc bnc" id="L95" title="All 2 branches missed.">    for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L96">      L label = labelIndex.get(c);</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">      for (int f = 0; f &lt; numFeatures; f++) {</span>
<span class="nc" id="L98">        F feature = featureIndex.get(f);</span>
<span class="nc" id="L99">        Pair&lt;L, F&gt; p = new Pair&lt;&gt;(label, feature);</span>
<span class="nc bnc" id="L100" title="All 2 branches missed.">        for (int val = 0; val &lt; wts[c][f].length; val++) {</span>
<span class="nc" id="L101">          Pair&lt;Pair&lt;L, F&gt;, Number&gt; key = new Pair&lt;&gt;(p, Integer.valueOf(val));</span>
<span class="nc" id="L102">          weightsCounter.incrementCount(key, wts[c][f][val]);</span>
        }
      }
    }
<span class="nc" id="L106">    return new NaiveBayesClassifier&lt;&gt;(weightsCounter, priors, labelSet);</span>

  }

  /**
   * The examples are assumed to be a list of RFVDatum.
   * The datums are assumed to not contain the zeroes and then they are added to each instance.
   */
  public NaiveBayesClassifier&lt;L, F&gt; trainClassifier(GeneralDataset&lt;L, F&gt; examples, Set&lt;F&gt; featureSet) {
<span class="nc" id="L115">    int numFeatures = featureSet.size();</span>
<span class="nc" id="L116">    int[][] data = new int[examples.size()][numFeatures];</span>
<span class="nc" id="L117">    int[] labels = new int[examples.size()];</span>
<span class="nc" id="L118">    labelIndex = new HashIndex&lt;&gt;();</span>
<span class="nc" id="L119">    featureIndex = new HashIndex&lt;&gt;();</span>
<span class="nc bnc" id="L120" title="All 2 branches missed.">    for (F feat : featureSet) {</span>
<span class="nc" id="L121">      featureIndex.add(feat);</span>
<span class="nc" id="L122">    }</span>
<span class="nc bnc" id="L123" title="All 2 branches missed.">    for (int d = 0; d &lt; examples.size(); d++) {</span>
<span class="nc" id="L124">      RVFDatum&lt;L, F&gt; datum = examples.getRVFDatum(d);</span>
<span class="nc" id="L125">      Counter&lt;F&gt; c = datum.asFeaturesCounter();</span>
<span class="nc bnc" id="L126" title="All 2 branches missed.">      for (F feature : c.keySet()) {</span>
<span class="nc" id="L127">        int fNo = featureIndex.indexOf(feature);</span>
<span class="nc" id="L128">        int value = (int) c.getCount(feature);</span>
<span class="nc" id="L129">        data[d][fNo] = value;</span>
<span class="nc" id="L130">      }</span>
<span class="nc" id="L131">      labelIndex.add(datum.label());</span>
<span class="nc" id="L132">      labels[d] = labelIndex.indexOf(datum.label());</span>

    }
<span class="nc" id="L135">    int numClasses = labelIndex.size();</span>
<span class="nc" id="L136">    return trainClassifier(data, labels, numFeatures, numClasses, labelIndex, featureIndex);</span>
  }


  /**
   * Here the data is assumed to be for every instance, array of length numFeatures
   * and the value of the feature is stored including zeroes.
   *
   * @return {@literal label,fno,value -&gt; weight}
   */
  private NBWeights trainWeights(int[][] data, int[] labels, int numFeatures, int numClasses) {
<span class="nc bnc" id="L147" title="All 2 branches missed.">    if (kind == JL) {</span>
<span class="nc" id="L148">      return trainWeightsJL(data, labels, numFeatures, numClasses);</span>
    }
<span class="nc bnc" id="L150" title="All 2 branches missed.">    if (kind == UCL) {</span>
<span class="nc" id="L151">      return trainWeightsUCL(data, labels, numFeatures, numClasses);</span>
    }
<span class="nc bnc" id="L153" title="All 2 branches missed.">    if (kind == CL) {</span>
<span class="nc" id="L154">      return trainWeightsCL(data, labels, numFeatures, numClasses);</span>
    }
<span class="nc" id="L156">    return null;</span>
  }

  private NBWeights trainWeightsJL(int[][] data, int[] labels, int numFeatures, int numClasses) {
<span class="nc" id="L160">    int[] numValues = numberValues(data, numFeatures);</span>
<span class="nc" id="L161">    double[] priors = new double[numClasses];</span>
<span class="nc" id="L162">    double[][][] weights = new double[numClasses][numFeatures][];</span>
    //init weights array
<span class="nc bnc" id="L164" title="All 2 branches missed.">    for (int cl = 0; cl &lt; numClasses; cl++) {</span>
<span class="nc bnc" id="L165" title="All 2 branches missed.">      for (int fno = 0; fno &lt; numFeatures; fno++) {</span>
<span class="nc" id="L166">        weights[cl][fno] = new double[numValues[fno]];</span>
      }
    }
<span class="nc bnc" id="L169" title="All 2 branches missed.">    for (int i = 0; i &lt; data.length; i++) {</span>
<span class="nc" id="L170">      priors[labels[i]]++;</span>
<span class="nc bnc" id="L171" title="All 2 branches missed.">      for (int fno = 0; fno &lt; numFeatures; fno++) {</span>
<span class="nc" id="L172">        weights[labels[i]][fno][data[i][fno]]++;</span>
      }
    }
<span class="nc bnc" id="L175" title="All 2 branches missed.">    for (int cl = 0; cl &lt; numClasses; cl++) {</span>
<span class="nc bnc" id="L176" title="All 2 branches missed.">      for (int fno = 0; fno &lt; numFeatures; fno++) {</span>
<span class="nc bnc" id="L177" title="All 2 branches missed.">        for (int val = 0; val &lt; numValues[fno]; val++) {</span>
<span class="nc" id="L178">          weights[cl][fno][val] = Math.log((weights[cl][fno][val] + alphaFeature) / (priors[cl] + alphaFeature * numValues[fno]));</span>
        }
      }
<span class="nc" id="L181">      priors[cl] = Math.log((priors[cl] + alphaClass) / (data.length + alphaClass * numClasses));</span>
    }
<span class="nc" id="L183">    return new NBWeights(priors, weights);</span>
  }

  private NBWeights trainWeightsUCL(int[][] data, int[] labels, int numFeatures, int numClasses) {
<span class="nc" id="L187">    int[] numValues = numberValues(data, numFeatures);</span>
<span class="nc" id="L188">    int[] sumValues = new int[numFeatures]; //how many feature-values are before this feature</span>
<span class="nc bnc" id="L189" title="All 2 branches missed.">    for (int j = 1; j &lt; numFeatures; j++) {</span>
<span class="nc" id="L190">      sumValues[j] = sumValues[j - 1] + numValues[j - 1];</span>
    }
<span class="nc" id="L192">    int[][] newdata = new int[data.length][numFeatures + 1];</span>
<span class="nc bnc" id="L193" title="All 2 branches missed.">    for (int i = 0; i &lt; data.length; i++) {</span>
<span class="nc" id="L194">      newdata[i][0] = 0;</span>
<span class="nc bnc" id="L195" title="All 2 branches missed.">      for (int j = 0; j &lt; numFeatures; j++) {</span>
<span class="nc" id="L196">        newdata[i][j + 1] = sumValues[j] + data[i][j] + 1;</span>
      }
    }
<span class="nc" id="L199">    int totalFeatures = sumValues[numFeatures - 1] + numValues[numFeatures - 1] + 1;</span>
<span class="nc" id="L200">    logger.info(&quot;total feats &quot; + totalFeatures);</span>
<span class="nc" id="L201">    LogConditionalObjectiveFunction&lt;L, F&gt; objective = new LogConditionalObjectiveFunction&lt;&gt;(totalFeatures, numClasses, newdata, labels, prior, sigma, 0.0);</span>
<span class="nc" id="L202">    Minimizer&lt;DiffFunction&gt; min = new QNMinimizer();</span>
<span class="nc" id="L203">    double[] argmin = min.minimize(objective, 1e-4, objective.initial());</span>
<span class="nc" id="L204">    double[][] wts = objective.to2D(argmin);</span>
<span class="nc" id="L205">    System.out.println(&quot;weights have dimension &quot; + wts.length);</span>
<span class="nc" id="L206">    return new NBWeights(wts, numValues);</span>
  }


  private NBWeights trainWeightsCL(int[][] data, int[] labels, int numFeatures, int numClasses) {

<span class="nc" id="L212">    LogConditionalEqConstraintFunction objective = new LogConditionalEqConstraintFunction(numFeatures, numClasses, data, labels, prior, sigma, 0.0);</span>
<span class="nc" id="L213">    Minimizer&lt;DiffFunction&gt; min = new QNMinimizer();</span>
<span class="nc" id="L214">    double[] argmin = min.minimize(objective, 1e-4, objective.initial());</span>
<span class="nc" id="L215">    double[][][] wts = objective.to3D(argmin);</span>
<span class="nc" id="L216">    double[] priors = objective.priors(argmin);</span>
<span class="nc" id="L217">    return new NBWeights(priors, wts);</span>
  }

  static int[] numberValues(int[][] data, int numFeatures) {
<span class="nc" id="L221">    int[] numValues = new int[numFeatures];</span>
<span class="nc bnc" id="L222" title="All 2 branches missed.">    for (int[] row : data) {</span>
<span class="nc bnc" id="L223" title="All 2 branches missed.">      for (int j = 0; j &lt; row.length; j++) {</span>
<span class="nc bnc" id="L224" title="All 2 branches missed.">        if (numValues[j] &lt; row[j] + 1) {</span>
<span class="nc" id="L225">          numValues[j] = row[j] + 1;</span>
        }
      }
    }
<span class="nc" id="L229">    return numValues;</span>
  }

  static class NBWeights {
    double[] priors;
    double[][][] weights;

<span class="nc" id="L236">    NBWeights(double[] priors, double[][][] weights) {</span>
<span class="nc" id="L237">      this.priors = priors;</span>
<span class="nc" id="L238">      this.weights = weights;</span>
<span class="nc" id="L239">    }</span>

    /**
     * create the parameters from a coded representation
     * where feature 0 is the prior etc.
     *
     */
<span class="nc" id="L246">    NBWeights(double[][] wts, int[] numValues) {</span>
<span class="nc" id="L247">      int numClasses = wts[0].length;</span>
<span class="nc" id="L248">      priors = new double[numClasses];</span>
<span class="nc" id="L249">      synchronized (System.class) {</span>
<span class="nc" id="L250">        System.arraycopy(wts[0], 0, priors, 0, numClasses);</span>
<span class="nc" id="L251">      }</span>
<span class="nc" id="L252">      int[] sumValues = new int[numValues.length];</span>
<span class="nc bnc" id="L253" title="All 2 branches missed.">      for (int j = 1; j &lt; numValues.length; j++) {</span>
<span class="nc" id="L254">        sumValues[j] = sumValues[j - 1] + numValues[j - 1];</span>
      }
<span class="nc" id="L256">      weights = new double[priors.length][sumValues.length][];</span>
<span class="nc bnc" id="L257" title="All 2 branches missed.">      for (int fno = 0; fno &lt; numValues.length; fno++) {</span>
<span class="nc bnc" id="L258" title="All 2 branches missed.">        for (int c = 0; c &lt; numClasses; c++) {</span>
<span class="nc" id="L259">          weights[c][fno] = new double[numValues[fno]];</span>
        }

<span class="nc bnc" id="L262" title="All 2 branches missed.">        for (int val = 0; val &lt; numValues[fno]; val++) {</span>
<span class="nc" id="L263">          int code = sumValues[fno] + val + 1;</span>
<span class="nc bnc" id="L264" title="All 2 branches missed.">          for (int cls = 0; cls &lt; numClasses; cls++) {</span>
<span class="nc" id="L265">            weights[cls][fno][val] = wts[code][cls];</span>
          }
        }
      }
<span class="nc" id="L269">    }</span>
  }

//  public static void main(String[] args) {
//    List examples = new ArrayList();
//    String leftLight = &quot;leftLight&quot;;
//    String rightLight = &quot;rightLight&quot;;
//    String broken = &quot;BROKEN&quot;;
//    String ok = &quot;OK&quot;;
//    Counter c1 = new ClassicCounter&lt;&gt;();
//    c1.incrementCount(leftLight, 0);
//    c1.incrementCount(rightLight, 0);
//    RVFDatum d1 = new RVFDatum(c1, broken);
//    examples.add(d1);
//    Counter c2 = new ClassicCounter&lt;&gt;();
//    c2.incrementCount(leftLight, 1);
//    c2.incrementCount(rightLight, 1);
//    RVFDatum d2 = new RVFDatum(c2, ok);
//    examples.add(d2);
//    Counter c3 = new ClassicCounter&lt;&gt;();
//    c3.incrementCount(leftLight, 0);
//    c3.incrementCount(rightLight, 1);
//    RVFDatum d3 = new RVFDatum(c3, ok);
//    examples.add(d3);
//    Counter c4 = new ClassicCounter&lt;&gt;();
//    c4.incrementCount(leftLight, 1);
//    c4.incrementCount(rightLight, 0);
//    RVFDatum d4 = new RVFDatum(c4, ok);
//    examples.add(d4);
//    Dataset data = new Dataset(examples.size());
//    data.addAll(examples);
//    NaiveBayesClassifier classifier = (NaiveBayesClassifier)
//        new NaiveBayesClassifierFactory(200, 200, 1.0,
//              LogPrior.LogPriorType.QUADRATIC.ordinal(),
//              NaiveBayesClassifierFactory.CL)
//            .trainClassifier(data);
//    classifier.print();
//    //now classifiy
//    for (int i = 0; i &lt; examples.size(); i++) {
//      RVFDatum d = (RVFDatum) examples.get(i);
//      Counter scores = classifier.scoresOf(d);
//      System.out.println(&quot;for datum &quot; + d + &quot; scores are &quot; + scores.toString());
//      System.out.println(&quot; class is &quot; + Counters.topKeys(scores, 1));
//      System.out.println(&quot; class should be &quot; + d.label());
//    }
//  }


//    String trainFile = args[0];
//    String testFile = args[1];
//    NominalDataReader nR = new NominalDataReader();
//    Map&lt;Integer, Index&lt;String&gt;&gt; indices = Generics.newHashMap();
//    List&lt;RVFDatum&lt;String, Integer&gt;&gt; train = nR.readData(trainFile, indices);
//    List&lt;RVFDatum&lt;String, Integer&gt;&gt; test = nR.readData(testFile, indices);
//    System.out.println(&quot;Constrained conditional likelihood no prior :&quot;);
//    for (int j = 0; j &lt; 100; j++) {
//      NaiveBayesClassifier&lt;String, Integer&gt; classifier = new NaiveBayesClassifierFactory&lt;String, Integer&gt;(0.1, 0.01, 0.6, LogPrior.LogPriorType.NULL.ordinal(), NaiveBayesClassifierFactory.CL).trainClassifier(train);
//      classifier.print();
//      //now classifiy
//
//      float accTrain = classifier.accuracy(train.iterator());
//      log.info(&quot;training accuracy &quot; + accTrain);
//      float accTest = classifier.accuracy(test.iterator());
//      log.info(&quot;test accuracy &quot; + accTest);
//
//    }
//    System.out.println(&quot;Unconstrained conditional likelihood no prior :&quot;);
//    for (int j = 0; j &lt; 100; j++) {
//      NaiveBayesClassifier&lt;String, Integer&gt; classifier = new NaiveBayesClassifierFactory&lt;String, Integer&gt;(0.1, 0.01, 0.6, LogPrior.LogPriorType.NULL.ordinal(), NaiveBayesClassifierFactory.UCL).trainClassifier(train);
//      classifier.print();
//      //now classify
//
//      float accTrain = classifier.accuracy(train.iterator());
//      log.info(&quot;training accuracy &quot; + accTrain);
//      float accTest = classifier.accuracy(test.iterator());
//      log.info(&quot;test accuracy &quot; + accTest);
//    }
//  }

  @Override
  public NaiveBayesClassifier&lt;L, F&gt; trainClassifier(GeneralDataset&lt;L, F&gt; dataset) {
<span class="nc bnc" id="L350" title="All 2 branches missed.">    if(dataset instanceof RVFDataset){</span>
<span class="nc" id="L351">      throw new RuntimeException(&quot;Not sure if RVFDataset runs correctly in this method. Please update this code if it does.&quot;);</span>
    }
<span class="nc" id="L353">    return trainClassifier(dataset.getDataArray(), dataset.labels, dataset.numFeatures(),</span>
<span class="nc" id="L354">        dataset.numClasses(), dataset.labelIndex, dataset.featureIndex);</span>
  }

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>