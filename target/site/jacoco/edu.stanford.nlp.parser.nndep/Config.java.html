<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>Config.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.parser.nndep</a> &gt; <span class="el_source">Config.java</span></div><h1>Config.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.parser.nndep;

import edu.stanford.nlp.international.Language;
import edu.stanford.nlp.ling.HasWord;
import edu.stanford.nlp.tagger.maxent.MaxentTagger;
import edu.stanford.nlp.trees.TreebankLanguagePack;
import edu.stanford.nlp.util.PropertiesUtils;
import edu.stanford.nlp.util.ReflectionLoading;

import java.util.List;
import java.util.Properties;
import java.util.function.Function;

/**
 * Defines configuration settings for training and testing the
 * neural-network dependency parser.
 *
 * @see DependencyParser
 *
 * @author Danqi Chen
 * @author Jon Gauthier
 */
public class Config {

  /**
   *   Out-of-vocabulary token string.
   */
  public static final String UNKNOWN = &quot;-UNKNOWN-&quot;;

   /**
   *   Root token string.
   */
  public static final String ROOT = &quot;-ROOT-&quot;;

   /**
   *   Non-existent token string.
   */
  public static final String NULL = &quot;-NULL-&quot;;

   /**
   *   Represent a non-existent token.
   */
  public static final int NONEXIST = -1;

   /**
   *   For printing messages.
   */
  public static final String SEPARATOR = &quot;###################&quot;;

  /**
   * The language being parsed.
   */
<span class="nc" id="L53">  public Language language = Language.UniversalEnglish;</span>

  /**
   * Number of threads to use during training. Also indirectly controls
   * how mini-batches are partitioned (more threads =&gt; more partitions
   * =&gt; smaller partitions).
   */
<span class="nc" id="L60">  public int trainingThreads = 1;</span>

  /**
   * Refuse to train on words which have a corpus frequency less than
   * this number.
   */
<span class="nc" id="L66">  public int wordCutOff = 1;</span>

  /**
   * Model weights will be initialized to random values within the
   * range {@code [-initRange, initRange]}.
   */
<span class="nc" id="L72">  public double initRange = 0.01;</span>

  /**
   * Maximum number of iterations for training
   */
<span class="nc" id="L77">  public int maxIter = 20000;</span>

  /**
   * Size of mini-batch for training. A random subset of training
   * examples of this size will be used to train the classifier on each
   * iteration.
   */
<span class="nc" id="L84">  public int batchSize = 10000;</span>

  /**
   * An epsilon value added to the denominator of the AdaGrad
   * expression for numerical stability
   */
<span class="nc" id="L90">  public double adaEps = 1e-6;</span>

  /**
   * Initial global learning rate for AdaGrad training
   */
<span class="nc" id="L95">  public double adaAlpha = 0.01;</span>

  /**
   * Regularization parameter. All weight updates are scaled by this
   * single parameter.
   */
<span class="nc" id="L101">  public double regParameter = 1e-8;</span>

  /**
   * Dropout probability. For each training example we randomly choose
   * some amount of units to disable in the neural network classifier.
   * This probability controls the proportion of units &quot;dropped out.&quot;
   */
<span class="nc" id="L108">  public double dropProb = 0.5;</span>

  /**
   * Size of the neural network hidden layer.
   */
<span class="nc" id="L113">  public int hiddenSize = 200;</span>

  /**
   * Dimensionality of the word embeddings used
   */
<span class="nc" id="L118">  public int embeddingSize = 50;</span>

  /**
   * Total number of tokens provided as input to the classifier. (Each
   * token is provided in word embedding form.)
   */
  // TODO: we can figure this out automatically based on features used.
  // Should remove this option once we make feature templates / dynamic features
  public static final int numTokens = 48;

  /**
   * Number of input tokens for which we should compute hidden-layer
   * unit activations.
   *
   * If zero, the parser will skip the pre-computation step.
   */
<span class="nc" id="L134">  public int numPreComputed = 100000;</span>

  /**
   * During training, run a full UAS evaluation after every
   * {@code evalPerIter} iterations.
   */
<span class="nc" id="L140">  public int evalPerIter = 100;</span>

  /**
   * During training, clear AdaGrad gradient histories after every
   * {@code clearGradientsPerIter} iterations. (If zero, never clear
   * gradients.)
   */
<span class="nc" id="L147">  public int clearGradientsPerIter = 0;</span>

  /**
   * Save an intermediate model file whenever we see an improved UAS
   * evaluation. (The frequency of these evaluations is configurable as
   * well; see {@link #evalPerIter}.)
   */
<span class="nc" id="L154">  public boolean saveIntermediate = true;</span>


  /**
   * Train a labeled parser if labeled = true, and a unlabeled one otherwise.
   */
<span class="nc" id="L160">  public boolean unlabeled = false;</span>

  /**
   * Use coarse POS instead of fine-grained POS if cPOS = true.
   */
<span class="nc" id="L165">  public boolean cPOS = false;</span>

  /**
  *  Exclude punctuations in evaluation if noPunc = true.
  */
<span class="nc" id="L170">  public boolean noPunc = true;</span>

  /**
  *  Update word embeddings when performing gradient descent.
  *  Set to false if you provide embeddings and do not want to finetune.
  */
<span class="nc" id="L176">  public boolean doWordEmbeddingGradUpdate = true;</span>

  /**
   * Describes language-specific properties necessary for training and
   * testing. By default,
   * {@link edu.stanford.nlp.trees.PennTreebankLanguagePack} will be
   * used.
   */
  public TreebankLanguagePack tlp;

  // --- Runtime parsing options

  /**
   * If non-null, when parsing raw text assume sentences have already
   * been split and are separated by the given delimiter.
   *
   * If null, the parser splits sentences automatically.
   */
<span class="nc" id="L194">  public String sentenceDelimiter = null;</span>

  /**
   * Defines a word-escaper to use when parsing raw sentences.
   *
   * As a command-line option, you should provide the fully qualified
   * class name of a valid escaper (that is, a class which implements
   * {@code Function&lt;List&lt;HasWord&gt;, List&lt;HasWord&gt;&gt;}).
   */
<span class="nc" id="L203">  public Function&lt;List&lt;HasWord&gt;, List&lt;HasWord&gt;&gt; escaper = null;</span>

  /**
   * Path to a tagger file compatible with
   * {@link edu.stanford.nlp.tagger.maxent.MaxentTagger}.
   */
<span class="nc" id="L209">  public String tagger = MaxentTagger.DEFAULT_JAR_PATH;</span>

<span class="nc" id="L211">  public Config(Properties properties) {</span>
<span class="nc" id="L212">    setProperties(properties);</span>
<span class="nc" id="L213">  }</span>

  private void setProperties(Properties props) {
<span class="nc" id="L216">    trainingThreads = PropertiesUtils.getInt(props, &quot;trainingThreads&quot;, trainingThreads);</span>
<span class="nc" id="L217">    wordCutOff = PropertiesUtils.getInt(props, &quot;wordCutOff&quot;, wordCutOff);</span>
<span class="nc" id="L218">    initRange = PropertiesUtils.getDouble(props, &quot;initRange&quot;, initRange);</span>
<span class="nc" id="L219">    maxIter = PropertiesUtils.getInt(props, &quot;maxIter&quot;, maxIter);</span>
<span class="nc" id="L220">    batchSize = PropertiesUtils.getInt(props, &quot;batchSize&quot;, batchSize);</span>
<span class="nc" id="L221">    adaEps = PropertiesUtils.getDouble(props, &quot;adaEps&quot;, adaEps);</span>
<span class="nc" id="L222">    adaAlpha = PropertiesUtils.getDouble(props, &quot;adaAlpha&quot;, adaAlpha);</span>
<span class="nc" id="L223">    regParameter = PropertiesUtils.getDouble(props, &quot;regParameter&quot;, regParameter);</span>
<span class="nc" id="L224">    dropProb = PropertiesUtils.getDouble(props, &quot;dropProb&quot;, dropProb);</span>
<span class="nc" id="L225">    hiddenSize = PropertiesUtils.getInt(props, &quot;hiddenSize&quot;, hiddenSize);</span>
<span class="nc" id="L226">    embeddingSize = PropertiesUtils.getInt(props, &quot;embeddingSize&quot;, embeddingSize);</span>
<span class="nc" id="L227">    numPreComputed = PropertiesUtils.getInt(props, &quot;numPreComputed&quot;, numPreComputed);</span>
<span class="nc" id="L228">    evalPerIter = PropertiesUtils.getInt(props, &quot;evalPerIter&quot;, evalPerIter);</span>
<span class="nc" id="L229">    clearGradientsPerIter = PropertiesUtils.getInt(props, &quot;clearGradientsPerIter&quot;, clearGradientsPerIter);</span>
<span class="nc" id="L230">    saveIntermediate = PropertiesUtils.getBool(props, &quot;saveIntermediate&quot;, saveIntermediate);</span>
<span class="nc" id="L231">    unlabeled = PropertiesUtils.getBool(props, &quot;unlabeled&quot;, unlabeled);</span>
<span class="nc" id="L232">    cPOS = PropertiesUtils.getBool(props, &quot;cPOS&quot;, cPOS);</span>
<span class="nc" id="L233">    noPunc = PropertiesUtils.getBool(props, &quot;noPunc&quot;, noPunc);</span>
<span class="nc" id="L234">    doWordEmbeddingGradUpdate = PropertiesUtils.getBool(props, &quot;doWordEmbeddingGradUpdate&quot;, doWordEmbeddingGradUpdate);</span>

    // Runtime parsing options
<span class="nc" id="L237">    sentenceDelimiter = PropertiesUtils.getString(props, &quot;sentenceDelimiter&quot;, sentenceDelimiter);</span>
<span class="nc" id="L238">    tagger = PropertiesUtils.getString(props, &quot;tagger.model&quot;, tagger);</span>

<span class="nc" id="L240">    String escaperClass = props.getProperty(&quot;escaper&quot;);</span>
<span class="nc bnc" id="L241" title="All 2 branches missed.">    escaper = escaperClass != null ? ReflectionLoading.loadByReflection(escaperClass) : null;</span>

    // Language options
<span class="nc bnc" id="L244" title="All 2 branches missed.">    language = props.containsKey(&quot;language&quot;)</span>
<span class="nc" id="L245">               ? getLanguage(props.getProperty(&quot;language&quot;))</span>
               : language;
<span class="nc" id="L247">    tlp = language.params.treebankLanguagePack();</span>
<span class="nc" id="L248">  }</span>

  /**
   * Get the {@link edu.stanford.nlp.international.Language}
   * object corresponding to the given language string.
   *
   * @return A {@link edu.stanford.nlp.international.Language}
   *         or {@code null} if no instance matches the given string.
   */
  private static Language getLanguage(String languageStr) {
<span class="nc bnc" id="L258" title="All 2 branches missed.">    for (Language l : Language.values()) {</span>
<span class="nc bnc" id="L259" title="All 2 branches missed.">      if (l.name().equalsIgnoreCase(languageStr))</span>
<span class="nc" id="L260">        return l;</span>
    }
<span class="nc" id="L262">    return null;</span>
  }

  public void printParameters() {
<span class="nc" id="L266">    System.err.printf(&quot;language = %s%n&quot;, language);</span>
<span class="nc" id="L267">    System.err.printf(&quot;trainingThreads = %d%n&quot;, trainingThreads);</span>
<span class="nc" id="L268">    System.err.printf(&quot;wordCutOff = %d%n&quot;, wordCutOff);</span>
<span class="nc" id="L269">    System.err.printf(&quot;initRange = %.2g%n&quot;, initRange);</span>
<span class="nc" id="L270">    System.err.printf(&quot;maxIter = %d%n&quot;, maxIter);</span>
<span class="nc" id="L271">    System.err.printf(&quot;batchSize = %d%n&quot;, batchSize);</span>
<span class="nc" id="L272">    System.err.printf(&quot;adaEps = %.2g%n&quot;, adaEps);</span>
<span class="nc" id="L273">    System.err.printf(&quot;adaAlpha = %.2g%n&quot;, adaAlpha);</span>
<span class="nc" id="L274">    System.err.printf(&quot;regParameter = %.2g%n&quot;, regParameter);</span>
<span class="nc" id="L275">    System.err.printf(&quot;dropProb = %.2g%n&quot;, dropProb);</span>
<span class="nc" id="L276">    System.err.printf(&quot;hiddenSize = %d%n&quot;, hiddenSize);</span>
<span class="nc" id="L277">    System.err.printf(&quot;embeddingSize = %d%n&quot;, embeddingSize);</span>
<span class="nc" id="L278">    System.err.printf(&quot;numPreComputed = %d%n&quot;, numPreComputed);</span>
<span class="nc" id="L279">    System.err.printf(&quot;evalPerIter = %d%n&quot;, evalPerIter);</span>
<span class="nc" id="L280">    System.err.printf(&quot;clearGradientsPerIter = %d%n&quot;, clearGradientsPerIter);</span>
<span class="nc" id="L281">    System.err.printf(&quot;saveItermediate = %b%n&quot;, saveIntermediate);</span>
<span class="nc" id="L282">    System.err.printf(&quot;unlabeled = %b%n&quot;, unlabeled);</span>
<span class="nc" id="L283">    System.err.printf(&quot;cPOS = %b%n&quot;, cPOS);</span>
<span class="nc" id="L284">    System.err.printf(&quot;noPunc = %b%n&quot;, noPunc);</span>
<span class="nc" id="L285">    System.err.printf(&quot;doWordEmbeddingGradUpdate = %b%n&quot;, doWordEmbeddingGradUpdate);</span>
<span class="nc" id="L286">  }</span>

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>