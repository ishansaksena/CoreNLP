<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>CGRunner.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.maxent</a> &gt; <span class="el_source">CGRunner.java</span></div><h1>CGRunner.java</h1><pre class="source lang-java linenums">/**
 * Title:       Stanford JavaNLP.
 * Description: A Maximum Entropy Toolkit.
 * Copyright:   Copyright (c) 2002. Kristina Toutanova, Stanford University
 * Company:     Stanford University, All Rights Reserved.
 */
package edu.stanford.nlp.maxent; 
import edu.stanford.nlp.util.logging.Redwood;

import java.util.Arrays;

import edu.stanford.nlp.maxent.iis.LambdaSolve;
import edu.stanford.nlp.optimization.*;
import edu.stanford.nlp.util.ReflectionLoading;

/**
 * This class will call an optimization method such as Conjugate Gradient or
 * Quasi-Newton  on a LambdaSolve object to find
 * optimal parameters, including imposing a Gaussian prior on those
 * parameters.
 *
 * @author Kristina Toutanova
 * @author Christopher Manning
 */
public class CGRunner  {

  /** A logger for this class */
<span class="nc" id="L28">  private static Redwood.RedwoodChannels log = Redwood.channels(CGRunner.class);</span>

  private static final boolean SAVE_LAMBDAS_REGULARLY = false;

  private final LambdaSolve prob;
  private final String filename;
  /**
   * Error tolerance passed to CGMinimizer
   */
  private final double tol;
  private final boolean useGaussianPrior;
  private final double priorSigmaS;
  private final double[] sigmaSquareds; // = null;

  private static final double DEFAULT_TOLERANCE = 1e-4;
  private static final double DEFAULT_SIGMASQUARED = 0.5;


  /**
   * Set up a LambdaSolve problem for solution by a Minimizer.
   * Uses a Gaussian prior with a sigma&lt;sup&gt;2&lt;/sup&gt; of 0.5.
   *
   * @param prob     The problem to solve
   * @param filename Used (with extension) to save intermediate results.
   */
  public CGRunner(LambdaSolve prob, String filename) {
<span class="nc" id="L54">    this(prob, filename, DEFAULT_SIGMASQUARED);</span>
<span class="nc" id="L55">  }</span>

  /**
   * Set up a LambdaSolve problem for solution by a Minimizer,
   * specifying a value for sigma&lt;sup&gt;2&lt;/sup&gt;.
   *
   * @param prob             The problem to solve
   * @param filename         Used (with extension) to save intermediate results.
   * @param priorSigmaS      The prior sigma&lt;sup&gt;2&lt;/sup&gt;: this doubled will be
   *                         used to divide the lambda&lt;sup&gt;2&lt;/sup&gt; values as the
   *                         prior penalty in the likelihood.  A value of 0.0
   *                         or Double.POSITIVE_INFINITY
   *                         indicates to not use regularization.
   */
  public CGRunner(LambdaSolve prob, String filename, double priorSigmaS) {
<span class="nc" id="L70">    this(prob, filename, DEFAULT_TOLERANCE, priorSigmaS);</span>
<span class="nc" id="L71">  }</span>

  /**
   * Set up a LambdaSolve problem for solution by a Minimizer.
   *
   * @param prob             The problem to solve
   * @param filename         Used (with extension) to save intermediate results.
   * @param tol              Tolerance of errors (passed to CG)
   * @param priorSigmaS      The prior sigma&lt;sup&gt;2&lt;/sup&gt;: this doubled will be
   *                         used to divide the lambda&lt;sup&gt;2&lt;/sup&gt; values as the
   *                         prior penalty.  A value of 0.0
   *                         or Double.POSITIVE_INFINITY
   *                         indicates to not use regularization.
   */
<span class="nc" id="L85">  public CGRunner(LambdaSolve prob, String filename, double tol, double priorSigmaS) {</span>
<span class="nc" id="L86">    this.prob = prob;</span>
<span class="nc" id="L87">    this.filename = filename;</span>
<span class="nc" id="L88">    this.tol = tol;</span>
<span class="nc bnc" id="L89" title="All 4 branches missed.">    this.useGaussianPrior = priorSigmaS != 0.0 &amp;&amp; priorSigmaS != Double.POSITIVE_INFINITY;</span>
<span class="nc" id="L90">    this.priorSigmaS = priorSigmaS;</span>
<span class="nc" id="L91">    this.sigmaSquareds = null;</span>
<span class="nc" id="L92">  }</span>


  /**
   * Set up a LambdaSolve problem for solution by a Minimizer.
   *
   * @param prob             The problem to solve
   * @param filename         Used (with extension) to save intermediate results.
   * @param tol              Tolerance of errors (passed to CG)
   * @param sigmaSquareds    The prior sigma&lt;sup&gt;2&lt;/sup&gt; for each feature: this doubled will be
   *                         used to divide the lambda&lt;sup&gt;2&lt;/sup&gt; values as the
   *                         prior penalty. This array must have size the number of features.
   *                         If it is null, no regularization will be performed.
   */
<span class="nc" id="L106">  public CGRunner(LambdaSolve prob, String filename, double tol, double[] sigmaSquareds) {</span>
<span class="nc" id="L107">    this.prob = prob;</span>
<span class="nc" id="L108">    this.filename = filename;</span>
<span class="nc" id="L109">    this.tol = tol;</span>
<span class="nc bnc" id="L110" title="All 2 branches missed.">    this.useGaussianPrior = sigmaSquareds !=null;</span>
<span class="nc" id="L111">    this.sigmaSquareds = sigmaSquareds;</span>
<span class="nc" id="L112">    this.priorSigmaS = -1.0; // not used</span>
<span class="nc" id="L113">  }</span>

  private void printOptimizationResults(LikelihoodFunction df, MonitorFunction monitor) {
<span class="nc" id="L116">    double negLogLike = df.valueAt(prob.lambda);</span>
<span class="nc" id="L117">    System.err.printf(&quot;After optimization neg (penalized) log cond likelihood: %1.2f%n&quot;, negLogLike);</span>
<span class="nc bnc" id="L118" title="All 2 branches missed.">    if (monitor != null) {</span>
<span class="nc" id="L119">      monitor.reportMonitoring(negLogLike);</span>
    }
<span class="nc" id="L121">    int numNonZero = 0;</span>
<span class="nc bnc" id="L122" title="All 2 branches missed.">    for (int i = 0; i &lt; prob.lambda.length; i++) {</span>
<span class="nc bnc" id="L123" title="All 2 branches missed.">      if (prob.lambda[i] != 0.0) {</span>
        // 0.0 == -0.0 in IEEE math!
<span class="nc" id="L125">        numNonZero++;</span>
      }
    }
<span class="nc" id="L128">    System.err.printf(&quot;Non-zero parameters: %d/%d (%1.2f%%)%n&quot;, numNonZero, prob.lambda.length,</span>
<span class="nc" id="L129">        (100.0 * numNonZero) / prob.lambda.length);</span>
<span class="nc" id="L130">  }</span>


  /**
   * Solves the problem using a quasi-newton method (L-BFGS).  The solution
   * is stored in the {@code lambda} array of {@code prob}.
   */
  public void solveQN() {
<span class="nc" id="L138">    LikelihoodFunction df = new LikelihoodFunction(prob, tol, useGaussianPrior, priorSigmaS, sigmaSquareds);</span>
<span class="nc" id="L139">    MonitorFunction monitor = new MonitorFunction(prob, df, filename);</span>
<span class="nc" id="L140">    Minimizer&lt;DiffFunction&gt; cgm = new QNMinimizer(monitor, 10);</span>

    // all parameters are started at 0.0
<span class="nc" id="L143">    prob.lambda = cgm.minimize(df, tol, new double[df.domainDimension()]);</span>
<span class="nc" id="L144">    printOptimizationResults(df, monitor);</span>
<span class="nc" id="L145">  }</span>

  public void solveOWLQN2(double weight) {
<span class="nc" id="L148">    LikelihoodFunction df = new LikelihoodFunction(prob, tol, useGaussianPrior, priorSigmaS, sigmaSquareds);</span>
<span class="nc" id="L149">    MonitorFunction monitor = new MonitorFunction(prob, df, filename);</span>
<span class="nc" id="L150">    Minimizer&lt;DiffFunction&gt; cgm = new QNMinimizer(monitor, 10);</span>
<span class="nc" id="L151">    ((QNMinimizer) cgm).useOWLQN(true, weight);</span>

    // all parameters are started at 0.0
<span class="nc" id="L154">    prob.lambda = cgm.minimize(df, tol, new double[df.domainDimension()]);</span>
<span class="nc" id="L155">    printOptimizationResults(df, monitor);</span>
<span class="nc" id="L156">  }</span>

  /**
   * Solves the problem using conjugate gradient (CG).  The solution
   * is stored in the {@code lambda} array of {@code prob}.
   */
  public void solveCG() {
<span class="nc" id="L163">    LikelihoodFunction df = new LikelihoodFunction(prob, tol, useGaussianPrior, priorSigmaS, sigmaSquareds);</span>
<span class="nc" id="L164">    MonitorFunction monitor = new MonitorFunction(prob, df, filename);</span>
<span class="nc" id="L165">    Minimizer&lt;DiffFunction&gt; cgm = new CGMinimizer(monitor);</span>

    // all parameters are started at 0.0
<span class="nc" id="L168">    prob.lambda = cgm.minimize(df, tol, new double[df.domainDimension()]);</span>
<span class="nc" id="L169">    printOptimizationResults(df, monitor);</span>
<span class="nc" id="L170">  }</span>

  /**
   * Solves the problem using OWLQN.  The solution
   * is stored in the {@code lambda} array of {@code prob}.  Note that the
   * likelihood function will be a penalized L2 likelihood function unless you
   * have turned this off via setting the priorSigmaS to 0.0.
   *
   * @param weight Controls the sparseness/regularization of the L1 solution.
   *     The bigger the number the sparser the solution.  Weights between
   *     0.01 and 1.0 typically give good performance.
   */
  public void solveL1(double weight) {
<span class="nc" id="L183">    LikelihoodFunction df = new LikelihoodFunction(prob, tol, useGaussianPrior, priorSigmaS, sigmaSquareds);</span>
<span class="nc" id="L184">    Minimizer&lt;DiffFunction&gt; owl = ReflectionLoading.loadByReflection(&quot;edu.stanford.nlp.optimization.OWLQNMinimizer&quot;, weight);</span>
<span class="nc" id="L185">    prob.lambda = owl.minimize(df, tol, new double[df.domainDimension()]);</span>
<span class="nc" id="L186">    printOptimizationResults(df, null);</span>
<span class="nc" id="L187">  }</span>


  /**
   * This class implements the DiffFunction interface for Minimizer
   */
  private static final class LikelihoodFunction implements DiffFunction {

    private final LambdaSolve model;
    private final double tol;
    private final boolean useGaussianPrior;
    private final double[] sigmaSquareds;
    private int valueAtCalls;
    private double likelihood;


<span class="nc" id="L203">    public LikelihoodFunction(LambdaSolve m, double tol, boolean useGaussianPrior, double sigmaSquared, double[] sigmaSquareds) {</span>
<span class="nc" id="L204">      model = m;</span>
<span class="nc" id="L205">      this.tol = tol;</span>
<span class="nc" id="L206">      this.useGaussianPrior = useGaussianPrior;</span>
<span class="nc bnc" id="L207" title="All 2 branches missed.">      if (useGaussianPrior) {</span>
        // keep separate prior on each parameter for flexibility
<span class="nc" id="L209">        this.sigmaSquareds = new double[model.lambda.length];</span>
<span class="nc bnc" id="L210" title="All 2 branches missed.">        if (sigmaSquareds != null) {</span>
<span class="nc" id="L211">          System.arraycopy(sigmaSquareds, 0, this.sigmaSquareds, 0, sigmaSquareds.length);</span>
        } else {
<span class="nc" id="L213">          Arrays.fill(this.sigmaSquareds, sigmaSquared);</span>
        }
      } else {
<span class="nc" id="L216">        this.sigmaSquareds = null;</span>
      }
<span class="nc" id="L218">    }</span>

    @Override
    public int domainDimension() {
<span class="nc" id="L222">      return model.lambda.length;</span>
    }

    public double likelihood() {
<span class="nc" id="L226">      return likelihood;</span>
    }

    public int numCalls() {
<span class="nc" id="L230">      return valueAtCalls;</span>
    }


    @Override
    public double valueAt(double[] lambda) {
<span class="nc" id="L236">      valueAtCalls++;</span>
<span class="nc" id="L237">      model.lambda = lambda;</span>
<span class="nc" id="L238">      double lik = model.logLikelihoodScratch();</span>

<span class="nc bnc" id="L240" title="All 2 branches missed.">      if (useGaussianPrior) {</span>
        //double twoSigmaSquared = 2 * sigmaSquared;
<span class="nc bnc" id="L242" title="All 2 branches missed.">        for (int i = 0; i &lt; lambda.length; i++) {</span>
<span class="nc" id="L243">          lik += (lambda[i] * lambda[i]) / (sigmaSquareds[i] + sigmaSquareds[i]);</span>
        }
      }
      // log.info(valueAtCalls + &quot; calls to valueAt;&quot; +
      //		       &quot; penalized log likelihood is &quot; + lik);

<span class="nc" id="L249">      likelihood = lik;</span>
<span class="nc" id="L250">      return lik;</span>
    }


    @Override
    public double[] derivativeAt(double[] lambda) {
<span class="nc" id="L256">      boolean eq = true;</span>
<span class="nc bnc" id="L257" title="All 2 branches missed.">      for (int j = 0; j &lt; lambda.length; j++) {</span>
<span class="nc bnc" id="L258" title="All 2 branches missed.">        if (Math.abs(lambda[j] - model.lambda[j]) &gt; tol) {</span>
<span class="nc" id="L259">          eq = false;</span>
<span class="nc" id="L260">          break;</span>
        }
      }
<span class="nc bnc" id="L263" title="All 2 branches missed.">      if (!eq) {</span>
<span class="nc" id="L264">        log.info(&quot;derivativeAt: call with different value&quot;);</span>
<span class="nc" id="L265">        valueAt(lambda);</span>
      }

<span class="nc" id="L268">      double[] drvs = model.getDerivatives();</span>

      // System.out.println(&quot;for lambdas &quot;+lambda[0]+&quot; &quot;+lambda[1] +
      //                   &quot; derivatives &quot;+drvs[0]+&quot; &quot;+drvs[1]);

<span class="nc bnc" id="L273" title="All 2 branches missed.">      if (useGaussianPrior) {</span>
        // prior penalty
<span class="nc bnc" id="L275" title="All 2 branches missed.">        for (int j = 0; j &lt; lambda.length; j++) {</span>
          // double sign=1;
          // if(lambda[j]&lt;=0){sign=-1;}
<span class="nc" id="L278">          drvs[j] += lambda[j] / sigmaSquareds[j];</span>
        }
      }

      //System.out.println(&quot;final derivatives &quot;+drvs[0]+&quot; &quot;+drvs[1]);
<span class="nc" id="L283">      return drvs;</span>
    }

  } // end static class LikelihoodFunction


  /**
   * This one is used in the monitor
   */
  private static final class MonitorFunction implements Function {

    private final LambdaSolve model;
    private final LikelihoodFunction lf;
    private final String filename;
    private int iterations; // = 0

<span class="nc" id="L299">    public MonitorFunction(LambdaSolve m, LikelihoodFunction lf, String filename) {</span>
<span class="nc" id="L300">      this.model = m;</span>
<span class="nc" id="L301">      this.lf = lf;</span>
<span class="nc" id="L302">      this.filename = filename;</span>
<span class="nc" id="L303">    }</span>

    @Override
    @SuppressWarnings({&quot;ConstantConditions&quot;, &quot;PointlessBooleanExpression&quot;})
    public double valueAt(double[] lambda) {
<span class="nc" id="L308">      double likelihood = lf.likelihood();</span>
      // this line is printed in the middle of the normal line of QN minimization, so put println at beginning
<span class="nc" id="L310">      log.info();</span>
<span class="nc" id="L311">      log.info(reportMonitoring(likelihood));</span>

      if (SAVE_LAMBDAS_REGULARLY  &amp;&amp; iterations &gt; 0 &amp;&amp; iterations % 5 == 0) {
        model.save_lambdas(filename + '.' + iterations + &quot;.lam&quot;);
      }

<span class="nc bnc" id="L317" title="All 4 branches missed.">      if (iterations &gt; 0 &amp;&amp; iterations % 30 == 0) {</span>
<span class="nc" id="L318">        model.checkCorrectness();</span>
      }
<span class="nc" id="L320">      iterations++;</span>

<span class="nc" id="L322">      return 42; // never cause premature termination.</span>
    }

    public String reportMonitoring(double likelihood) {
<span class="nc" id="L326">      return &quot;Iter. &quot; + iterations + &quot;: &quot; + &quot;neg. log cond. likelihood = &quot; + likelihood + &quot; [&quot; + lf.numCalls() + &quot; calls to valueAt]&quot;;</span>
    }

    @Override
    public int domainDimension() {
<span class="nc" id="L331">      return lf.domainDimension();</span>
    }

  } // end static class MonitorFunction

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>