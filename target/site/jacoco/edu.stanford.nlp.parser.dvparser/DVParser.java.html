<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>DVParser.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.parser.dvparser</a> &gt; <span class="el_source">DVParser.java</span></div><h1>DVParser.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.parser.dvparser; 
import edu.stanford.nlp.util.logging.Redwood;

import java.io.FileFilter;
import java.io.FileWriter;
import java.io.IOException;
import java.text.DecimalFormat;
import java.text.NumberFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.IdentityHashMap;
import java.util.List;
import java.util.Random;
import edu.stanford.nlp.io.IOUtils;
import edu.stanford.nlp.io.RuntimeIOException;
import edu.stanford.nlp.ling.Word;
import edu.stanford.nlp.math.ArrayMath;
import edu.stanford.nlp.optimization.QNMinimizer;
import edu.stanford.nlp.parser.common.ArgUtils;
import edu.stanford.nlp.parser.common.ParserQuery;
import edu.stanford.nlp.parser.lexparser.EvaluateTreebank;
import edu.stanford.nlp.parser.lexparser.LexicalizedParser;
import edu.stanford.nlp.parser.lexparser.Options;
import edu.stanford.nlp.parser.lexparser.TrainOptions;
import edu.stanford.nlp.trees.CompositeTreeTransformer;
import edu.stanford.nlp.trees.TreeTransformer;
import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.Treebank;
import edu.stanford.nlp.trees.Trees;
import edu.stanford.nlp.util.Generics;
import edu.stanford.nlp.util.Pair;
import edu.stanford.nlp.util.ScoredObject;
import edu.stanford.nlp.util.Timing;

/**
 * @author John Bauer &amp;amp; Richard Socher
 */
public class DVParser  {

  /** A logger for this class */
<span class="nc" id="L43">  private static Redwood.RedwoodChannels log = Redwood.channels(DVParser.class);</span>
  DVModel dvModel;
  LexicalizedParser parser;
  Options op;

  public Options getOp() {
<span class="nc" id="L49">    return op;</span>
  }

  DVModel getDVModel() {
<span class="nc" id="L53">    return dvModel;</span>
  }

<span class="nc" id="L56">  private static final NumberFormat NF = new DecimalFormat(&quot;0.00&quot;);</span>
<span class="nc" id="L57">  private static final NumberFormat FILENAME = new DecimalFormat(&quot;0000&quot;);</span>

  static public List&lt;Tree&gt; getTopParsesForOneTree(LexicalizedParser parser, int dvKBest, Tree tree,
                                                  TreeTransformer transformer) {
<span class="nc" id="L61">    ParserQuery pq = parser.parserQuery();</span>
<span class="nc" id="L62">    List&lt;Word&gt; sentence = tree.yieldWords();</span>
    // Since the trees are binarized and otherwise manipulated, we
    // need to chop off the last word in order to remove the end of
    // sentence symbol
<span class="nc bnc" id="L66" title="All 2 branches missed.">    if (sentence.size() &lt;= 1) {</span>
<span class="nc" id="L67">      return null;</span>
    }
<span class="nc" id="L69">    sentence = sentence.subList(0, sentence.size() - 1);</span>
<span class="nc bnc" id="L70" title="All 2 branches missed.">    if (!pq.parse(sentence)) {</span>
<span class="nc" id="L71">      log.info(&quot;Failed to use the given parser to reparse sentence \&quot;&quot; + sentence + &quot;\&quot;&quot;);</span>
<span class="nc" id="L72">      return null;</span>
    }
<span class="nc" id="L74">    List&lt;Tree&gt; parses = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L75">    List&lt;ScoredObject&lt;Tree&gt;&gt; bestKParses = pq.getKBestPCFGParses(dvKBest);</span>
<span class="nc bnc" id="L76" title="All 2 branches missed.">    for (ScoredObject&lt;Tree&gt; so : bestKParses) {</span>
<span class="nc" id="L77">      Tree result = so.object();</span>
<span class="nc bnc" id="L78" title="All 2 branches missed.">      if (transformer != null) {</span>
<span class="nc" id="L79">        result = transformer.transformTree(result);</span>
      }
<span class="nc" id="L81">      parses.add(result);</span>
<span class="nc" id="L82">    }</span>
<span class="nc" id="L83">    return parses;</span>
  }

  static IdentityHashMap&lt;Tree, List&lt;Tree&gt;&gt; getTopParses(LexicalizedParser parser, Options op,
                                                        Collection&lt;Tree&gt; trees, TreeTransformer transformer,
                                                        boolean outputUpdates) {
<span class="nc" id="L89">    IdentityHashMap&lt;Tree, List&lt;Tree&gt;&gt; topParses = new IdentityHashMap&lt;&gt;();</span>
<span class="nc bnc" id="L90" title="All 2 branches missed.">    for (Tree tree : trees) {</span>
<span class="nc" id="L91">      List&lt;Tree&gt; parses = getTopParsesForOneTree(parser, op.trainOptions.dvKBest, tree, transformer);</span>
<span class="nc" id="L92">      topParses.put(tree, parses);</span>
<span class="nc bnc" id="L93" title="All 4 branches missed.">      if (outputUpdates &amp;&amp; topParses.size() % 10 == 0) {</span>
<span class="nc" id="L94">        log.info(&quot;Processed &quot; + topParses.size() + &quot; trees&quot;);</span>
      }
<span class="nc" id="L96">    }</span>
<span class="nc bnc" id="L97" title="All 2 branches missed.">    if (outputUpdates) {</span>
<span class="nc" id="L98">      log.info(&quot;Finished processing &quot; + topParses.size() + &quot; trees&quot;);</span>
    }
<span class="nc" id="L100">    return topParses;</span>
  }

  IdentityHashMap&lt;Tree, List&lt;Tree&gt;&gt; getTopParses(List&lt;Tree&gt; trees, TreeTransformer transformer) {
<span class="nc" id="L104">    return getTopParses(parser, op, trees, transformer, false);</span>
  }

  public void train(List&lt;Tree&gt; sentences, IdentityHashMap&lt;Tree, byte[]&gt; compressedParses, Treebank testTreebank, String modelPath, String resultsRecordPath) throws IOException {
    // process:
    //   we come up with a cost and a derivative for the model
    //   we always use the gold tree as the example to train towards
    //   every time through, we will look at the top N trees from
    //     the LexicalizedParser and pick the best one according to
    //     our model (at the start, this is essentially random)
    // we use QN to minimize the cost function for the model
    // to do this minimization, we turn all of the matrices in the
    //   DVModel into one big Theta, which is the set of variables to
    //   be optimized by the QN.

<span class="nc" id="L119">    Timing timing = new Timing();</span>
<span class="nc" id="L120">    long maxTrainTimeMillis = op.trainOptions.maxTrainTimeSeconds * 1000;</span>
<span class="nc" id="L121">    int batchCount = 0;</span>
<span class="nc" id="L122">    int debugCycle = 0;</span>
<span class="nc" id="L123">    double bestLabelF1 = 0.0;</span>

<span class="nc bnc" id="L125" title="All 2 branches missed.">    if (op.trainOptions.useContextWords) {</span>
<span class="nc bnc" id="L126" title="All 2 branches missed.">      for (Tree tree : sentences) {</span>
<span class="nc" id="L127">        Trees.convertToCoreLabels(tree);</span>
<span class="nc" id="L128">        tree.setSpans();</span>
<span class="nc" id="L129">      }</span>
    }

    // for AdaGrad
<span class="nc" id="L133">    double[] sumGradSquare = new double[dvModel.totalParamSize()];</span>
<span class="nc" id="L134">    Arrays.fill(sumGradSquare, 1.0);</span>

<span class="nc" id="L136">    int numBatches = sentences.size() / op.trainOptions.batchSize + 1;</span>
<span class="nc" id="L137">    log.info(&quot;Training on &quot; + sentences.size() + &quot; trees in &quot; + numBatches + &quot; batches&quot;);</span>
<span class="nc" id="L138">    log.info(&quot;Times through each training batch: &quot; + op.trainOptions.trainingIterations);</span>
<span class="nc" id="L139">    log.info(&quot;QN iterations per batch: &quot; + op.trainOptions.qnIterationsPerBatch);</span>
<span class="nc bnc" id="L140" title="All 2 branches missed.">    for (int iter = 0; iter &lt; op.trainOptions.trainingIterations; ++iter) {</span>
<span class="nc" id="L141">      List&lt;Tree&gt; shuffledSentences = new ArrayList&lt;&gt;(sentences);</span>
<span class="nc" id="L142">      Collections.shuffle(shuffledSentences, dvModel.rand);</span>
<span class="nc bnc" id="L143" title="All 2 branches missed.">      for (int batch = 0; batch &lt; numBatches; ++batch) {</span>
<span class="nc" id="L144">        ++batchCount;</span>
        // This did not help performance
        //log.info(&quot;Setting AdaGrad's sum of squares to 1...&quot;);
        //Arrays.fill(sumGradSquare, 1.0);

<span class="nc" id="L149">        log.info(&quot;======================================&quot;);</span>
<span class="nc" id="L150">        log.info(&quot;Iteration &quot; + iter + &quot; batch &quot; + batch);</span>

        // Each batch will be of the specified batch size, except the
        // last batch will include any leftover trees at the end of
        // the list
<span class="nc" id="L155">        int startTree = batch * op.trainOptions.batchSize;</span>
<span class="nc" id="L156">        int endTree = (batch + 1) * op.trainOptions.batchSize;</span>
<span class="nc bnc" id="L157" title="All 2 branches missed.">        if (endTree &gt; shuffledSentences.size()) {</span>
<span class="nc" id="L158">          endTree = shuffledSentences.size();</span>
        }

<span class="nc" id="L161">        executeOneTrainingBatch(shuffledSentences.subList(startTree, endTree), compressedParses, sumGradSquare);</span>

<span class="nc" id="L163">        long totalElapsed = timing.report();</span>
<span class="nc" id="L164">        log.info(&quot;Finished iteration &quot; + iter + &quot; batch &quot; + batch + &quot;; total training time &quot; + totalElapsed + &quot; ms&quot;);</span>

<span class="nc bnc" id="L166" title="All 4 branches missed.">        if (maxTrainTimeMillis &gt; 0 &amp;&amp; totalElapsed &gt; maxTrainTimeMillis) {</span>
          // no need to debug output, we're done now
<span class="nc" id="L168">          break;</span>
        }

<span class="nc bnc" id="L171" title="All 4 branches missed.">        if (op.trainOptions.debugOutputFrequency &gt; 0 &amp;&amp; batchCount % op.trainOptions.debugOutputFrequency == 0) {</span>
<span class="nc" id="L172">          log.info(&quot;Finished &quot; + batchCount + &quot; total batches, running evaluation cycle&quot;);</span>
          // Time for debugging output!
<span class="nc" id="L174">          double tagF1 = 0.0;</span>
<span class="nc" id="L175">          double labelF1 = 0.0;</span>
<span class="nc bnc" id="L176" title="All 2 branches missed.">          if (testTreebank != null) {</span>
<span class="nc" id="L177">            EvaluateTreebank evaluator = new EvaluateTreebank(attachModelToLexicalizedParser());</span>
<span class="nc" id="L178">            evaluator.testOnTreebank(testTreebank);</span>
<span class="nc" id="L179">            labelF1 = evaluator.getLBScore();</span>
<span class="nc" id="L180">            tagF1 = evaluator.getTagScore();</span>
<span class="nc bnc" id="L181" title="All 2 branches missed.">            if (labelF1 &gt; bestLabelF1) {</span>
<span class="nc" id="L182">              bestLabelF1 = labelF1;</span>
            }
<span class="nc" id="L184">            log.info(&quot;Best label f1 on dev set so far: &quot; + NF.format(bestLabelF1));</span>
          }

<span class="nc" id="L187">          String tempName = null;</span>
<span class="nc bnc" id="L188" title="All 2 branches missed.">          if (modelPath != null) {</span>
<span class="nc" id="L189">            tempName = modelPath;</span>
<span class="nc bnc" id="L190" title="All 2 branches missed.">            if (modelPath.endsWith(&quot;.ser.gz&quot;)) {</span>
<span class="nc" id="L191">              tempName = modelPath.substring(0, modelPath.length() - 7) + &quot;-&quot; + FILENAME.format(debugCycle) + &quot;-&quot; + NF.format(labelF1) + &quot;.ser.gz&quot;;</span>
            }
<span class="nc" id="L193">            saveModel(tempName);</span>
          }

<span class="nc" id="L196">          String statusLine = (&quot;CHECKPOINT:&quot; +</span>
                               &quot; iteration &quot; + iter +
                               &quot; batch &quot; + batch +
<span class="nc" id="L199">                               &quot; labelF1 &quot; + NF.format(labelF1) +</span>
<span class="nc" id="L200">                               &quot; tagF1 &quot; + NF.format(tagF1) +</span>
<span class="nc" id="L201">                               &quot; bestLabelF1 &quot; + NF.format(bestLabelF1) +</span>
                               &quot; model &quot; + tempName +
                               op.trainOptions +
                               &quot; word vectors: &quot; + op.lexOptions.wordVectorFile +
                               &quot; numHid: &quot; + op.lexOptions.numHid);
<span class="nc" id="L206">          log.info(statusLine);</span>
<span class="nc bnc" id="L207" title="All 2 branches missed.">          if (resultsRecordPath != null) {</span>
<span class="nc" id="L208">            FileWriter fout = new FileWriter(resultsRecordPath, true); // append</span>
<span class="nc" id="L209">            fout.write(statusLine);</span>
<span class="nc" id="L210">            fout.write(&quot;\n&quot;);</span>
<span class="nc" id="L211">            fout.close();</span>
          }

<span class="nc" id="L214">          ++debugCycle;</span>
        }
      }
<span class="nc" id="L217">      long totalElapsed = timing.report();</span>

<span class="nc bnc" id="L219" title="All 4 branches missed.">      if (maxTrainTimeMillis &gt; 0 &amp;&amp; totalElapsed &gt; maxTrainTimeMillis) {</span>
        // no need to debug output, we're done now
<span class="nc" id="L221">        log.info(&quot;Max training time exceeded, exiting&quot;);</span>
<span class="nc" id="L222">        break;</span>
      }
    }
<span class="nc" id="L225">  }</span>

  static final int MINIMIZER = 3;

  public void executeOneTrainingBatch(List&lt;Tree&gt; trainingBatch, IdentityHashMap&lt;Tree, byte[]&gt; compressedParses, double[] sumGradSquare) {
<span class="nc" id="L230">    Timing convertTiming = new Timing();</span>
<span class="nc" id="L231">    convertTiming.doing(&quot;Converting trees&quot;);</span>
<span class="nc" id="L232">    IdentityHashMap&lt;Tree, List&lt;Tree&gt;&gt; topParses = CacheParseHypotheses.convertToTrees(trainingBatch, compressedParses, op.trainOptions.trainingThreads);</span>
<span class="nc" id="L233">    convertTiming.done();</span>

<span class="nc" id="L235">    DVParserCostAndGradient gcFunc = new DVParserCostAndGradient(trainingBatch, topParses, dvModel, op);</span>
<span class="nc" id="L236">    double[] theta = dvModel.paramsToVector();</span>

    //maxFuncIter = 10;
    // 1: QNMinimizer, 2: SGD
<span class="nc bnc" id="L240" title="All 4 branches missed.">    switch (MINIMIZER) {</span>
    case (1): {
<span class="nc" id="L242">      QNMinimizer qn = new QNMinimizer(op.trainOptions.qnEstimates, true);</span>
<span class="nc" id="L243">      qn.useMinPackSearch();</span>
<span class="nc" id="L244">      qn.useDiagonalScaling();</span>
<span class="nc" id="L245">      qn.terminateOnAverageImprovement(true);</span>
<span class="nc" id="L246">      qn.terminateOnNumericalZero(true);</span>
<span class="nc" id="L247">      qn.terminateOnRelativeNorm(true);</span>

<span class="nc" id="L249">      theta = qn.minimize(gcFunc, op.trainOptions.qnTolerance, theta, op.trainOptions.qnIterationsPerBatch);</span>
<span class="nc" id="L250">      break;</span>
    }
    case 2:{
      //Minimizer smd = new SGDMinimizer();    	double tol = 1e-4;    	theta = smd.minimize(gcFunc,tol,theta,op.trainOptions.qnIterationsPerBatch);
<span class="nc" id="L254">      double lastCost = 0, currCost = 0;</span>
<span class="nc" id="L255">      boolean firstTime = true;</span>
<span class="nc bnc" id="L256" title="All 2 branches missed.">      for(int i = 0; i &lt; op.trainOptions.qnIterationsPerBatch; i++){</span>
        //gcFunc.calculate(theta);
<span class="nc" id="L258">        double[] grad = gcFunc.derivativeAt(theta);</span>
<span class="nc" id="L259">        currCost = gcFunc.valueAt(theta);</span>
<span class="nc" id="L260">        log.info(&quot;batch cost: &quot; + currCost);</span>
        //    		if(!firstTime){
        //    			if(currCost &gt; lastCost){
        //    				System.out.println(&quot;HOW IS FUNCTION VALUE INCREASING????!!! ... still updating theta&quot;);
        //    			}
        //    			if(Math.abs(currCost - lastCost) &lt; 0.0001){
        //    				System.out.println(&quot;function value is not decreasing. stop&quot;);
        //    			}
        //    		}else{
        //    			firstTime = false;
        //    		}
<span class="nc" id="L271">        lastCost = currCost;</span>
<span class="nc" id="L272">        ArrayMath.addMultInPlace(theta, grad, -1*op.trainOptions.learningRate);</span>
      }
<span class="nc" id="L274">      break;</span>
    }
    case 3:{
      // AdaGrad
<span class="nc" id="L278">      double eps = 1e-3;</span>
<span class="nc" id="L279">      double currCost = 0;</span>
<span class="nc bnc" id="L280" title="All 2 branches missed.">      for(int i = 0; i &lt; op.trainOptions.qnIterationsPerBatch; i++){</span>
<span class="nc" id="L281">        double[] gradf = gcFunc.derivativeAt(theta);</span>
<span class="nc" id="L282">        currCost = gcFunc.valueAt(theta);</span>
<span class="nc" id="L283">        log.info(&quot;batch cost: &quot; + currCost);</span>
<span class="nc bnc" id="L284" title="All 2 branches missed.">        for (int feature =0; feature&lt;gradf.length;feature++ ) {</span>
<span class="nc" id="L285">          sumGradSquare[feature] = sumGradSquare[feature] + gradf[feature]*gradf[feature];</span>
<span class="nc" id="L286">          theta[feature] = theta[feature] - (op.trainOptions.learningRate * gradf[feature]/(Math.sqrt(sumGradSquare[feature])+eps));</span>
        }
      }
<span class="nc" id="L289">      break;</span>
    }
    default: {
<span class="nc" id="L292">      throw new IllegalArgumentException(&quot;Unsupported minimizer &quot; + MINIMIZER);</span>
    }
    }


<span class="nc" id="L297">    dvModel.vectorToParams(theta);</span>
<span class="nc" id="L298">  }</span>

<span class="nc" id="L300">  public DVParser(DVModel model, LexicalizedParser parser) {</span>
<span class="nc" id="L301">    this.parser = parser;</span>
<span class="nc" id="L302">    this.op = parser.getOp();</span>
<span class="nc" id="L303">    this.dvModel = model;</span>
<span class="nc" id="L304">  }</span>

<span class="nc" id="L306">  public DVParser(LexicalizedParser parser) {</span>
<span class="nc" id="L307">    this.parser = parser;</span>
<span class="nc" id="L308">    this.op = parser.getOp();</span>

<span class="nc bnc" id="L310" title="All 2 branches missed.">    if (op.trainOptions.randomSeed == 0) {</span>
<span class="nc" id="L311">      op.trainOptions.randomSeed = System.nanoTime();</span>
<span class="nc" id="L312">      log.info(&quot;Random seed not set, using randomly chosen seed of &quot; + op.trainOptions.randomSeed);</span>
    } else {
<span class="nc" id="L314">      log.info(&quot;Random seed set to &quot; + op.trainOptions.randomSeed);</span>
    }

<span class="nc" id="L317">    log.info(&quot;Word vector file: &quot; + op.lexOptions.wordVectorFile);</span>
<span class="nc" id="L318">    log.info(&quot;Size of word vectors: &quot; + op.lexOptions.numHid);</span>
<span class="nc" id="L319">    log.info(&quot;Number of hypothesis trees to train against: &quot; + op.trainOptions.dvKBest);</span>
<span class="nc" id="L320">    log.info(&quot;Number of trees in one batch: &quot; + op.trainOptions.batchSize);</span>
<span class="nc" id="L321">    log.info(&quot;Number of iterations of trees: &quot; + op.trainOptions.trainingIterations);</span>
<span class="nc" id="L322">    log.info(&quot;Number of qn iterations per batch: &quot; + op.trainOptions.qnIterationsPerBatch);</span>
<span class="nc" id="L323">    log.info(&quot;Learning rate: &quot; + op.trainOptions.learningRate);</span>
<span class="nc" id="L324">    log.info(&quot;Delta margin: &quot; + op.trainOptions.deltaMargin);</span>
<span class="nc" id="L325">    log.info(&quot;regCost: &quot; + op.trainOptions.regCost);</span>
<span class="nc" id="L326">    log.info(&quot;Using unknown word vector for numbers: &quot; + op.trainOptions.unknownNumberVector);</span>
<span class="nc" id="L327">    log.info(&quot;Using unknown dashed word vector heuristics: &quot; + op.trainOptions.unknownDashedWordVectors);</span>
<span class="nc" id="L328">    log.info(&quot;Using unknown word vector for capitalized words: &quot; + op.trainOptions.unknownCapsVector);</span>
<span class="nc" id="L329">    log.info(&quot;Using unknown number vector for Chinese words: &quot; + op.trainOptions.unknownChineseNumberVector);</span>
<span class="nc" id="L330">    log.info(&quot;Using unknown year vector for Chinese words: &quot; + op.trainOptions.unknownChineseYearVector);</span>
<span class="nc" id="L331">    log.info(&quot;Using unknown percent vector for Chinese words: &quot; + op.trainOptions.unknownChinesePercentVector);</span>
<span class="nc" id="L332">    log.info(&quot;Initial matrices scaled by: &quot; + op.trainOptions.scalingForInit);</span>
<span class="nc" id="L333">    log.info(&quot;Training will use &quot; + op.trainOptions.trainingThreads + &quot; thread(s)&quot;);</span>
<span class="nc bnc" id="L334" title="All 2 branches missed.">    log.info(&quot;Context words are &quot; + ((op.trainOptions.useContextWords) ? &quot;on&quot; : &quot;off&quot;));</span>
<span class="nc bnc" id="L335" title="All 2 branches missed.">    log.info(&quot;Model will &quot; + ((op.trainOptions.dvSimplifiedModel) ? &quot;&quot; : &quot;not &quot;) + &quot;be simplified&quot;);</span>

<span class="nc" id="L337">    this.dvModel = new DVModel(op, parser.stateIndex, parser.ug, parser.bg);</span>

<span class="nc bnc" id="L339" title="All 2 branches missed.">    if (dvModel.unaryTransform.size() != dvModel.unaryScore.size()) {</span>
<span class="nc" id="L340">      throw new AssertionError(&quot;Unary transform and score size not the same&quot;);</span>
    }
<span class="nc bnc" id="L342" title="All 2 branches missed.">    if (dvModel.binaryTransform.size() != dvModel.binaryScore.size()) {</span>
<span class="nc" id="L343">      throw new AssertionError(&quot;Binary transform and score size not the same&quot;);</span>
    }
<span class="nc" id="L345">  }</span>

  public boolean runGradientCheck(List&lt;Tree&gt; sentences, IdentityHashMap&lt;Tree, byte[]&gt; compressedParses) {
<span class="nc" id="L348">    log.info(&quot;Gradient check: converting &quot; + sentences.size() + &quot; compressed trees&quot;);</span>
<span class="nc" id="L349">    IdentityHashMap&lt;Tree, List&lt;Tree&gt;&gt; topParses = CacheParseHypotheses.convertToTrees(sentences, compressedParses, op.trainOptions.trainingThreads);</span>
<span class="nc" id="L350">    log.info(&quot;Done converting trees&quot;);</span>
<span class="nc" id="L351">    DVParserCostAndGradient gcFunc = new DVParserCostAndGradient(sentences, topParses, dvModel, op);</span>
<span class="nc" id="L352">    return gcFunc.gradientCheck(1000, 50, dvModel.paramsToVector());</span>
  }

  public static TreeTransformer buildTrainTransformer(Options op) {
<span class="nc" id="L356">    CompositeTreeTransformer transformer = LexicalizedParser.buildTrainTransformer(op);</span>
<span class="nc" id="L357">    return transformer;</span>
  }

  public LexicalizedParser attachModelToLexicalizedParser() {
<span class="nc" id="L361">    LexicalizedParser newParser = LexicalizedParser.copyLexicalizedParser(parser);</span>
<span class="nc" id="L362">    DVModelReranker reranker = new DVModelReranker(dvModel);</span>
<span class="nc" id="L363">    newParser.reranker = reranker;</span>
<span class="nc" id="L364">    return newParser;</span>
  }

  public void saveModel(String filename) {
<span class="nc" id="L368">    log.info(&quot;Saving serialized model to &quot; + filename);</span>
<span class="nc" id="L369">    LexicalizedParser newParser = attachModelToLexicalizedParser();</span>
<span class="nc" id="L370">    newParser.saveParserToSerialized(filename);</span>
<span class="nc" id="L371">    log.info(&quot;... done&quot;);</span>
<span class="nc" id="L372">  }</span>

  public static DVParser loadModel(String filename, String[] args) {
<span class="nc" id="L375">    log.info(&quot;Loading serialized model from &quot; + filename);</span>
    DVParser dvparser;
    try {
<span class="nc" id="L378">      dvparser = IOUtils.readObjectFromURLOrClasspathOrFileSystem(filename);</span>
<span class="nc" id="L379">      dvparser.op.setOptions(args);</span>
<span class="nc" id="L380">    } catch (IOException e) {</span>
<span class="nc" id="L381">      throw new RuntimeIOException(e);</span>
<span class="nc" id="L382">    } catch (ClassNotFoundException e) {</span>
<span class="nc" id="L383">      throw new RuntimeIOException(e);</span>
<span class="nc" id="L384">    }</span>
<span class="nc" id="L385">    log.info(&quot;... done&quot;);</span>
<span class="nc" id="L386">    return dvparser;</span>
  }

  public static DVModel getModelFromLexicalizedParser(LexicalizedParser parser) {
<span class="nc bnc" id="L390" title="All 2 branches missed.">    if (!(parser.reranker instanceof DVModelReranker)) {</span>
<span class="nc" id="L391">      throw new IllegalArgumentException(&quot;This parser does not contain a DVModel reranker&quot;);</span>
    }
<span class="nc" id="L393">    DVModelReranker reranker = (DVModelReranker) parser.reranker;</span>
<span class="nc" id="L394">    return reranker.getModel();</span>
  }

  public static void help() {
<span class="nc" id="L398">    log.info(&quot;Options supplied by this file:&quot;);</span>
<span class="nc" id="L399">    log.info(&quot;  -model &lt;name&gt;: When training, the name of the model to save.  Otherwise, the name of the model to load.&quot;);</span>
<span class="nc" id="L400">    log.info(&quot;  -parser &lt;name&gt;: When training, the LexicalizedParser to use as the base model.&quot;);</span>
<span class="nc" id="L401">    log.info(&quot;  -cachedTrees &lt;name&gt;: The name of the file containing a treebank with cached parses.  See CacheParseHypotheses.java&quot;);</span>
<span class="nc" id="L402">    log.info(&quot;  -treebank &lt;name&gt; [filter]: A treebank to use instead of cachedTrees.  Trees will be reparsed.  Slow.&quot;);</span>
<span class="nc" id="L403">    log.info(&quot;  -testTreebank &lt;name&gt; [filter]: A treebank for testing the model.&quot;);</span>
<span class="nc" id="L404">    log.info(&quot;  -train: Run training over the treebank, testing on the testTreebank.&quot;);</span>
<span class="nc" id="L405">    log.info(&quot;  -continueTraining &lt;name&gt;: The name of a file to continue training.&quot;);</span>
<span class="nc" id="L406">    log.info(&quot;  -nofilter: Rules for the parser will not be filtered based on the training treebank.&quot;);</span>
<span class="nc" id="L407">    log.info(&quot;  -runGradientCheck: Run a gradient check.&quot;);</span>
<span class="nc" id="L408">    log.info(&quot;  -resultsRecord: A file for recording info on intermediate results&quot;);</span>
<span class="nc" id="L409">    log.info();</span>
<span class="nc" id="L410">    log.info(&quot;Options overlapping the parser:&quot;);</span>
<span class="nc" id="L411">    log.info(&quot;  -trainingThreads &lt;int&gt;: How many threads to use when training.&quot;);</span>
<span class="nc" id="L412">    log.info(&quot;  -dvKBest &lt;int&gt;: How many hypotheses to use from the underlying parser.&quot;);</span>
<span class="nc" id="L413">    log.info(&quot;  -trainingIterations &lt;int&gt;: When training, how many times to go through the train set.&quot;);</span>
<span class="nc" id="L414">    log.info(&quot;  -regCost &lt;double&gt;: How large of a cost to put on regularization.&quot;);</span>
<span class="nc" id="L415">    log.info(&quot;  -batchSize &lt;int&gt;: How many trees to use in each batch of the training.&quot;);</span>
<span class="nc" id="L416">    log.info(&quot;  -qnIterationsPerBatch &lt;int&gt;: How many steps to take per batch.&quot;);</span>
<span class="nc" id="L417">    log.info(&quot;  -qnEstimates &lt;int&gt;: Parameter for qn optimization.&quot;);</span>
<span class="nc" id="L418">    log.info(&quot;  -qnTolerance &lt;double&gt;: Tolerance for early exit when optimizing a batch.&quot;);</span>
<span class="nc" id="L419">    log.info(&quot;  -debugOutputFrequency &lt;int&gt;: How frequently to score a model when training and write out intermediate models.&quot;);</span>
<span class="nc" id="L420">    log.info(&quot;  -maxTrainTimeSeconds &lt;int&gt;: How long to train before terminating.&quot;);</span>
<span class="nc" id="L421">    log.info(&quot;  -randomSeed &lt;long&gt;: A starting point for the random number generator.  Setting this should lead to repeatable results, even taking into account randomness.  Otherwise, a new random seed will be picked.&quot;);</span>
<span class="nc" id="L422">    log.info(&quot;  -wordVectorFile &lt;name&gt;: A filename to load word vectors from.&quot;);</span>
<span class="nc" id="L423">    log.info(&quot;  -numHid: The size of the matrices.  In most circumstances, should be set to the size of the word vectors.&quot;);</span>
<span class="nc" id="L424">    log.info(&quot;  -learningRate: The rate of optimization when training&quot;);</span>
<span class="nc" id="L425">    log.info(&quot;  -deltaMargin: How much we punish trees for being incorrect when training&quot;);</span>
<span class="nc" id="L426">    log.info(&quot;  -(no)unknownNumberVector: Whether or not to use a word vector for unknown numbers&quot;);</span>
<span class="nc" id="L427">    log.info(&quot;  -(no)unknownDashedWordVectors: Whether or not to split unknown dashed words&quot;);</span>
<span class="nc" id="L428">    log.info(&quot;  -(no)unknownCapsVector: Whether or not to use a word vector for unknown words with capitals&quot;);</span>
<span class="nc" id="L429">    log.info(&quot;  -dvSimplifiedModel: Use a greatly dumbed down DVModel&quot;);</span>
<span class="nc" id="L430">    log.info(&quot;  -scalingForInit: How much to scale matrices when creating a new DVModel&quot;);</span>
<span class="nc" id="L431">    log.info(&quot;  -baseParserWeight: A weight to give the original LexicalizedParser when testing (0.2 seems to work well for English)&quot;);</span>
<span class="nc" id="L432">    log.info(&quot;  -unkWord: The vector representing unknown word in the word vectors file&quot;);</span>
<span class="nc" id="L433">    log.info(&quot;  -transformMatrixType: A couple different methods for initializing transform matrices&quot;);</span>
<span class="nc" id="L434">    log.info(&quot;  -(no)trainWordVectors: whether or not to train the word vectors along with the matrices.  True by default&quot;);</span>
<span class="nc" id="L435">  }</span>

  /**
   * An example command line for training a new parser:
   * &lt;br&gt;
   *  nohup java -mx6g edu.stanford.nlp.parser.dvparser.DVParser -cachedTrees /scr/nlp/data/dvparser/wsj/cached.wsj.train.simple.ser.gz -train -testTreebank  /afs/ir/data/linguistic-data/Treebank/3/parsed/mrg/wsj/22 2200-2219 -debugOutputFrequency 400 -nofilter -trainingThreads 5 -parser /u/nlp/data/lexparser/wsjPCFG.nocompact.simple.ser.gz -trainingIterations 40 -batchSize 25 -model /scr/nlp/data/dvparser/wsj/wsj.combine.v2.ser.gz -unkWord &quot;*UNK*&quot; -dvCombineCategories &amp;gt; /scr/nlp/data/dvparser/wsj/wsj.combine.v2.out 2&amp;gt;&amp;amp;1 &amp;amp;
   */
  public static void main(String[] args)
    throws IOException, ClassNotFoundException
  {
<span class="nc bnc" id="L445" title="All 2 branches missed.">    if (args.length == 0) {</span>
<span class="nc" id="L446">      help();</span>
<span class="nc" id="L447">      System.exit(2);</span>
    }

<span class="nc" id="L450">    log.info(&quot;Running DVParser with arguments:&quot;);</span>
<span class="nc bnc" id="L451" title="All 2 branches missed.">    for (String arg : args) {</span>
<span class="nc" id="L452">      log.info(&quot;  &quot; + arg);</span>
    }
<span class="nc" id="L454">    log.info();</span>

<span class="nc" id="L456">    String parserPath = null;</span>
<span class="nc" id="L457">    String trainTreebankPath = null;</span>
<span class="nc" id="L458">    FileFilter trainTreebankFilter = null;</span>
<span class="nc" id="L459">    String cachedTrainTreesPath = null;</span>

<span class="nc" id="L461">    boolean runGradientCheck = false;</span>
<span class="nc" id="L462">    boolean runTraining = false;</span>

<span class="nc" id="L464">    String testTreebankPath = null;</span>
<span class="nc" id="L465">    FileFilter testTreebankFilter = null;</span>

<span class="nc" id="L467">    String initialModelPath = null;</span>
<span class="nc" id="L468">    String modelPath = null;</span>

<span class="nc" id="L470">    boolean filter = true;</span>

<span class="nc" id="L472">    String resultsRecordPath = null;</span>

<span class="nc" id="L474">    List&lt;String&gt; unusedArgs = new ArrayList&lt;&gt;();</span>

    // These parameters can be null or 0 if the model was not
    // serialized with the new parameters.  Setting the options at the
    // command line will override these defaults.
    // TODO: if/when we integrate back into the main branch and
    // rebuild models, we can get rid of this
<span class="nc" id="L481">    List&lt;String&gt; argsWithDefaults = new ArrayList&lt;&gt;(Arrays.asList(new String[]{</span>
            &quot;-wordVectorFile&quot;, Options.LexOptions.DEFAULT_WORD_VECTOR_FILE,
<span class="nc" id="L483">            &quot;-dvKBest&quot;, Integer.toString(TrainOptions.DEFAULT_K_BEST),</span>
<span class="nc" id="L484">            &quot;-batchSize&quot;, Integer.toString(TrainOptions.DEFAULT_BATCH_SIZE),</span>
<span class="nc" id="L485">            &quot;-trainingIterations&quot;, Integer.toString(TrainOptions.DEFAULT_TRAINING_ITERATIONS),</span>
<span class="nc" id="L486">            &quot;-qnIterationsPerBatch&quot;, Integer.toString(TrainOptions.DEFAULT_QN_ITERATIONS_PER_BATCH),</span>
<span class="nc" id="L487">            &quot;-regCost&quot;, Double.toString(TrainOptions.DEFAULT_REGCOST),</span>
<span class="nc" id="L488">            &quot;-learningRate&quot;, Double.toString(TrainOptions.DEFAULT_LEARNING_RATE),</span>
<span class="nc" id="L489">            &quot;-deltaMargin&quot;, Double.toString(TrainOptions.DEFAULT_DELTA_MARGIN),</span>
            &quot;-unknownNumberVector&quot;,
            &quot;-unknownDashedWordVectors&quot;,
            &quot;-unknownCapsVector&quot;,
            &quot;-unknownchinesepercentvector&quot;,
            &quot;-unknownchinesenumbervector&quot;,
            &quot;-unknownchineseyearvector&quot;,
            &quot;-unkWord&quot;, &quot;*UNK*&quot;,
            &quot;-transformMatrixType&quot;, &quot;DIAGONAL&quot;,
<span class="nc" id="L498">            &quot;-scalingForInit&quot;, Double.toString(TrainOptions.DEFAULT_SCALING_FOR_INIT),</span>
            &quot;-trainWordVectors&quot;,
    }));
<span class="nc" id="L501">    argsWithDefaults.addAll(Arrays.asList(args));</span>
<span class="nc" id="L502">    args = argsWithDefaults.toArray(new String[argsWithDefaults.size()]);</span>

<span class="nc bnc" id="L504" title="All 2 branches missed.">    for (int argIndex = 0; argIndex &lt; args.length; ) {</span>
<span class="nc bnc" id="L505" title="All 2 branches missed.">      if (args[argIndex].equalsIgnoreCase(&quot;-parser&quot;)) {</span>
<span class="nc" id="L506">        parserPath = args[argIndex + 1];</span>
<span class="nc" id="L507">        argIndex += 2;</span>
<span class="nc bnc" id="L508" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-testTreebank&quot;)) {</span>
<span class="nc" id="L509">        Pair&lt;String, FileFilter&gt; treebankDescription = ArgUtils.getTreebankDescription(args, argIndex, &quot;-testTreebank&quot;);</span>
<span class="nc" id="L510">        argIndex = argIndex + ArgUtils.numSubArgs(args, argIndex) + 1;</span>
<span class="nc" id="L511">        testTreebankPath = treebankDescription.first();</span>
<span class="nc" id="L512">        testTreebankFilter = treebankDescription.second();</span>
<span class="nc bnc" id="L513" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-treebank&quot;)) {</span>
<span class="nc" id="L514">        Pair&lt;String, FileFilter&gt; treebankDescription = ArgUtils.getTreebankDescription(args, argIndex, &quot;-treebank&quot;);</span>
<span class="nc" id="L515">        argIndex = argIndex + ArgUtils.numSubArgs(args, argIndex) + 1;</span>
<span class="nc" id="L516">        trainTreebankPath = treebankDescription.first();</span>
<span class="nc" id="L517">        trainTreebankFilter = treebankDescription.second();</span>
<span class="nc bnc" id="L518" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-cachedTrees&quot;)) {</span>
<span class="nc" id="L519">        cachedTrainTreesPath = args[argIndex + 1];</span>
<span class="nc" id="L520">        argIndex += 2;</span>
<span class="nc bnc" id="L521" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-runGradientCheck&quot;)) {</span>
<span class="nc" id="L522">        runGradientCheck = true;</span>
<span class="nc" id="L523">        argIndex++;</span>
<span class="nc bnc" id="L524" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-train&quot;)) {</span>
<span class="nc" id="L525">        runTraining = true;</span>
<span class="nc" id="L526">        argIndex++;</span>
<span class="nc bnc" id="L527" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-model&quot;)) {</span>
<span class="nc" id="L528">        modelPath = args[argIndex + 1];</span>
<span class="nc" id="L529">        argIndex += 2;</span>
<span class="nc bnc" id="L530" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-nofilter&quot;)) {</span>
<span class="nc" id="L531">        filter = false;</span>
<span class="nc" id="L532">        argIndex++;</span>
<span class="nc bnc" id="L533" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-continueTraining&quot;)) {</span>
<span class="nc" id="L534">        runTraining = true;</span>
<span class="nc" id="L535">        filter = false;</span>
<span class="nc" id="L536">        initialModelPath = args[argIndex + 1];</span>
<span class="nc" id="L537">        argIndex += 2;</span>
<span class="nc bnc" id="L538" title="All 2 branches missed.">      } else if (args[argIndex].equalsIgnoreCase(&quot;-resultsRecord&quot;)) {</span>
<span class="nc" id="L539">        resultsRecordPath = args[argIndex + 1];</span>
<span class="nc" id="L540">        argIndex += 2;</span>
      } else {
<span class="nc" id="L542">        unusedArgs.add(args[argIndex++]);</span>
      }
    }

<span class="nc bnc" id="L546" title="All 4 branches missed.">    if (parserPath == null &amp;&amp; modelPath == null) {</span>
<span class="nc" id="L547">      throw new IllegalArgumentException(&quot;Must supply either a base parser model with -parser or a serialized DVParser with -model&quot;);</span>
    }

<span class="nc bnc" id="L550" title="All 6 branches missed.">    if (!runTraining &amp;&amp; modelPath == null &amp;&amp; !runGradientCheck) {</span>
<span class="nc" id="L551">      throw new IllegalArgumentException(&quot;Need to either train a new model, run the gradient check or specify a model to load with -model&quot;);</span>
    }

<span class="nc" id="L554">    String[] newArgs = unusedArgs.toArray(new String[unusedArgs.size()]);</span>
<span class="nc" id="L555">    DVParser dvparser = null;</span>
<span class="nc" id="L556">    LexicalizedParser lexparser = null;</span>
<span class="nc bnc" id="L557" title="All 2 branches missed.">    if (initialModelPath != null) {</span>
<span class="nc" id="L558">      lexparser = LexicalizedParser.loadModel(initialModelPath, newArgs);</span>
<span class="nc" id="L559">      DVModel model = getModelFromLexicalizedParser(lexparser);</span>
<span class="nc" id="L560">      dvparser = new DVParser(model, lexparser);</span>
<span class="nc bnc" id="L561" title="All 4 branches missed.">    } else if (runTraining || runGradientCheck) {</span>
<span class="nc" id="L562">      lexparser = LexicalizedParser.loadModel(parserPath, newArgs);</span>
<span class="nc" id="L563">      dvparser = new DVParser(lexparser);</span>
<span class="nc bnc" id="L564" title="All 2 branches missed.">    } else if (modelPath != null) {</span>
<span class="nc" id="L565">      lexparser = LexicalizedParser.loadModel(modelPath, newArgs);</span>
<span class="nc" id="L566">      DVModel model = getModelFromLexicalizedParser(lexparser);</span>
<span class="nc" id="L567">      dvparser = new DVParser(model, lexparser);</span>
    }

<span class="nc" id="L570">    List&lt;Tree&gt; trainSentences = new ArrayList&lt;&gt;();</span>
<span class="nc" id="L571">    IdentityHashMap&lt;Tree, byte[]&gt; trainCompressedParses = Generics.newIdentityHashMap();</span>

<span class="nc bnc" id="L573" title="All 2 branches missed.">    if (cachedTrainTreesPath != null) {</span>
<span class="nc bnc" id="L574" title="All 2 branches missed.">      for (String path : cachedTrainTreesPath.split(&quot;,&quot;)) {</span>
<span class="nc" id="L575">        List&lt;Pair&lt;Tree, byte[]&gt;&gt; cache = IOUtils.readObjectFromFile(path);</span>

<span class="nc bnc" id="L577" title="All 2 branches missed.">        for (Pair&lt;Tree, byte[]&gt; pair : cache) {</span>
<span class="nc" id="L578">          trainSentences.add(pair.first());</span>
<span class="nc" id="L579">          trainCompressedParses.put(pair.first(), pair.second());</span>
<span class="nc" id="L580">        }</span>

<span class="nc" id="L582">        log.info(&quot;Read in &quot; + cache.size() + &quot; trees from &quot; + path);</span>
      }
    }

<span class="nc bnc" id="L586" title="All 2 branches missed.">    if (trainTreebankPath != null) {</span>
      // TODO: make the transformer a member of the model?
<span class="nc" id="L588">      TreeTransformer transformer = buildTrainTransformer(dvparser.getOp());</span>

<span class="nc" id="L590">      Treebank treebank = dvparser.getOp().tlpParams.memoryTreebank();;</span>
<span class="nc" id="L591">      treebank.loadPath(trainTreebankPath, trainTreebankFilter);</span>
<span class="nc" id="L592">      treebank = treebank.transform(transformer);</span>
<span class="nc" id="L593">      log.info(&quot;Read in &quot; + treebank.size() + &quot; trees from &quot; + trainTreebankPath);</span>

<span class="nc" id="L595">      CacheParseHypotheses cacher = new CacheParseHypotheses(dvparser.parser);</span>
<span class="nc" id="L596">      CacheParseHypotheses.CacheProcessor processor = new CacheParseHypotheses.CacheProcessor(cacher, lexparser, dvparser.op.trainOptions.dvKBest, transformer);</span>
<span class="nc bnc" id="L597" title="All 2 branches missed.">      for (Tree tree : treebank) {</span>
<span class="nc" id="L598">        trainSentences.add(tree);</span>
<span class="nc" id="L599">        trainCompressedParses.put(tree, processor.process(tree).second);</span>
        //System.out.println(tree);
<span class="nc" id="L601">      }</span>

<span class="nc" id="L603">      log.info(&quot;Finished parsing &quot; + treebank.size() + &quot; trees, getting &quot; + dvparser.op.trainOptions.dvKBest + &quot; hypotheses each&quot;);</span>
    }

<span class="nc bnc" id="L606" title="All 6 branches missed.">    if ((runTraining || runGradientCheck) &amp;&amp; filter) {</span>
<span class="nc" id="L607">      log.info(&quot;Filtering rules for the given training set&quot;);</span>
<span class="nc" id="L608">      dvparser.dvModel.setRulesForTrainingSet(trainSentences, trainCompressedParses);</span>
<span class="nc" id="L609">      log.info(&quot;Done filtering rules; &quot; + dvparser.dvModel.numBinaryMatrices + &quot; binary matrices, &quot; + dvparser.dvModel.numUnaryMatrices + &quot; unary matrices, &quot; + dvparser.dvModel.wordVectors.size() + &quot; word vectors&quot;);</span>
    }

    //dvparser.dvModel.printAllMatrices();

<span class="nc" id="L614">    Treebank testTreebank = null;</span>
<span class="nc bnc" id="L615" title="All 2 branches missed.">    if (testTreebankPath != null) {</span>
<span class="nc" id="L616">      log.info(&quot;Reading in trees from &quot; + testTreebankPath);</span>
<span class="nc bnc" id="L617" title="All 2 branches missed.">      if (testTreebankFilter != null) {</span>
<span class="nc" id="L618">        log.info(&quot;Filtering on &quot; + testTreebankFilter);</span>
      }
<span class="nc" id="L620">      testTreebank = dvparser.getOp().tlpParams.memoryTreebank();;</span>
<span class="nc" id="L621">      testTreebank.loadPath(testTreebankPath, testTreebankFilter);</span>
<span class="nc" id="L622">      log.info(&quot;Read in &quot; + testTreebank.size() + &quot; trees for testing&quot;);</span>
    }

//    runGradientCheck= true;
<span class="nc bnc" id="L626" title="All 2 branches missed.">    if (runGradientCheck) {</span>
<span class="nc" id="L627">      log.info(&quot;Running gradient check on &quot; + trainSentences.size() + &quot; trees&quot;);</span>
<span class="nc" id="L628">      dvparser.runGradientCheck(trainSentences, trainCompressedParses);</span>
    }

<span class="nc bnc" id="L631" title="All 2 branches missed.">    if (runTraining) {</span>
<span class="nc" id="L632">      log.info(&quot;Training the RNN parser&quot;);</span>
<span class="nc" id="L633">      log.info(&quot;Current train options: &quot; + dvparser.getOp().trainOptions);</span>
<span class="nc" id="L634">      dvparser.train(trainSentences, trainCompressedParses, testTreebank, modelPath, resultsRecordPath);</span>
<span class="nc bnc" id="L635" title="All 2 branches missed.">      if (modelPath != null) {</span>
<span class="nc" id="L636">        dvparser.saveModel(modelPath);</span>
      }
    }

<span class="nc bnc" id="L640" title="All 2 branches missed.">    if (testTreebankPath != null) {</span>
<span class="nc" id="L641">      EvaluateTreebank evaluator = new EvaluateTreebank(dvparser.attachModelToLexicalizedParser());</span>
<span class="nc" id="L642">      evaluator.testOnTreebank(testTreebank);</span>
    }


<span class="nc" id="L646">    log.info(&quot;Successfully ran DVParser&quot;);</span>
<span class="nc" id="L647">  }</span>

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>