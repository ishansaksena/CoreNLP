<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>TrainOptions.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.parser.lexparser</a> &gt; <span class="el_source">TrainOptions.java</span></div><h1>TrainOptions.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.parser.lexparser; 
import edu.stanford.nlp.util.logging.Redwood;

import edu.stanford.nlp.trees.Tree;
import edu.stanford.nlp.trees.TreeTransformer;
import edu.stanford.nlp.ling.CategoryWordTag;

import java.util.Set;
import java.io.PrintWriter;
import java.io.Serializable;


/**
 * Non-language-specific options for training a grammar from a treebank.
 * These options are not used at parsing time.
 *
 * @author Dan Klein
 * @author Christopher Manning
 */
public class TrainOptions implements Serializable  {

  /** A logger for this class */
<span class="fc" id="L23">  private static Redwood.RedwoodChannels log = Redwood.channels(TrainOptions.class);</span>

<span class="fc" id="L25">  public String trainTreeFile = null; // same for me -- Teg</span>

  /* THESE OPTIONS AFFECT ONLY TRAIN TIME */

<span class="fc" id="L29">  public TrainOptions() {}</span>

<span class="fc" id="L31">  public int trainLengthLimit = 100000;</span>

  /** Add all test set trees to training data for PCFG.
   *  (Currently only supported in FactoredParser main.)
   */
<span class="fc" id="L36">  public boolean cheatPCFG = false;</span>

  /** Whether to do &quot;horizontal Markovization&quot; (as in ACL 2003 paper).
   *  False means regular PCFG expansions.
   */
<span class="fc" id="L41">  public boolean markovFactor = false;</span>
<span class="fc" id="L42">  public int markovOrder = 1;</span>
<span class="fc" id="L43">  public boolean hSelSplit = false; // good with true;</span>
<span class="fc" id="L44">  public int HSEL_CUT = 10;</span>

  /** Whether or not to mark final states in binarized grammar.
   *  This must be off to get most value out of grammar compaction.
   */
<span class="fc" id="L49">  public boolean markFinalStates = true;</span>

  /**
   * A POS tag has to have been attributed to more than this number of word
   * types before it is regarded as an open-class tag.  Unknown words will
   * only possibly be tagged as open-class tags (unless flexiTag is on).
   * If flexiTag is on, unknown words will be able to be tagged any POS for
   * which the unseenMap has nonzero count (that is, the tag was seen for
   * a new word after unseen signature counting was started).
   */
<span class="fc" id="L59">  public int openClassTypesThreshold = 50;</span>

  /**
   * Start to aggregate signature-tag pairs only for words unseen in the first
   * this fraction of the data.
   */
<span class="fc" id="L65">  public double fractionBeforeUnseenCounting = 0.5;</span>

  /**
   * If true, declare early -- leave this on except maybe with markov on.
   * @return Whether to do outside factorization in binarization of the grammar
   */
  public boolean outsideFactor() {
<span class="nc bnc" id="L72" title="All 2 branches missed.">    return !markovFactor;</span>
  }

  /**
   * This variable controls doing parent annotation of phrasal nodes.  Good.
   */
<span class="fc" id="L78">  public boolean PA = true;</span>
  /**
   * This variable controls doing 2 levels of parent annotation.  Bad.
   */
<span class="fc" id="L82">  public boolean gPA = false;</span>

<span class="fc" id="L84">  public boolean postPA = false;</span>
<span class="fc" id="L85">  public boolean postGPA = false;</span>

  /**
   * Only split the &quot;common high KL divergence&quot; parent categories.... Good.
   */
<span class="fc" id="L90">  public boolean selectiveSplit = false; //true;</span>

<span class="fc" id="L92">  public double selectiveSplitCutOff = 0.0;</span>

<span class="fc" id="L94">  public boolean selectivePostSplit = false;</span>

<span class="fc" id="L96">  public double selectivePostSplitCutOff = 0.0;</span>

  /** Whether, in post-splitting of categories, nodes are annotated with the
   *  (grand)parent's base category or with its complete subcategorized
   *  category.
   */
<span class="fc" id="L102">  public boolean postSplitWithBaseCategory = false;</span>

  /**
   * Selective Sister annotation.
   */
<span class="fc" id="L107">  public boolean sisterAnnotate = false;</span>

  public Set&lt;String&gt; sisterSplitters;

  /**
   * Mark all unary nodes specially.  Good for just PCFG. Bad for factored.
   * markUnary affects phrasal nodes. A value of 0 means to do nothing;
   * a value of 1 means to mark the parent (higher) node of a unary rewrite.
   * A value of 2 means to mark the child (lower) node of a unary rewrie.
   * Values of 1 and 2 only apply if the child (lower) node is phrasal.
   * (A value of 1 is better than 2 in combos.)  A value of 1 corresponds
   * to the old boolean -unary flag.
   */
<span class="fc" id="L120">  public int markUnary = 0;</span>

  /** Mark POS tags which are the sole member of their phrasal constituent.
   *  This is like markUnary=2, applied to POS tags.
   */
<span class="fc" id="L125">  public boolean markUnaryTags = false;</span>


  /**
   * Mark all pre-preterminals (also does splitBaseNP: don't need both)
   */
<span class="fc" id="L131">  public boolean splitPrePreT = false;</span>


  /**
   * Parent annotation on tags.  Good (for PCFG?)
   */
<span class="fc" id="L137">  public boolean tagPA = false;//true;</span>

  /**
   * Do parent annotation on tags selectively.  Neutral, but less splits.
   */
<span class="fc" id="L142">  public boolean tagSelectiveSplit = false;</span>

<span class="fc" id="L144">  public double tagSelectiveSplitCutOff = 0.0;</span>

<span class="fc" id="L146">  public boolean tagSelectivePostSplit = false;</span>

<span class="fc" id="L148">  public double tagSelectivePostSplitCutOff = 0.0;</span>

  /**
   * Right edge is right-recursive (X &lt;&lt; X) Bad. (NP only is good)
   */
<span class="fc" id="L153">  public boolean rightRec = false;//true;</span>

  /**
   * Left edge is right-recursive (X &lt;&lt; X)  Bad.
   */
<span class="fc" id="L158">  public boolean leftRec = false;</span>

  /**
   * Promote/delete punctuation like Collins.  Bad (!)
   */
<span class="fc" id="L163">  public boolean collinsPunc = false;</span>

  /**
   * Set the splitter strings.  These are a set of parent and/or grandparent
   * annotated categories which should be split off.
   */
  public Set&lt;String&gt; splitters;

  public Set postSplitters;

  public Set&lt;String&gt; deleteSplitters;

  /**
   * Just for debugging: check that your tree transforms work correctly.  This
   * will print the transformations of the first printTreeTransformations trees.
   */
<span class="fc" id="L179">  public int printTreeTransformations = 0;</span>

  public PrintWriter printAnnotatedPW;
  public PrintWriter printBinarizedPW;

  // todo [cdm nov 2012]: At present this does nothing. It should print the list of all states of a grammar it trains
  // Maybe just make it an anytime option and print it at the same time that verbose printing of tags is done?
<span class="fc" id="L186">  public boolean printStates = false;</span>

  /** How to compact grammars as FSMs.
   *  0 = no compaction [uses makeSyntheticLabel1],
   *  1 = no compaction but use label names that wrap from right to left in binarization [uses makeSyntheticLabel2],
   *  2 = wrapping labels and materialize unary at top rewriting passive to active,
   *  3 = ExactGrammarCompactor,
   *  4 = LossyGrammarCompactor,
   *  5 = CategoryMergingGrammarCompactor.
   *  (May 2007 CDM note: options 4 and 5 don't seem to be functioning sensibly.  0, 1, and 3
   *  seem to be the 'good' options. 2 is only useful as input to 3.  There seems to be
   *  no reason not to use 0, despite the default.)
   */
<span class="fc" id="L199">  public int compactGrammar = 3; // exact compaction on by default</span>

<span class="fc" id="L201">  public boolean leftToRight = false; // whether to binarize left to right or head out</span>

  public int compactGrammar() {
<span class="nc bnc" id="L204" title="All 2 branches missed.">    if (markovFactor) {</span>
<span class="nc" id="L205">      return compactGrammar;</span>
    }
<span class="nc" id="L207">    return 0;</span>
  }

<span class="fc" id="L210">  public boolean noTagSplit = false;</span>

  /**
   * CHANGE ANYTHING BELOW HERE AT YOUR OWN RISK
   */

  /**
   * Enables linear rule smoothing during grammar extraction
   * but before grammar compaction. The alpha term is the same
   * as that described in Petrov et al. (2006), and has range [0,1].
   */
<span class="fc" id="L221">  public boolean ruleSmoothing = false;</span>
<span class="fc" id="L222">  public double ruleSmoothingAlpha = 0.0;</span>

  /**
   * TODO wsg2011: This is the old grammar smoothing parameter that no
   * longer does anything in the parser. It should be removed.
   */
<span class="fc" id="L228">  public boolean smoothing = false;</span>

  /*  public boolean factorOut = false;
  public boolean rightBonus = false;
  public boolean brokenDep = false;*/

  /** Discounts the count of BinaryRule's (only, apparently) in training data. */
<span class="fc" id="L235">  public double ruleDiscount = 0.0;</span>

  //public boolean outsideFilter = false;

<span class="fc" id="L239">  public boolean printAnnotatedRuleCounts = false;</span>
<span class="fc" id="L240">  public boolean printAnnotatedStateCounts = false;</span>

  /** Where to use the basic or split tags in the dependency grammar */
<span class="fc" id="L243">  public boolean basicCategoryTagsInDependencyGrammar = false;</span>

  /**
   * A transformer to use on the training data before any other
   * processing step.  This is specified by using the -preTransformer
   * flag when training the parser.  A comma separated list of classes
   * will be turned into a CompositeTransformer.  This can be used to
   * strip subcategories, to run a tsurgeon pattern, or any number of
   * other useful operations.
   */
<span class="fc" id="L253">  public TreeTransformer preTransformer = null;</span>

  /**
   * A set of files to use as extra information in the lexicon.  This
   * can provide tagged words which are not part of trees
   */
<span class="fc" id="L259">  public String taggedFiles = null;</span>

  /**
   * Use the method reported by Berkeley for splitting and recombining
   * states.  This is an experimental and still in development
   * reimplementation of that work.
   */
<span class="fc" id="L266">  public boolean predictSplits = false;</span>

  /**
   * If we are predicting splits, we loop this many times
   */
<span class="fc" id="L271">  public int splitCount = 1;</span>

  /**
   * If we are predicting splits, we recombine states at this rate every loop
   */
<span class="fc" id="L276">  public double splitRecombineRate = 0.0;</span>

  /**
   * When binarizing trees, don't annotate the labels with anything
   */
<span class="fc" id="L281">  public boolean simpleBinarizedLabels = false;</span>

  /**
   * When binarizing trees, don't binarize trees with two children.
   * Only applies when using inside markov binarization for now.
   */
<span class="fc" id="L287">  public boolean noRebinarization = false;</span>

  /**
   * If the training algorithm allows for parallelization, how many
   * threads to use
   */
<span class="fc" id="L293">  public int trainingThreads = 1;</span>

  /**
   * When training the DV parsing method, how many of the top K trees
   * to analyze from the underlying parser
   */
  static public final int DEFAULT_K_BEST = 100;
<span class="fc" id="L300">  public int dvKBest = DEFAULT_K_BEST;</span>

  /**
   * When training a parsing method where the training has a (max)
   * number of iterations, how many iterations to loop
   */
  static public final int DEFAULT_TRAINING_ITERATIONS = 40;
<span class="fc" id="L307">  public int trainingIterations = DEFAULT_TRAINING_ITERATIONS;</span>

  /**
   * When training using batches of trees, such as in the DVParser,
   * how many trees to use in one batch
   */
  static public final int DEFAULT_BATCH_SIZE = 25;
<span class="fc" id="L314">  public int batchSize = DEFAULT_BATCH_SIZE;</span>
  /**
   * regularization constant
   */
  public static final double DEFAULT_REGCOST = 0.0001;
<span class="fc" id="L319">  public double regCost = DEFAULT_REGCOST;</span>

  /**
   * When training the DV parsing method, how many iterations to loop
   * for one batch of trees
   */
  static public final int DEFAULT_QN_ITERATIONS_PER_BATCH = 1;
<span class="fc" id="L326">  public int qnIterationsPerBatch = DEFAULT_QN_ITERATIONS_PER_BATCH;</span>

  /**
   * When training the DV parsing method, how many estimates to keep
   * for the qn approximation.
   */
<span class="fc" id="L332">  public int qnEstimates = 15;</span>

  /**
   * When training the DV parsing method, the tolerance to use if we
   * want to stop qn early
   */
<span class="fc" id="L338">  public double qnTolerance = 15;</span>

  /**
   * If larger than 0, the parser may choose to output debug information
   * every X seconds, X iterations, or some other similar metric
   */
<span class="fc" id="L344">  public int debugOutputFrequency = 0;</span>

<span class="fc" id="L346">  public long randomSeed = 0;</span>

  public static final double DEFAULT_LEARNING_RATE = 0.1;
  /**
   * How fast to learn (can mean different things for different algorithms)
   */
<span class="fc" id="L352">  public double learningRate = DEFAULT_LEARNING_RATE;</span>

  public static final double DEFAULT_DELTA_MARGIN = 0.1;
  /**
   * How much to penalize the wrong trees for how different they are
   * from the gold tree when training
   */
<span class="fc" id="L359">  public double deltaMargin = DEFAULT_DELTA_MARGIN;</span>

  /**
   * Whether or not to build an unknown word vector specifically for numbers
   */
<span class="fc" id="L364">  public boolean unknownNumberVector = true;</span>

  /**
   * Whether or not to handle unknown dashed words by taking the last part
   */
<span class="fc" id="L369">  public boolean unknownDashedWordVectors = true;</span>

  /**
   * Whether or not to build an unknown word vector for words with caps in them
   */
<span class="fc" id="L374">  public boolean unknownCapsVector = true;</span>

  /**
   * Make the dv model as simple as possible
   */
<span class="fc" id="L379">  public boolean dvSimplifiedModel = false;</span>

  /**
   * Whether or not to build an unknown word vector to match Chinese years
   */
<span class="fc" id="L384">  public boolean unknownChineseYearVector = true;</span>

  /**
   * Whether or not to build an unknown word vector to match Chinese numbers
   */
<span class="fc" id="L389">  public boolean unknownChineseNumberVector = true;</span>

  /**
   * Whether or not to build an unknown word vector to match Chinese percentages
   */
<span class="fc" id="L394">  public boolean unknownChinesePercentVector = true;</span>

  public static final double DEFAULT_SCALING_FOR_INIT = 0.5;
  /**
   * How much to scale certain parameters when initializing models.
   * For example, the DVParser uses this to rescale its initial
   * matrices.
   */
<span class="fc" id="L402">  public double scalingForInit = DEFAULT_SCALING_FOR_INIT;</span>

<span class="fc" id="L404">  public int maxTrainTimeSeconds = 0;</span>

  public static final String DEFAULT_UNK_WORD = &quot;*UNK*&quot;;
  /**
   * Some models will use external data sources which contain
   * information about unknown words.  This variable is a way to
   * provide the name of the unknown word in the external data source.
   */
<span class="fc" id="L412">  public String unkWord = DEFAULT_UNK_WORD;</span>

  /**
   * Whether or not to lowercase word vectors
   */
<span class="fc" id="L417">  public boolean lowercaseWordVectors = false;</span>

<span class="pc" id="L419">  public enum TransformMatrixType {</span>
<span class="fc" id="L420">    DIAGONAL, RANDOM, OFF_DIAGONAL, RANDOM_ZEROS</span>
  }

<span class="fc" id="L423">  public TransformMatrixType transformMatrixType = TransformMatrixType.DIAGONAL;</span>

  /**
   * Specifically for the DVModel, uses words on either side of a
   * context when combining constituents.  Gives perhaps a microscopic
   * improvement in performance but causes a large slowdown.
   */
<span class="fc" id="L430">  public boolean useContextWords = false;</span>

  /**
   * Do we want a model that uses word vectors (such as the DVParser)
   * to train those word vectors when training the model?
   * &lt;br&gt;
   * Note: models prior to 2014-02-13 may have incorrect values in
   * this field, as it was originally a compile time constant
   */
<span class="fc" id="L439">  public boolean trainWordVectors = true;</span>

  public static final int DEFAULT_STALLED_ITERATION_LIMIT = 12;
  /**
   * How many iterations to allow training to stall before taking the
   * best model, if training in an iterative manner
   */
<span class="fc" id="L446">  public int stalledIterationLimit = DEFAULT_STALLED_ITERATION_LIMIT;</span>
  
  /** Horton-Strahler number/dimension (Maximilian Schlund) */
  public boolean markStrahler;

  public void display() {
<span class="nc" id="L452">    log.info(toString());</span>
<span class="nc" id="L453">  }</span>

  @Override
  public String toString() {
<span class="nc" id="L457">    StringBuilder result = new StringBuilder();</span>
<span class="nc" id="L458">    result.append(&quot;Train parameters:\n&quot;);</span>
<span class="nc" id="L459">    result.append(&quot; smooth=&quot; + smoothing + &quot;\n&quot;);</span>
<span class="nc" id="L460">    result.append(&quot; PA=&quot; + PA + &quot;\n&quot;);</span>
<span class="nc" id="L461">    result.append(&quot; GPA=&quot; + gPA + &quot;\n&quot;);</span>
<span class="nc" id="L462">    result.append(&quot; selSplit=&quot; + selectiveSplit + &quot;\n&quot;);</span>
<span class="nc bnc" id="L463" title="All 2 branches missed.">    result.append(&quot; (&quot; + selectiveSplitCutOff + ((deleteSplitters != null) ? (&quot;; deleting &quot; + deleteSplitters): &quot;&quot;) + &quot;)&quot; + &quot;\n&quot;);</span>
<span class="nc" id="L464">    result.append(&quot; mUnary=&quot; + markUnary + &quot;\n&quot;);</span>
<span class="nc" id="L465">    result.append(&quot; mUnaryTags=&quot; + markUnaryTags + &quot;\n&quot;);</span>
<span class="nc" id="L466">    result.append(&quot; sPPT=&quot; + splitPrePreT + &quot;\n&quot;);</span>
<span class="nc" id="L467">    result.append(&quot; tagPA=&quot; + tagPA + &quot;\n&quot;);</span>
<span class="nc" id="L468">    result.append(&quot; tagSelSplit=&quot; + tagSelectiveSplit + &quot; (&quot; + tagSelectiveSplitCutOff + &quot;)&quot; + &quot;\n&quot;);</span>
<span class="nc" id="L469">    result.append(&quot; rightRec=&quot; + rightRec + &quot;\n&quot;);</span>
<span class="nc" id="L470">    result.append(&quot; leftRec=&quot; + leftRec + &quot;\n&quot;);</span>
<span class="nc" id="L471">    result.append(&quot; collinsPunc=&quot; + collinsPunc + &quot;\n&quot;);</span>
<span class="nc" id="L472">    result.append(&quot; markov=&quot; + markovFactor + &quot;\n&quot;);</span>
<span class="nc" id="L473">    result.append(&quot; mOrd=&quot; + markovOrder + &quot;\n&quot;);</span>
<span class="nc" id="L474">    result.append(&quot; hSelSplit=&quot; + hSelSplit + &quot; (&quot; + HSEL_CUT + &quot;)&quot; + &quot;\n&quot;);</span>
<span class="nc" id="L475">    result.append(&quot; compactGrammar=&quot; + compactGrammar() + &quot;\n&quot;);</span>
<span class="nc" id="L476">    result.append(&quot; postPA=&quot; + postPA + &quot;\n&quot;);</span>
<span class="nc" id="L477">    result.append(&quot; postGPA=&quot; + postGPA + &quot;\n&quot;);</span>
<span class="nc" id="L478">    result.append(&quot; selPSplit=&quot; + selectivePostSplit + &quot; (&quot; + selectivePostSplitCutOff + &quot;)&quot; + &quot;\n&quot;);</span>
<span class="nc" id="L479">    result.append(&quot; tagSelPSplit=&quot; + tagSelectivePostSplit + &quot; (&quot; + tagSelectivePostSplitCutOff + &quot;)&quot; + &quot;\n&quot;);</span>
<span class="nc" id="L480">    result.append(&quot; postSplitWithBase=&quot; + postSplitWithBaseCategory + &quot;\n&quot;);</span>
<span class="nc" id="L481">    result.append(&quot; fractionBeforeUnseenCounting=&quot; + fractionBeforeUnseenCounting + &quot;\n&quot;);</span>
<span class="nc" id="L482">    result.append(&quot; openClassTypesThreshold=&quot; + openClassTypesThreshold + &quot;\n&quot;);</span>
<span class="nc" id="L483">    result.append(&quot; preTransformer=&quot; + preTransformer + &quot;\n&quot;);</span>
<span class="nc" id="L484">    result.append(&quot; taggedFiles=&quot; + taggedFiles + &quot;\n&quot;);</span>
<span class="nc" id="L485">    result.append(&quot; predictSplits=&quot; + predictSplits + &quot;\n&quot;);</span>
<span class="nc" id="L486">    result.append(&quot; splitCount=&quot; + splitCount + &quot;\n&quot;);</span>
<span class="nc" id="L487">    result.append(&quot; splitRecombineRate=&quot; + splitRecombineRate + &quot;\n&quot;);</span>
<span class="nc" id="L488">    result.append(&quot; simpleBinarizedLabels=&quot; + simpleBinarizedLabels + &quot;\n&quot;);</span>
<span class="nc" id="L489">    result.append(&quot; noRebinarization=&quot; + noRebinarization + &quot;\n&quot;);</span>
<span class="nc" id="L490">    result.append(&quot; trainingThreads=&quot; + trainingThreads + &quot;\n&quot;);</span>
<span class="nc" id="L491">    result.append(&quot; dvKBest=&quot; + dvKBest + &quot;\n&quot;);</span>
<span class="nc" id="L492">    result.append(&quot; trainingIterations=&quot; + trainingIterations + &quot;\n&quot;);</span>
<span class="nc" id="L493">    result.append(&quot; batchSize=&quot; + batchSize + &quot;\n&quot;);</span>
<span class="nc" id="L494">    result.append(&quot; regCost=&quot; + regCost + &quot;\n&quot;);</span>
<span class="nc" id="L495">    result.append(&quot; qnIterationsPerBatch=&quot; + qnIterationsPerBatch + &quot;\n&quot;);</span>
<span class="nc" id="L496">    result.append(&quot; qnEstimates=&quot; + qnEstimates + &quot;\n&quot;);</span>
<span class="nc" id="L497">    result.append(&quot; qnTolerance=&quot; + qnTolerance + &quot;\n&quot;);</span>
<span class="nc" id="L498">    result.append(&quot; debugOutputFrequency=&quot; + debugOutputFrequency + &quot;\n&quot;);</span>
<span class="nc" id="L499">    result.append(&quot; randomSeed=&quot; + randomSeed + &quot;\n&quot;);</span>
<span class="nc" id="L500">    result.append(&quot; learningRate=&quot; + learningRate + &quot;\n&quot;);</span>
<span class="nc" id="L501">    result.append(&quot; deltaMargin=&quot; + deltaMargin + &quot;\n&quot;);</span>
<span class="nc" id="L502">    result.append(&quot; unknownNumberVector=&quot; + unknownNumberVector + &quot;\n&quot;);</span>
<span class="nc" id="L503">    result.append(&quot; unknownDashedWordVectors=&quot; + unknownDashedWordVectors + &quot;\n&quot;);</span>
<span class="nc" id="L504">    result.append(&quot; unknownCapsVector=&quot; + unknownCapsVector + &quot;\n&quot;);</span>
<span class="nc" id="L505">    result.append(&quot; unknownChineseYearVector=&quot; + unknownChineseYearVector + &quot;\n&quot;);</span>
<span class="nc" id="L506">    result.append(&quot; unknownChineseNumberVector=&quot; + unknownChineseNumberVector + &quot;\n&quot;);</span>
<span class="nc" id="L507">    result.append(&quot; unknownChinesePercentVector=&quot; + unknownChinesePercentVector + &quot;\n&quot;);</span>
<span class="nc" id="L508">    result.append(&quot; dvSimplifiedModel=&quot; + dvSimplifiedModel + &quot;\n&quot;);</span>
<span class="nc" id="L509">    result.append(&quot; scalingForInit=&quot; + scalingForInit + &quot;\n&quot;);</span>
<span class="nc" id="L510">    result.append(&quot; maxTrainTimeSeconds=&quot; + maxTrainTimeSeconds + &quot;\n&quot;);</span>
<span class="nc" id="L511">    result.append(&quot; unkWord=&quot; + unkWord + &quot;\n&quot;);</span>
<span class="nc" id="L512">    result.append(&quot; lowercaseWordVectors=&quot; + lowercaseWordVectors + &quot;\n&quot;);</span>
<span class="nc" id="L513">    result.append(&quot; transformMatrixType=&quot; + transformMatrixType + &quot;\n&quot;);</span>
<span class="nc" id="L514">    result.append(&quot; useContextWords=&quot; + useContextWords + &quot;\n&quot;);</span>
<span class="nc" id="L515">    result.append(&quot; trainWordVectors=&quot; + trainWordVectors + &quot;\n&quot;);</span>
<span class="nc" id="L516">    result.append(&quot; stalledIterationLimit=&quot; + stalledIterationLimit + &quot;\n&quot;);</span>
<span class="nc" id="L517">    result.append(&quot; markStrahler=&quot; + markStrahler + &quot;\n&quot;);</span>
<span class="nc" id="L518">    return result.toString();</span>
  }

  public static void printTrainTree(PrintWriter pw, String message, Tree t) {
    PrintWriter myPW;
<span class="nc bnc" id="L523" title="All 2 branches missed.">    if (pw == null) {</span>
<span class="nc" id="L524">      myPW = new PrintWriter(System.out, true);</span>
    } else {
<span class="nc" id="L526">      myPW = pw;</span>
    }
<span class="nc bnc" id="L528" title="All 4 branches missed.">    if (message != null &amp;&amp; pw == null) {</span>
      // hard coded to not print message if using file output!
<span class="nc" id="L530">      myPW.println(message);</span>
    }
    // TODO FIXME:  wtf is this shit
<span class="nc" id="L533">    boolean previousState = CategoryWordTag.printWordTag;</span>
<span class="nc" id="L534">    CategoryWordTag.printWordTag = false;</span>
<span class="nc" id="L535">    t.pennPrint(myPW);</span>
<span class="nc" id="L536">    CategoryWordTag.printWordTag = previousState;</span>
<span class="nc" id="L537">  }</span>

  private static final long serialVersionUID = 72571349843538L;
} // end class Train
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>