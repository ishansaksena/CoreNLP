<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>StochasticDiffFunctionTester.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Stanford CoreNLP</a> &gt; <a href="index.source.html" class="el_package">edu.stanford.nlp.optimization</a> &gt; <span class="el_source">StochasticDiffFunctionTester.java</span></div><h1>StochasticDiffFunctionTester.java</h1><pre class="source lang-java linenums">package edu.stanford.nlp.optimization; 
import edu.stanford.nlp.util.logging.Redwood;

import edu.stanford.nlp.math.ArrayMath;

import java.util.Random;
import java.util.ArrayList;
import java.util.List;
import java.text.NumberFormat;
import java.text.DecimalFormat;
import java.io.PrintWriter;
import java.io.FileOutputStream;
import java.io.IOException;

/**
 * @author Alex Kleeman
 */
public class StochasticDiffFunctionTester  {

  /** A logger for this class */
<span class="nc" id="L21">  private static Redwood.RedwoodChannels log = Redwood.channels(StochasticDiffFunctionTester.class);</span>
<span class="nc" id="L22">  static private double EPS = 1e-8;</span>
<span class="nc" id="L23">  static private boolean quiet = false;</span>

  protected int testBatchSize;
  protected int numBatches;
  protected AbstractStochasticCachingDiffFunction thisFunc;

  double[]  approxGrad,fullGrad,diff,Hv,HvFD,v,curGrad,gradFD;
<span class="nc" id="L30">  double diffNorm,diffValue,fullValue,approxValue,diffGrad,maxGradDiff = 0.0,maxHvDiff = 0.0;</span>
  Random generator;
<span class="nc" id="L32">  private static NumberFormat nf = new DecimalFormat(&quot;00.0&quot;);</span>

<span class="nc" id="L34">  public StochasticDiffFunctionTester(Function function){</span>

    // check for derivatives
<span class="nc bnc" id="L37" title="All 2 branches missed.">    if (!(function instanceof AbstractStochasticCachingDiffFunction)) {</span>
<span class="nc" id="L38">      log.info(&quot;Attempt to test non stochastic function using StochasticDiffFunctionTester&quot;);</span>
<span class="nc" id="L39">      throw new UnsupportedOperationException();</span>
    }

<span class="nc" id="L42">    thisFunc = (AbstractStochasticCachingDiffFunction) function; // Make sure the function is Stochastic</span>

<span class="nc" id="L44">    generator = new Random(System.currentTimeMillis());  // used to generate random test vectors</span>

    //  Look for a good batchSize to test with by getting factors
<span class="nc" id="L47">    testBatchSize = (int) getTestBatchSize(thisFunc.dataDimension());</span>

    //  Again make sure that our calculated batchSize is actually valid
<span class="nc bnc" id="L50" title="All 6 branches missed.">    if(testBatchSize &lt; 0 || testBatchSize &gt; thisFunc.dataDimension() || (thisFunc.dataDimension()%testBatchSize != 0)){</span>
<span class="nc" id="L51">      log.info(&quot;Invalid testBatchSize found, testing aborted.  Data size: &quot; + thisFunc.dataDimension() + &quot; batchSize: &quot; + testBatchSize);</span>
<span class="nc" id="L52">      System.exit(1);</span>
    }

<span class="nc" id="L55">    numBatches = thisFunc.dataDimension()/testBatchSize;</span>

<span class="nc" id="L57">    sayln(&quot;StochasticDiffFunctionTester created with:&quot;);</span>
<span class="nc" id="L58">    sayln(&quot;   data dimension  = &quot; + thisFunc.dataDimension());</span>
<span class="nc" id="L59">    sayln(&quot;   batch size = &quot; + testBatchSize);</span>
<span class="nc" id="L60">    sayln(&quot;   number of batches = &quot; + numBatches);</span>

<span class="nc" id="L62">  }</span>



  private void sayln(String s) {
<span class="nc bnc" id="L67" title="All 2 branches missed.">    if (!quiet) {</span>
<span class="nc" id="L68">      log.info(s);</span>
    }
<span class="nc" id="L70">  }</span>








  //  Get Prime Factors of an integer ....
  //  Code was originally from    http://www.idinews.com/sourcecode/IntegerFunction.html
  //  Decompose integer into prime factors
  //  ------------------------------------

  //  Upon return result[0] contains the number of factors (0 if N is 0), and
  //  result[1] . . . result[result[0]] contain the factors in ascending order.

  private static long[] primeFactors(long N)
<span class="nc" id="L88">  {long [] fctr = new long[64];       //  Result array</span>
<span class="nc" id="L89">    long n = Math.abs(N);              //  Guard against negative</span>

<span class="nc" id="L91">    short fctrIndex = 0;</span>

<span class="nc bnc" id="L93" title="All 2 branches missed.">    if (n &gt; 0) {                       //  Guard against zero</span>

      //  First do special cases 2 and 3

<span class="nc bnc" id="L97" title="All 2 branches missed.">      while (n % 2 == 0)  {fctr[++fctrIndex] = 2; n /= 2;}</span>
<span class="nc bnc" id="L98" title="All 2 branches missed.">      while (n % 3 == 0)  {fctr[++fctrIndex] = 3; n /= 3;}</span>

      //  Then every 6n-1 and 6n+1 until the divisor exceeds the square root
      //  of the current quotient.  NOTE:  Some trial divisors will be
      //  non-primes, e.g. 25, 35, 49, 55.  They have no effect, however,
      //  since their prime factors will already have been tried.

<span class="nc bnc" id="L105" title="All 2 branches missed.">      for (int k = 5; k*k &lt;= n; k += 6)</span>
<span class="nc bnc" id="L106" title="All 2 branches missed.">        for (int dvsr = k; dvsr &lt;= k+2; dvsr+=2)</span>
<span class="nc bnc" id="L107" title="All 2 branches missed.">        { while (n % dvsr == 0)</span>
<span class="nc" id="L108">        {fctr[++fctrIndex] = dvsr;  n /= dvsr;}</span>
        }

<span class="nc bnc" id="L111" title="All 2 branches missed.">      if (n &gt; 1) fctr[++fctrIndex] = n; //  Store final factor, if any</span>
    }

<span class="nc" id="L114">    fctr[0] = fctrIndex;                //  Store number of factors</span>
<span class="nc" id="L115">    return fctr;</span>
  }


  /**
   *      getTestBatchSize - This function takes as input the size of the data and returns the largest factor of the data size
   *    this is done so that when testing the function we are gaurenteed to have equally sized batches, and that the fewest
   *    number of evaluations needs to be made in order to test the function.
   *
   * @param size   - The size of the current data set
   * @return         The largest factor of the data size
   */
  private static long getTestBatchSize(long size){

<span class="nc" id="L129">    long testBatchSize = 1;</span>

<span class="nc" id="L131">    long[] factors = primeFactors( size );</span>

<span class="nc" id="L133">    long factorCount = factors[0];</span>

    // Calculate the batchsize for the factors
<span class="nc bnc" id="L136" title="All 2 branches missed.">    if( factorCount == 0 ){</span>
<span class="nc" id="L137">      log.info(&quot;Attempt to test function on data of prime dimension.  This would involve a batchSize of 1 and may take a very long time.&quot;);</span>
<span class="nc" id="L138">      System.exit(1);</span>
<span class="nc bnc" id="L139" title="All 2 branches missed.">    }else if (factorCount == 2){</span>
<span class="nc" id="L140">      testBatchSize = (int) factors[1];</span>
    }else {
      //  find the largest factor.
<span class="nc bnc" id="L143" title="All 2 branches missed.">      for( int f = 1; f&lt; factorCount;f++){</span>
<span class="nc" id="L144">        testBatchSize *= factors[f];</span>
      }
    }

<span class="nc" id="L148">    return testBatchSize;</span>
  }







  /**
   *
   * This function tests to make sure that the sum of the stochastic calculated gradients is equal to the
   *  full gradient.  This requires using ordered sampling, so if the ObjectiveFunction itself randomizes
   *  the inputs this function will likely fail.
   *
   *
   * @param x   is the point to evaluate the function at
   * @param functionTolerance   is the tolerance to place on the infinity norm of the gradient and value
   * @return  boolean indicating success or failure.
   */

  public boolean testSumOfBatches(double[] x, double functionTolerance){
<span class="nc" id="L170">    boolean ret = false;</span>
<span class="nc" id="L171">    log.info(&quot;Making sure that the sum of stochastic gradients equals the full gradient&quot;);</span>


<span class="nc" id="L174">    AbstractStochasticCachingDiffFunction.SamplingMethod tmpSampleMethod = thisFunc.sampleMethod;</span>
<span class="nc" id="L175">    StochasticCalculateMethods tmpMethod = thisFunc.method;</span>

    //Make sure that our function is using ordered sampling.  Otherwise we have no gaurentees.
<span class="nc" id="L178">    thisFunc.sampleMethod = AbstractStochasticCachingDiffFunction.SamplingMethod.Ordered;</span>

<span class="nc bnc" id="L180" title="All 2 branches missed.">    if(thisFunc.method==StochasticCalculateMethods.NoneSpecified){</span>
<span class="nc" id="L181">      log.info(&quot;No calculate method has been specified&quot;);</span>
    }

<span class="nc" id="L184">    approxValue = 0;</span>
<span class="nc" id="L185">    approxGrad = new double[x.length];</span>
<span class="nc" id="L186">    curGrad = new double[x.length];</span>
<span class="nc" id="L187">    fullGrad = new double[x.length];</span>
<span class="nc" id="L188">    double percent = 0.0;</span>

    //This loop runs through all the batches and sums of the calculations to compare against the full gradient
<span class="nc bnc" id="L191" title="All 2 branches missed.">    for (int i = 0; i &lt; numBatches ; i ++){</span>

<span class="nc" id="L193">      percent = 100*((double) i)/(numBatches);</span>


      //  update the value
<span class="nc" id="L197">      approxValue += thisFunc.valueAt(x,v,testBatchSize);</span>

      //  update the gradient
<span class="nc" id="L200">      thisFunc.returnPreviousValues = true;</span>
<span class="nc" id="L201">      System.arraycopy(thisFunc.derivativeAt(x,v,testBatchSize ), 0,curGrad, 0, curGrad.length);</span>

      //Update Approximate
<span class="nc" id="L204">      approxGrad = ArrayMath.pairwiseAdd(approxGrad,curGrad);</span>
<span class="nc" id="L205">      double norm = ArrayMath.norm(approxGrad);</span>

<span class="nc" id="L207">      System.err.printf(&quot;%5.1f percent complete  %6.2f \n&quot;,percent,norm);</span>

    }

    // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    // Get the full gradient and value, these should equal the approximates
    // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
<span class="nc" id="L214">    log.info(&quot;About to calculate the full derivative and value&quot;);</span>
<span class="nc" id="L215">    System.arraycopy(thisFunc.derivativeAt(x),0,fullGrad,0,fullGrad.length);</span>
<span class="nc" id="L216">    thisFunc.returnPreviousValues = true;</span>
<span class="nc" id="L217">    fullValue = thisFunc.valueAt(x);</span>

<span class="nc" id="L219">    diff = new double[x.length];</span>

<span class="nc bnc" id="L221" title="All 2 branches missed.">    if( (ArrayMath.norm_inf(diff = ArrayMath.pairwiseSubtract(fullGrad,approxGrad))) &lt; functionTolerance){</span>
<span class="nc" id="L222">      sayln(&quot;&quot;);</span>
<span class="nc" id="L223">      sayln(&quot;Success: sum of batch gradients equals full gradient&quot;);</span>
<span class="nc" id="L224">      ret = true;</span>
    }else{
<span class="nc" id="L226">      diffNorm = ArrayMath.norm(diff);</span>
<span class="nc" id="L227">      sayln(&quot;&quot;);</span>
<span class="nc" id="L228">      sayln(&quot;Failure: sum of batch gradients minus full gradient has norm &quot; + diffNorm);</span>
<span class="nc" id="L229">      ret = false;</span>
    }


<span class="nc bnc" id="L233" title="All 2 branches missed.">    if(Math.abs(approxValue - fullValue) &lt; functionTolerance){</span>
<span class="nc" id="L234">      sayln(&quot;&quot;);</span>
<span class="nc" id="L235">      sayln(&quot;Success: sum of batch values equals full value&quot;);</span>
<span class="nc" id="L236">      ret = true;</span>
    }else{
<span class="nc" id="L238">      sayln(&quot;&quot;);</span>
<span class="nc" id="L239">      sayln(&quot;Failure: sum of batch values minus full value has norm &quot; + Math.abs(approxValue - fullValue));</span>
<span class="nc" id="L240">      ret = false;</span>
    }

<span class="nc" id="L243">    thisFunc.sampleMethod = tmpSampleMethod;</span>
<span class="nc" id="L244">    thisFunc.method = tmpMethod;</span>

<span class="nc" id="L246">    return ret;</span>
  }








  /**
   *
   * This function tests to make sure that the sum of the stochastic calculated gradients is equal to the
   *  full gradient.  This requires using ordered sampling, so if the ObjectiveFunction itself randomizes
   *  the inputs this function will likely fail.
   *
   *
   * @param x   is the point to evaluate the function at
   * @param functionTolerance   is the tolerance to place on the infinity norm of the gradient and value
   * @return  boolean indicating success or failure.
   */

  public boolean testDerivatives(double[] x, double functionTolerance){
<span class="nc" id="L269">    boolean ret = false;</span>
<span class="nc" id="L270">    boolean compareHess = true;</span>
<span class="nc" id="L271">    log.info(&quot;Making sure that the stochastic derivatives are ok.&quot;);</span>


<span class="nc" id="L274">    AbstractStochasticCachingDiffFunction.SamplingMethod tmpSampleMethod = thisFunc.sampleMethod;</span>
<span class="nc" id="L275">    StochasticCalculateMethods tmpMethod = thisFunc.method;</span>

    //Make sure that our function is using ordered sampling.  Otherwise we have no gaurentees.
<span class="nc" id="L278">    thisFunc.sampleMethod = AbstractStochasticCachingDiffFunction.SamplingMethod.Ordered;</span>

<span class="nc bnc" id="L280" title="All 2 branches missed.">    if(thisFunc.method==StochasticCalculateMethods.NoneSpecified){</span>
<span class="nc" id="L281">      log.info(&quot;No calculate method has been specified&quot;);</span>
<span class="nc bnc" id="L282" title="All 2 branches missed.">    } else if( !thisFunc.method.calculatesHessianVectorProduct() ){</span>
<span class="nc" id="L283">      compareHess = false;</span>
    }

<span class="nc" id="L286">    approxValue = 0;</span>
<span class="nc" id="L287">    approxGrad = new double[x.length];</span>
<span class="nc" id="L288">    curGrad = new double[x.length];</span>
<span class="nc" id="L289">    Hv = new double[x.length];</span>


<span class="nc" id="L292">    double percent = 0.0;</span>

    //This loop runs through all the batches and sums of the calculations to compare against the full gradient
<span class="nc bnc" id="L295" title="All 2 branches missed.">    for (int i = 0; i &lt; numBatches ; i ++){</span>

<span class="nc" id="L297">      percent = 100*((double) i)/(numBatches);</span>

      //Can't figure out how to get a carriage return???  ohh well
<span class="nc" id="L300">      System.err.printf(&quot;%5.1f percent complete\n&quot;,percent);</span>


      //  update the &quot;hopefully&quot; correct Hessian
<span class="nc" id="L304">      thisFunc.method = tmpMethod;</span>
<span class="nc" id="L305">      System.arraycopy(thisFunc.HdotVAt(x,v,testBatchSize),0,Hv,0,Hv.length);</span>

      //  Now get the hessian through finite difference
<span class="nc" id="L308">      thisFunc.method = StochasticCalculateMethods.ExternalFiniteDifference;</span>
<span class="nc" id="L309">      System.arraycopy(thisFunc.derivativeAt(x,v,testBatchSize ), 0,gradFD, 0, gradFD.length);</span>
<span class="nc" id="L310">      thisFunc.recalculatePrevBatch = true;</span>
<span class="nc" id="L311">      System.arraycopy(thisFunc.HdotVAt(x,v,gradFD,testBatchSize),0,HvFD,0,HvFD.length);</span>

      //Compare the difference
<span class="nc" id="L314">      double DiffHv = ArrayMath.norm_inf(ArrayMath.pairwiseSubtract(Hv,HvFD));</span>

      //Keep track of the biggest H.v error
<span class="nc bnc" id="L317" title="All 2 branches missed.">      if (DiffHv &gt; maxHvDiff){maxHvDiff = DiffHv;}</span>

    }

<span class="nc bnc" id="L321" title="All 2 branches missed.">    if( maxHvDiff &lt; functionTolerance){</span>
<span class="nc" id="L322">      sayln(&quot;&quot;);</span>
<span class="nc" id="L323">      sayln(&quot;Success: Hessian approximations lined up&quot;);</span>
<span class="nc" id="L324">      ret = true;</span>
    }else{
<span class="nc" id="L326">      sayln(&quot;&quot;);</span>
<span class="nc" id="L327">      sayln(&quot;Failure: Hessian approximation at somepoint was off by &quot; + maxHvDiff);</span>
<span class="nc" id="L328">      ret = false;</span>
    }

<span class="nc" id="L331">    thisFunc.sampleMethod = tmpSampleMethod;</span>
<span class="nc" id="L332">    thisFunc.method = tmpMethod;</span>

<span class="nc" id="L334">    return ret;</span>
  }


  /*
  This function is used to get a lower bound on the condition number.  as it stands this is pretty straight forward:

    a random point (x) and vector (v) are generated, the Raleigh quotient ( v.H(x).v / v.v ) is then taken which provides both
    a lower bound on the largest eigenvalue, and an upper bound on the smallest eigenvalue.  This can then be used to
    come up with a lower bound on the condition number of the hessian.
   */

  public double testConditionNumber(int samples){
<span class="nc" id="L347">    double maxSeen = 0.0;</span>
<span class="nc" id="L348">    double minSeen = 0.0;</span>
<span class="nc" id="L349">    double[] thisV = new double[ thisFunc.domainDimension() ];</span>
<span class="nc" id="L350">    double[] thisX = new double[thisV.length];</span>
<span class="nc" id="L351">    gradFD = new double[thisV.length];</span>
<span class="nc" id="L352">    HvFD = new double[thisV.length];</span>

    double thisVHV;
<span class="nc" id="L355">    boolean isNeg = false;</span>
<span class="nc" id="L356">    boolean isPos = false;</span>
<span class="nc" id="L357">    boolean isSemi = false;</span>

<span class="nc" id="L359">    thisFunc.method = StochasticCalculateMethods.ExternalFiniteDifference;</span>

<span class="nc bnc" id="L361" title="All 2 branches missed.">    for(int j=0;j&lt;samples;j++){</span>

<span class="nc bnc" id="L363" title="All 2 branches missed.">      for (int i=0; i&lt; thisV.length; i++){</span>
<span class="nc" id="L364">        thisV[i] = generator.nextDouble();</span>
      }
<span class="nc bnc" id="L366" title="All 2 branches missed.">      for (int i=0; i&lt; thisX.length; i++){</span>
<span class="nc" id="L367">        thisX[i] = generator.nextDouble();</span>
      }

<span class="nc" id="L370">      log.info(&quot;Evaluating Hessian Product&quot;);</span>
<span class="nc" id="L371">      System.arraycopy(thisFunc.derivativeAt(thisX,thisV,testBatchSize ), 0,gradFD, 0, gradFD.length);</span>
<span class="nc" id="L372">      thisFunc.recalculatePrevBatch = true;</span>
<span class="nc" id="L373">      System.arraycopy(thisFunc.HdotVAt(thisX,thisV,gradFD,testBatchSize),0,HvFD,0,HvFD.length);</span>

<span class="nc" id="L375">      thisVHV = ArrayMath.innerProduct(thisV,HvFD);</span>

<span class="nc bnc" id="L377" title="All 2 branches missed.">      if( Math.abs(thisVHV) &gt; maxSeen){</span>
<span class="nc" id="L378">        maxSeen = Math.abs(thisVHV);</span>
      }

<span class="nc bnc" id="L381" title="All 2 branches missed.">      if( Math.abs(thisVHV) &lt; minSeen){</span>
<span class="nc" id="L382">        minSeen = Math.abs(thisVHV);</span>
      }

<span class="nc bnc" id="L385" title="All 2 branches missed.">      if( thisVHV &lt; 0 ){</span>
<span class="nc" id="L386">        isNeg = true;</span>
      }

<span class="nc bnc" id="L389" title="All 2 branches missed.">      if( thisVHV &gt; 0){</span>
<span class="nc" id="L390">        isPos = true;</span>
      }

<span class="nc bnc" id="L393" title="All 2 branches missed.">      if( thisVHV ==0 ){</span>
<span class="nc" id="L394">        isSemi = true;</span>
      }

<span class="nc" id="L397">      log.info(&quot;It:&quot; + j + &quot;  C:&quot; + maxSeen/minSeen + &quot;N:&quot; + isNeg + &quot;P:&quot; + isPos + &quot;S:&quot; + isSemi);</span>
    }

<span class="nc" id="L400">    System.out.println(&quot;Condition Number of: &quot; + maxSeen/minSeen);</span>
<span class="nc" id="L401">    System.out.println(&quot;Is negative: &quot; + isNeg);</span>
<span class="nc" id="L402">    System.out.println(&quot;Is positive: &quot; + isPos);</span>
<span class="nc" id="L403">    System.out.println(&quot;Is semi:     &quot; + isSemi);</span>

<span class="nc" id="L405">    return maxSeen/minSeen;</span>
  }


  public double[] getVariance(double[] x){
<span class="nc" id="L410">    return getVariance(x,testBatchSize);</span>
  }

  public double[] getVariance(double[] x, int batchSize){

<span class="nc" id="L415">    double[] ret = new double[4];</span>
<span class="nc" id="L416">    double[] fullHx = new double[thisFunc.domainDimension()];</span>
<span class="nc" id="L417">    double[] thisHx = new double[x.length];</span>
<span class="nc" id="L418">    double[] thisGrad = new double[x.length];</span>
<span class="nc" id="L419">    List&lt;double[]&gt; HxList = new ArrayList&lt;&gt;();</span>

    /*
    PrintWriter file = null;
    NumberFormat nf = new DecimalFormat(&quot;0.000E0&quot;);

    try{
      file = new PrintWriter(new FileOutputStream(&quot;var.out&quot;),true);
    }
    catch (IOException e){
      log.info(&quot;Caught IOException outputing List to file: &quot; + e.getMessage());
      System.exit(1);
    }
    */

    //get the full hessian
<span class="nc" id="L435">    thisFunc.sampleMethod = AbstractStochasticCachingDiffFunction.SamplingMethod.Ordered;</span>
<span class="nc" id="L436">    System.arraycopy(thisFunc.derivativeAt(x,x,thisFunc.dataDimension()),0,thisGrad,0,thisGrad.length);</span>
<span class="nc" id="L437">    System.arraycopy(thisFunc.HdotVAt(x,x,thisGrad,thisFunc.dataDimension()),0,fullHx,0,fullHx.length);</span>
<span class="nc" id="L438">    double fullNorm = ArrayMath.norm(fullHx);</span>
<span class="nc" id="L439">    double hessScale = ((double) thisFunc.dataDimension()) / ((double) batchSize);</span>
<span class="nc" id="L440">    thisFunc.sampleMethod = AbstractStochasticCachingDiffFunction.SamplingMethod.RandomWithReplacement;</span>

<span class="nc" id="L442">    int n = 100;</span>
    double simDelta;
    double ratDelta;
<span class="nc" id="L445">    double simMean = 0;</span>
<span class="nc" id="L446">    double ratMean = 0;</span>
<span class="nc" id="L447">    double simS = 0;</span>
<span class="nc" id="L448">    double ratS = 0;</span>
<span class="nc" id="L449">    int k = 0;</span>
<span class="nc" id="L450">    log.info(fullHx[4] +&quot;  &quot; + x[4]);</span>
<span class="nc bnc" id="L451" title="All 2 branches missed.">    for(int i = 0; i&lt;n; i++){</span>
<span class="nc" id="L452">      System.arraycopy(thisFunc.derivativeAt(x,x,batchSize),0,thisGrad,0,thisGrad.length);</span>
<span class="nc" id="L453">      System.arraycopy(thisFunc.HdotVAt(x,x,thisGrad,batchSize),0,thisHx,0,thisHx.length);</span>
<span class="nc" id="L454">      ArrayMath.multiplyInPlace(thisHx,hessScale);</span>

<span class="nc" id="L456">      double thisNorm = ArrayMath.norm(thisHx);</span>
<span class="nc" id="L457">      double sim = ArrayMath.innerProduct(thisHx,fullHx)/(thisNorm*fullNorm);</span>
<span class="nc" id="L458">      double rat = thisNorm/fullNorm;</span>
<span class="nc" id="L459">      k += 1;</span>

<span class="nc" id="L461">      simDelta = sim - simMean;</span>
<span class="nc" id="L462">      simMean += simDelta/k;</span>
<span class="nc" id="L463">      simS += simDelta*(sim-simMean);</span>

<span class="nc" id="L465">      ratDelta = rat-ratMean;</span>
<span class="nc" id="L466">      ratMean += ratDelta/k;</span>
<span class="nc" id="L467">      ratS += ratDelta*(rat-ratMean);</span>

      //file.println( nf.format(sim) + &quot; , &quot; + nf.format(rat));

    }

<span class="nc" id="L473">    double simVar = simS/(k-1);</span>
<span class="nc" id="L474">    double ratVar = ratS/(k-1);</span>


    //file.close();

<span class="nc" id="L479">    ret[0]=simMean;</span>
<span class="nc" id="L480">    ret[1]=simVar;</span>
<span class="nc" id="L481">    ret[2]=ratMean;</span>
<span class="nc" id="L482">    ret[3]=ratVar;</span>
<span class="nc" id="L483">    return ret;</span>
  }


  public void testVariance(double[] x){

<span class="nc" id="L489">    int[] batchSizes = {10,20,35,50,75,150,300,500,750,1000,5000,10000};</span>
    double[] varResult;

<span class="nc" id="L492">    PrintWriter file = null;</span>
<span class="nc" id="L493">    NumberFormat nf = new DecimalFormat(&quot;0.000E0&quot;);</span>

    try{
<span class="nc" id="L496">      file = new PrintWriter(new FileOutputStream(&quot;var.out&quot;),true);</span>
    }
<span class="nc" id="L498">    catch (IOException e){</span>
<span class="nc" id="L499">      log.info(&quot;Caught IOException outputing List to file: &quot; + e.getMessage());</span>
<span class="nc" id="L500">      System.exit(1);</span>
<span class="nc" id="L501">    }</span>

<span class="nc bnc" id="L503" title="All 2 branches missed.">    for(int bSize:batchSizes){</span>

<span class="nc" id="L505">      varResult = getVariance(x,bSize);</span>
<span class="nc" id="L506">      file.println(bSize + &quot;,&quot; + nf.format(varResult[0]) + &quot;,&quot; + nf.format(varResult[1]) + &quot;,&quot; + nf.format(varResult[2]) + &quot;,&quot; + nf.format(varResult[3]));</span>
<span class="nc" id="L507">      log.info(&quot;Batch size of: &quot; + bSize + &quot;   &quot; + varResult[0] + &quot;,&quot; + nf.format(varResult[1]) + &quot;,&quot; + nf.format(varResult[2]) + &quot;,&quot; + nf.format(varResult[3]));</span>
    }


<span class="nc" id="L511">    file.close();</span>

<span class="nc" id="L513">  }</span>

  /*
  public double getNormVariance(List&lt;double[]&gt; thisList){
    double[] ratio = new double[thisList.size()];
    double[] mean = new double[thisList.get(0).length];
    double sizeInv = 1/( (double) thisList.size() );

    for(double[] arr:thisList){
      for(int i=0;i&lt;arr.length;i++){
        mean[i] += arr[i]*sizeInv;
      }
    }

    double meanNorm = ArrayMath.norm(mean);

    for(int i=0;i&lt;thisList.size();i++){
      ratio[i] = (ArrayMath.norm(thisList.get(i))/ meanNorm);
    }

    arrayToFile(ratio,&quot;ratio.out&quot;);

    return ArrayMath.variance(ratio);

  }

  public double getSimVariance(List&lt;double[]&gt; thisList){

    double[] ang = new double[thisList.size()];
    double[] mean = new double[thisList.get(0).length];
    double sizeInv = 1/( (double) thisList.size() );

    for(double[] arr:thisList){
      for(int i=0;i&lt;arr.length;i++){
        mean[i] += arr[i]*sizeInv;
      }
    }

    double meanNorm = ArrayMath.norm(mean);

    for(int i=0;i&lt;thisList.size();i++){
      ang[i] = ArrayMath.innerProduct(thisList.get(i),mean);
      ang[i] = ang[i]/ ( meanNorm * ArrayMath.norm(thisList.get(i)));
    }

    arrayToFile(ang,&quot;angle.out&quot;);

    return ArrayMath.variance(ang);
  }
  */

  public void listToFile(List&lt;double[]&gt; thisList,String fileName){
<span class="nc" id="L565">    PrintWriter file = null;</span>
<span class="nc" id="L566">    NumberFormat nf = new DecimalFormat(&quot;0.000E0&quot;);</span>

    try{
<span class="nc" id="L569">      file = new PrintWriter(new FileOutputStream(fileName),true);</span>
    }
<span class="nc" id="L571">    catch (IOException e){</span>
<span class="nc" id="L572">      log.info(&quot;Caught IOException outputing List to file: &quot; + e.getMessage());</span>
<span class="nc" id="L573">      System.exit(1);</span>
<span class="nc" id="L574">    }</span>

<span class="nc bnc" id="L576" title="All 2 branches missed.">    for(double[] element:thisList){</span>
<span class="nc bnc" id="L577" title="All 2 branches missed.">      for(double val:element){</span>
<span class="nc" id="L578">        file.print(nf.format(val) + &quot;  &quot;);</span>
      }
<span class="nc" id="L580">      file.println(&quot;&quot;);</span>
<span class="nc" id="L581">    }</span>

<span class="nc" id="L583">    file.close();</span>

<span class="nc" id="L585">  }</span>

  public void arrayToFile(double[] thisArray,String fileName){
<span class="nc" id="L588">    PrintWriter file = null;</span>
<span class="nc" id="L589">    NumberFormat nf = new DecimalFormat(&quot;0.000E0&quot;);</span>

    try{
<span class="nc" id="L592">      file = new PrintWriter(new FileOutputStream(fileName),true);</span>
    }
<span class="nc" id="L594">    catch (IOException e){</span>
<span class="nc" id="L595">      log.info(&quot;Caught IOException outputing List to file: &quot; + e.getMessage());</span>
<span class="nc" id="L596">      System.exit(1);</span>
<span class="nc" id="L597">    }</span>

<span class="nc bnc" id="L599" title="All 2 branches missed.">    for(double element:thisArray){</span>
<span class="nc" id="L600">      file.print(nf.format(element) + &quot;  &quot;);</span>
    }

<span class="nc" id="L603">    file.close();</span>

<span class="nc" id="L605">  }</span>

  /**
   * testObjectiveFunction
   *  This function was written to provide a test for accuracy of stochastic objective functions.  The test
   *  checks for the following properties:
   *
   * 1) The sum of the value over each batch equals the full value
   * 2) The sum of the gradients over each batch equals the full gradient
   * 3) The gradient calculated using Incorporated Finite Difference is never more than functionTolerance from the
   *        gradient using External Finite Difference
   * 4) The hessian vector also does not varry between Incorporated and External Finite Difference
   *
   * @param function          The function to test
   * @param x                 The point to use for testing (v is generated randomly
   * @param functionTolerance The tolerance
   */

  /*
  public boolean testObjectiveFunction(Function function, double[] x, double functionTolerance){



    approxGrad = new double[x.length];
    curGrad = new double[x.length];
    approxValue = 0;

    //Generate the initial vectors
    for (int i = 0; i &lt; x.length; i ++){
      approxGrad[i] = 0;
      v[i] = generator.nextDouble() ;
    }


    //This loop runs through all the batches and sums of the calculations to compare against the full gradient
    for (int i = 0; i &lt; numBatches ; i ++){

// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
// Perform calculation using IncorporatedFiniteDifference
// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

      dfunction.method = StochasticCalculateMethods.IncorporatedFiniteDifference;

      //  update the value
      approxValue += dfunction.valueAt(x,v,testBatchSize);

      //  update the gradient
      dfunction.returnPreviousValues = true;
      System.arraycopy(dfunction.derivativeAt(x,v,testBatchSize ), 0,curGrad, 0, curGrad.length);

      //  update the Hessian
      dfunction.returnPreviousValues = true;
      System.arraycopy(dfunction.HdotVAt(x,v,testBatchSize),0,HvAD,0,HvAD.length);

      //Update Approximate
      approxGrad = ArrayMath.pairwiseAdd(approxGrad,curGrad);

 // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 // Perform calculations using external finite difference
 // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

      dfunction.method = StochasticCalculateMethods.ExternalFiniteDifference;

      dfunction.recalculatePrevBatch = true;
      System.arraycopy(dfunction.derivativeAt(x,v,testBatchSize ), 0,gradFD, 0, gradFD.length);

      dfunction.recalculatePrevBatch = true;
      System.arraycopy(dfunction.HdotVAt(x,v,gradFD,testBatchSize),0,HvFD,0,HvFD.length);

      double DiffGrad = ArrayMath.norm_inf(ArrayMath.pairwiseSubtract(gradFD,curGrad));

      // Keep track of the biggest error.
      if (DiffGrad &gt; maxGradDiff){maxGradDiff = DiffGrad;}

      double DiffHv = ArrayMath.norm_inf(ArrayMath.pairwiseSubtract(HvAD,HvFD));

      //Keep track of the biggest H.v error
      if (DiffHv &gt; maxHvDiff){maxHvDiff = DiffHv;}
    }

 // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
 // Get the full gradient and value, these should equal the approximates
 // !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
    System.arraycopy(dfunction.derivativeAt(x),0,fullGrad,0,fullGrad.length);
    fullValue = dfunction.valueAt(x);


    if(ArrayMath.norm_inf(ArrayMath.pairwiseSubtract(fullGrad,approxGrad)) &lt; functionTolerance){
      sayln(&quot;&quot;);
      sayln(&quot;  Gradient is looking good&quot;);
    }else{
      diff = new double[x.length];
      diff = ArrayMath.pairwiseSubtract(approxGrad,fullGrad);
      diffNorm = ArrayMath.norm(diff);
      sayln(&quot;&quot;);
      sayln(&quot;  Seems there is a problem.  Gradient is off by norm of &quot; + diffNorm);
    };

    if( maxGradDiff &lt; functionTolerance ){
      sayln(&quot;&quot;);
      sayln(&quot;  Both gradients are the same&quot;);
    }else{
      diffValue = approxValue - fullValue;
      sayln(&quot;&quot;);
      sayln(&quot;  Seems there is a problem.  The two methods of calculating the gradient are different  max |AD-FD|_inf Error of &quot; + maxGradDiff);
    };


    if( Math.abs(fullValue - approxValue) &lt; functionTolerance){
      sayln(&quot;&quot;);
      sayln(&quot;  Value is looking good&quot;);
    }else{
      diffValue = approxValue - fullValue;
      sayln(&quot;&quot;);
      sayln(&quot;  Seems there is a problem.  Value is off by &quot; + diffValue);
    };

    if(maxHvDiff &lt; functionTolerance){
      sayln(&quot;&quot;);
      sayln(&quot;  Hv Approimations line up well&quot;);
    }else{
      sayln(&quot;&quot;);
      sayln(&quot;    Seems there is a problem.  Hv approximations aren't quite close enough -- max |AD-FD|_inf Error of &quot; + maxHvDiff);
    }

    return true;
  }
*/

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.7.8.201612092310</span></div></body></html>